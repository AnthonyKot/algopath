{
  "dp": [
    {
      "id": "dp-1",
      "title": "Edit Distance (Levenshtein)",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/edit-distance/",
      "related": [
        {
          "id": "dp-5",
          "title": "Regular Expression Matching",
          "category": "dp"
        },
        {
          "id": "graphs-1",
          "title": "Word Ladder",
          "category": "graphs"
        }
      ],
      "tags": [
        "dp",
        "strings",
        "edit-distance"
      ],
      "follow_up": {
        "scenario": "Spell checker for a search engine. Need millisecond response for 'Did you mean?'.",
        "trade_off": "O(N^2) DP is too slow for real-time query correction on millions of terms.",
        "strategy": "Use a Levenshtein Automaton or BK-Trees for retrieval. For exact distance between long strings, use Bit-parallelism.",
        "answering_guide": "The magic phrase is <strong>'fuzzy search'</strong>. If you mention <strong>'Lucene Automaton'</strong> or <strong>'Bit-parallelism'</strong>, you win instantly."
      },
      "problem_statement": "Given two strings `word1` and `word2`, return the minimum number of operations required to convert `word1` to `word2`.\n\nYou have the following three operations permitted on a word:\n1. Insert a character\n2. Delete a character\n3. Replace a character\n\n**Example 1:**\n```\nInput: word1 = \"horse\", word2 = \"ros\"\nOutput: 3\nExplanation: \nhorse -> rorse (replace 'h' with 'r')\nrorse -> rose (remove 'r')\nrose -> ros (remove 'e')\n```\n\n**Example 2:**\n```\nInput: word1 = \"intention\", word2 = \"execution\"\nOutput: 5\nExplanation: \nintention -> inention (remove 't')\ninention -> enention (replace 'i' with 'e')\nenention -> exention (replace 'n' with 'x')\nexention -> exection (replace 'n' with 'c')\nexection -> execution (insert 'u')\n```",
      "explanation": {
        "understanding_the_problem": "We need to find the *minimum* number of edits (insert, delete, replace) to transform one string into another. The 'minimum' requirement is a strong hint that this is an optimization problem, and since it involves sequences (strings), it's a classic candidate for Dynamic Programming.\n\nThe core idea of DP is to solve a complex problem by breaking it down into simpler, overlapping subproblems. The edit distance between `word1` and `word2` depends on the edit distances of their prefixes.",
        "brute_force": "We can define a function `dp(i, j)` that gives the edit distance between the first `i` characters of `word1` and the first `j` characters of `word2`. Our goal is to find `dp(m, n)` where `m` and `n` are the lengths of the words.\n\nTo calculate `dp(i, j)`, we look at the last characters, `word1[i-1]` and `word2[j-1]`.\n\n- **If the characters match:** No operation is needed. The cost is just the result of the subproblem for the prefixes without these characters: `dp(i-1, j-1)`.\n- **If the characters don't match:** We must perform an operation. We choose the one with the minimum cost:\n    - **Replace:** `1 + dp(i-1, j-1)` (cost of replace + cost of smaller subproblem)\n    - **Delete:** `1 + dp(i-1, j)` (cost of delete + cost of matching `word1` prefix to `word2`)\n    - **Insert:** `1 + dp(i, j-1)` (cost of insert + cost of matching `word1` to `word2` prefix)",
        "bottleneck": "A purely recursive solution would be extremely slow because it would re-calculate the same subproblems (e.g., `dp(3, 4)`) many times. This is the 'overlapping subproblems' property that DP is designed to solve.",
        "optimized_approach": "We can use a 2D table (an array of arrays) to store the results of the subproblems, a technique called memoization or tabulation. Let `dp[i][j]` be the minimum edit distance between `word1.slice(0, i)` and `word2.slice(0, j)`.\n\nWe fill this table bottom-up, starting with the base cases (like the distance from an empty string to a prefix), and use the state transition logic described above to fill the entire table. The final answer is the value in the bottom-right cell, `dp[m][n]`.",
        "algorithm_steps": "1.  **Initialization:**\n    a.  Create a `dp` table of size `(m+1) x (n+1)`.\n    b.  `dp[0][0] = 0` (distance between two empty strings is 0).\n    c.  Fill the first row: `dp[0][j] = j`. This is the cost to create `word2`'s prefix from an empty string (j insertions).\n    d.  Fill the first column: `dp[i][0] = i`. This is the cost to reduce `word1`'s prefix to an empty string (i deletions).\n\n2.  **Main Loop:**\n    a.  Iterate with `i` from 1 to `m` and `j` from 1 to `n`.\n    b.  If `word1[i-1] == word2[j-1]`, then `dp[i][j] = dp[i-1][j-1]`.\n    c.  Otherwise, `dp[i][j] = 1 + min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1])`.\n\n3.  **Result:** Return `dp[m][n]`.\n\n**Space Optimization:** Notice that to compute a cell `dp[i][j]`, we only need values from the previous row and the current row. This means we can optimize the space from O(mn) to O(n) by only storing one or two rows at a time. The provided solution uses a clever single-array approach for this."
      },
      "code": "function editDistance(s1, s2) {\n  let [m, n] = [s1.length, s2.length];\n  if (m < n) { [s1, s2] = [s2, s1]; [m, n] = [n, m]; }\n  const dp = Array(n+1).fill(0);\n  for (let j = 0; j <= n; j++) dp[j] = j;\n  for (let i = 1; i <= m; i++) {\n    let prev = i;\n    for (let j = 1; j <= n; j++) {\n      let temp = dp[j];\n      dp[j] = s1[i-1] === s2[j-1] ? dp[j-1] : 1 + Math.min(dp[j-1], prev, dp[j]);\n      prev = temp;\n    }\n  }\n  return dp[n];\n}",
      "complexity": {
        "time": "O(m*n)",
        "space": "O(min(m,n))",
        "explanation_time": "We need to compute the value for each of the m*n cells in our conceptual DP table. Each computation takes constant time. Therefore, the time complexity is O(m*n), where m and n are the lengths of the two strings.",
        "explanation_space": "The standard 2D DP table approach requires O(m*n) space. However, the provided solution is space-optimized. It only stores one row of the DP table at a time, reducing the space complexity to O(n), where n is the length of the shorter string (due to the initial swap)."
      },
      "annotations": [
        {
          "lines": [
            3
          ],
          "text": "Swap so s2 is shorter - dp array is smaller for space optimization"
        },
        {
          "lines": [
            4,
            5
          ],
          "text": "Base case: dp[j] = j means j insertions to transform empty string to s2[0..j]"
        },
        {
          "lines": [
            7
          ],
          "text": "'prev' stores dp[i-1][j-1] from 2D table - the diagonal value we need"
        },
        {
          "lines": [
            9
          ],
          "text": "'temp' saves dp[j] before overwrite, becomes 'prev' for next column"
        },
        {
          "lines": [
            10
          ],
          "text": "Core recurrence: if chars match use diagonal, else 1 + min(replace, insert, delete)"
        },
        {
          "lines": [
            14
          ],
          "text": "Final answer: min edits to transform s1[0..m] to s2[0..n]"
        }
      ],
      "quizzes": [
        {
          "question": "What technique best fits a problem with overlapping subproblems?",
          "options": [
            "Greedy",
            "Divide & Conquer",
            "Dynamic Programming",
            "Backtracking"
          ],
          "correct": 2
        },
        {
          "question": "What is the main bottleneck of naive recursion here?",
          "options": [
            "Stack overflow",
            "Repeated subproblems",
            "Memory allocation",
            "String copying"
          ],
          "correct": 1
        },
        {
          "question": "How do we avoid recomputing the same subproblems?",
          "options": [
            "Sorting",
            "Hashing",
            "Memoization",
            "Parallelism"
          ],
          "correct": 2
        },
        {
          "question": "What does dp[i][j] represent in this solution?",
          "options": [
            "Max edits",
            "Min edits for prefixes",
            "Character count",
            "String length"
          ],
          "correct": 1
        },
        {
          "question": "What is the space complexity after optimization?",
          "options": [
            "O(m*n)",
            "O(m+n)",
            "O(min(m,n))",
            "O(1)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "dp-2",
      "title": "Longest Increasing Subsequence",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/longest-increasing-subsequence/",
      "related": [
        {
          "id": "dp-4",
          "title": "Burst Balloons",
          "category": "dp"
        },
        {
          "id": "trees-1",
          "title": "Range Sum Queries",
          "category": "trees"
        }
      ],
      "tags": [
        "dp",
        "binary-search",
        "lis"
      ],
      "problem_statement": "Given an integer array `nums`, return the length of the longest strictly increasing subsequence.\n\n**Example 1:**\n```\nInput: nums = [10,9,2,5,3,7,101,18]\nOutput: 4\nExplanation: The longest increasing subsequence is [2,3,7,101], therefore the length is 4.\n```\n\n**Example 2:**\n```\nInput: nums = [0,1,0,3,2,3]\nOutput: 4\n```",
      "explanation": {
        "understanding_the_problem": "We need to find the length of the longest subsequence where each element is strictly greater than the previous one. The elements do not need to be contiguous. For example, in `[10, 9, 2, 5, 3, 7, 101]`, the longest increasing subsequence is `[2, 3, 7, 101]`, so the answer is 4.",
        "brute_force": "This is a classic Dynamic Programming problem. A common DP approach is to define an array, `dp`, where `dp[i]` represents the length of the longest increasing subsequence that **ends at index `i`**.\n\nTo calculate `dp[i]`, we look at all previous indices `j < i`. If `nums[j] < nums[i]`, it means we can potentially extend an increasing subsequence that ended at `j`. We would take the longest such subsequence (`dp[j]`) and add 1. We check this for all valid `j`'s and take the maximum.\n\nThe final answer is the maximum value in the entire `dp` array, since the LIS could end at any index.",
        "bottleneck": "The DP approach described above requires two nested loops. The outer loop iterates from `i = 0 to N-1`, and the inner loop iterates from `j = 0 to i-1`. This results in an O(N²) time complexity, which is good but can be too slow for larger constraints.",
        "optimized_approach": "A more advanced and efficient solution uses a clever combination of patience and binary search, achieving O(N log N) time. The core idea is to maintain an array (let's call it `tails`) which stores the smallest tail of all increasing subsequences with length `i+1` at `tails[i]`.\n\nFor example, if `tails` is `[2, 5, 7]`, it means we have found an LIS of length 1 ending in 2, an LIS of length 2 ending in 5, and an LIS of length 3 ending in 7. This `tails` array will always be sorted.\n\nWhen we process a new number `num`:\n- If `num` is greater than the last element of `tails`, we can extend the longest subsequence found so far. We append `num` to `tails`.\n- If `num` is not greater, it might be able to start a new, shorter subsequence with a more promising (smaller) tail. We find the position of the smallest element in `tails` that is greater than or equal to `num` and replace it. This search can be done with binary search.",
        "algorithm_steps": "Here we describe the O(N²) DP approach, which is more intuitive to start with:\n\n1.  **Initialization:** Create a `dp` array of the same size as `nums`, and fill it with `1`s. This represents the base case where each element is itself an increasing subsequence of length 1.\n2.  **Main Loop:**\n    a.  Iterate `i` from 1 to `N-1` (for each element).\n    b.  Nest a loop with `j` from 0 to `i-1` (to check all previous elements).\n    c.  If `nums[j] < nums[i]`, it's possible to extend the subsequence ending at `j`. Update `dp[i]` to be `max(dp[i], dp[j] + 1)`.\n3.  **Result:** After the loops, the answer is the maximum value found in the `dp` array."
      },
      "code": "function lengthOfLIS(nums) {\n  const n = nums.length;\n  const dp = new Array(n).fill(1);\n  for (let i = 1; i < n; i++) {\n    for (let j = 0; j < i; j++) {\n      if (nums[j] < nums[i]) dp[i] = Math.max(dp[i], dp[j] + 1);\n    }\n  }\n  return Math.max(...dp);\n}",
      "complexity": {
        "time": "O(N²)",
        "space": "O(N)",
        "explanation_time": "The provided code uses a straightforward DP approach. There are two nested loops. The outer loop runs N times, and the inner loop runs up to N times. This gives a time complexity of O(N²).",
        "explanation_space": "We use an additional array `dp` of the same size as the input array `nums`. This requires O(N) space."
      },
      "annotations": [
        {
          "lines": [
            2,
            3
          ],
          "text": "Initialize dp array: dp[i] = 1 means each element is an LIS of length 1 by itself"
        },
        {
          "lines": [
            4
          ],
          "text": "Outer loop: for each position i, find the longest increasing subsequence ending at i"
        },
        {
          "lines": [
            5
          ],
          "text": "Inner loop: check all previous elements j < i to find valid predecessors"
        },
        {
          "lines": [
            6
          ],
          "text": "Core recurrence: if nums[j] < nums[i], we can extend LIS ending at j. Take the maximum"
        },
        {
          "lines": [
            9
          ],
          "text": "Answer is the max of all dp values since LIS could end at any index"
        }
      ],
      "quizzes": [
        {
          "question": "What does dp[i] represent in the LIS solution?",
          "options": [
            "Total subsequences",
            "LIS ending at index i",
            "LIS starting at i",
            "Array length"
          ],
          "correct": 1
        },
        {
          "question": "What is the bottleneck of O(N²) DP for LIS?",
          "options": [
            "Memory usage",
            "Nested loops checking all pairs",
            "Recursion depth",
            "String operations"
          ],
          "correct": 1
        },
        {
          "question": "How can we optimize LIS to O(N log N)?",
          "options": [
            "Hash table",
            "Binary search on tails array",
            "Sorting first",
            "Divide and conquer"
          ],
          "correct": 1
        },
        {
          "question": "Why do we initialize dp[i] = 1?",
          "options": [
            "Array indexing",
            "Each element is LIS of length 1",
            "Prevent overflow",
            "Match array size"
          ],
          "correct": 1
        },
        {
          "question": "How do we get the final answer?",
          "options": [
            "dp[0]",
            "dp[n-1]",
            "Max of all dp values",
            "Sum of dp values"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "dp-3",
      "title": "Coin Change (Min Coins)",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/coin-change/",
      "related": [
        {
          "id": "dp-2",
          "title": "Longest Increasing Subsequence",
          "category": "dp"
        }
      ],
      "tags": [
        "dp",
        "knapsack",
        "greedy"
      ],
      "problem_statement": "You are given an integer array `coins` representing coins of different denominations and an integer `amount` representing a total amount of money.\n\nReturn the fewest number of coins that you need to make up that amount. If that amount of money cannot be made up by any combination of the coins, return -1.\n\nYou may assume that you have an infinite number of each kind of coin.\n\n**Example 1:**\n```\nInput: coins = [1,2,5], amount = 11\nOutput: 3\nExplanation: 11 = 5 + 5 + 1\n```",
      "explanation": {
        "understanding_the_problem": "We need to form a target `amount` using the fewest possible coins from a given set. We can use each coin denomination as many times as we want. This is a classic optimization problem that fits the Dynamic Programming pattern, specifically a variation known as the **Unbounded Knapsack** problem.",
        "brute_force": "A recursive approach would be to try to solve for `amount` by taking one coin and then recursively solving for `amount - coin`. We would do this for every coin in our set and take the minimum result. For example, `minCoins(amount) = 1 + min(minCoins(amount - c1), minCoins(amount - c2), ...)`.",
        "bottleneck": "The purely recursive solution is very slow because it recalculates the same subproblems repeatedly. For instance, to calculate the coins for amount 10, we might explore `10 -> 5 -> 2` and also `10 -> 8 -> 5 -> 2`. The solution for amount 5 and 2 would be computed multiple times.",
        "optimized_approach": "We can use a bottom-up Dynamic Programming approach to solve this efficiently. We'll create a `dp` array where `dp[i]` stores the minimum number of coins needed to make an amount of `i`.\n\nWe build this array from `dp[0]` up to `dp[amount]`. To compute `dp[i]`, we consider each coin denomination. If we use a `coin`, the total number of coins will be `1 + dp[i - coin]`. We simply take the minimum value across all possible coins we can use.",
        "algorithm_steps": "1.  **Initialization:**\n    a.  Create a `dp` array of size `amount + 1`.\n    b.  Initialize all values to a placeholder for infinity (e.g., `amount + 1`). This signifies that an amount is not yet reachable.\n    c.  Set the base case: `dp[0] = 0`. It takes 0 coins to make an amount of 0.\n\n2.  **Main Loop (Bottom-Up):**\n    a.  Iterate `i` from 1 to `amount`. For each `i`, we are solving for the minimum coins to make amount `i`.\n    b.  Inside, loop through each `coin` in our `coins` array.\n    c.  If `i >= coin`, it's possible to use this coin. We update our answer for `dp[i]` with the minimum of its current value and `1 + dp[i - coin]`.\n\n3.  **Result:**\n    a.  After the loops, if `dp[amount]` is still our infinity placeholder, it means the amount was unreachable. Return -1.\n    b.  Otherwise, `dp[amount]` holds the answer."
      },
      "code": "function coinChange(coins, amount) {\n  const dp = new Array(amount + 1).fill(Infinity);\n  dp[0] = 0;\n  for (let coin of coins) {\n    for (let x = coin; x <= amount; x++) {\n      dp[x] = Math.min(dp[x], dp[x - coin] + 1);\n    }\n  }\n  return dp[amount] === Infinity ? -1 : dp[amount];\n}",
      "complexity": {
        "time": "O(A * C)",
        "space": "O(A)",
        "explanation_time": "Let A be the target amount and C be the number of coin denominations. We have two nested loops. The outer loop iterates through the coins (C times in the provided code, though it can be swapped), and the inner loop iterates up to the amount (A times). This gives a total time complexity of O(A * C).",
        "explanation_space": "We use a `dp` array of size `amount + 1` to store the intermediate solutions for all amounts from 0 to `amount`. This requires O(A) space."
      },
      "annotations": [
        {
          "lines": [
            2
          ],
          "text": "Create dp array of size amount+1, initialize with Infinity (unreachable)"
        },
        {
          "lines": [
            3
          ],
          "text": "Base case: 0 coins needed to make amount 0"
        },
        {
          "lines": [
            4
          ],
          "text": "Outer loop: iterate through each coin denomination"
        },
        {
          "lines": [
            5
          ],
          "text": "Inner loop: for each amount >= coin, check if using this coin improves the solution"
        },
        {
          "lines": [
            6
          ],
          "text": "Core recurrence: dp[x] = min(dp[x], 1 + dp[x-coin]) - use coin if it gives fewer coins"
        },
        {
          "lines": [
            9
          ],
          "text": "Return answer, or -1 if amount is still unreachable (Infinity)"
        }
      ],
      "quizzes": [
        {
          "question": "What DP pattern does Coin Change follow?",
          "options": [
            "0/1 Knapsack",
            "Unbounded Knapsack",
            "LCS",
            "Matrix Chain"
          ],
          "correct": 1
        },
        {
          "question": "Why is naive recursion slow for this problem?",
          "options": [
            "Stack overflow",
            "Repeated subproblems",
            "Too many coins",
            "Large amounts"
          ],
          "correct": 1
        },
        {
          "question": "What does dp[i] store?",
          "options": [
            "Number of ways",
            "Min coins for amount i",
            "Max coins",
            "Coin values"
          ],
          "correct": 1
        },
        {
          "question": "Why initialize dp with Infinity?",
          "options": [
            "Prevent overflow",
            "Mark unreachable amounts",
            "Faster comparison",
            "Memory efficiency"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity?",
          "options": [
            "O(N)",
            "O(N²)",
            "O(Amount × Coins)",
            "O(2^N)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "dp-4",
      "title": "Burst Balloons",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/burst-balloons/",
      "tags": [
        "dp",
        "interval-dp",
        "optimization"
      ],
      "problem_statement": "You are given `n` balloons, indexed from `0` to `n - 1`. Each balloon is painted with a number on it represented by an array `nums`.\n\nYou are asked to burst all the balloons. If you burst the `i-th` balloon, you will get `nums[i-1] * nums[i] * nums[i+1]` coins. If `i-1` or `i+1` goes out of bounds of the array, then treat it as if there is a balloon with a `1` painted on it.\n\nReturn the maximum coins you can collect by bursting the balloons wisely.\n\n**Example 1:**\n```\nInput: nums = [3,1,5,8]\nOutput: 167\nExplanation:\nnums = [3,1,5,8] -> [3,5,8] -> [3,8] -> [8] -> []\ncoins =  3*1*5    +   3*5*8   +  1*3*8  + 1*8*1 = 167\n```",
      "explanation": {
        "understanding_the_problem": "We need to find the optimal order of bursting balloons to maximize our score. The main difficulty is that bursting a balloon affects the score of future bursts because the 'adjacent' balloons change. This dynamic dependency makes a simple greedy approach unlikely to work.",
        "brute_force": "A standard DP approach might be to define `dp(balloons_left)` as the max score. However, the subproblems are not independent. The score for bursting a balloon in a sub-array depends on the boundaries, which might have been burst already. This makes the state hard to define.\n\nThe key insight is to change our perspective: instead of thinking 'which balloon to burst first?', we should think **'which balloon to burst last?'**",
        "bottleneck": "The standard 'burst first' thinking leads to coupled subproblems. If we burst `nums[i]`, the subproblems `(0, i-1)` and `(i+1, n-1)` are not independent, because a future burst in the left part might need a boundary from the right part.",
        "optimized_approach": "Let's focus on the **last** balloon to be burst within an interval `(i, j)`. Suppose this last balloon is `k`. When we burst `k`, all other balloons between `i` and `j` are already gone. This means its neighbors must be `i` and `j`. The coins gained from this final burst are `nums[i] * nums[k] * nums[j]`.\n\nThe total score is then `(max score from interval (i, k)) + (max score from interval (k, j)) + nums[i] * nums[k] * nums[j]`. The subproblems `(i, k)` and `(k, j)` are now independent! This is a pattern known as **Interval DP**.",
        "algorithm_steps": "1.  **Preprocessing:** Pad the `nums` array with a `1` at both ends. This handles the edge cases gracefully. Let the new length be `n`.\n2.  **Initialization:** Create a 2D `dp` table of size `n x n`, where `dp[i][j]` will store the max coins from bursting balloons in the open interval `(i, j)`.\n3.  **Main Loop (Interval DP):**\n    a.  Iterate over the interval length `len`, from 2 up to `n-1`.\n    b.  Iterate over the start index `i`, from 0 up to `n - 1 - len`.\n    c.  The end index `j` will be `i + len`.\n    d.  For our interval `(i, j)`, iterate through every possible last balloon `k` where `i < k < j`.\n    e.  Apply the transition: `dp[i][j] = max(dp[i][j], dp[i][k] + dp[k][j] + nums[i] * nums[k] * nums[j])`.\n\n4.  **Result:** The answer for the entire range `(0, n-1)` will be stored in `dp[0][n-1]`."
      },
      "code": "function maxCoins(nums) {\n  const n = nums.length;\n  nums = [1, ...nums, 1];\n  const dp = Array(n+2).fill().map(() => Array(n+2).fill(0));\n  for (let len = 1; len <= n; len++) {\n    for (let i = 1; i + len - 1 <= n; i++) {\n      const j = i + len - 1;\n      for (let k = i; k <= j; k++) {\n        dp[i][j] = Math.max(dp[i][j], nums[i-1]*nums[k]*nums[j+1] + dp[i][k-1] + dp[k+1][j]);\n      }\n    }\n  }\n  return dp[1][n];\n}",
      "complexity": {
        "time": "O(N³)",
        "space": "O(N²)",
        "explanation_time": "Let N be the number of balloons in the original array. We have three nested loops to fill the DP table: one for the interval length, one for the start position, and one for the 'last balloon' `k`. Each of these loops runs approximately N times, leading to a cubic time complexity of O(N³).",
        "explanation_space": "We use a 2D DP table of size roughly (N+2) x (N+2) to store the results of the subproblems. This requires O(N²) space."
      },
      "annotations": [
        {
          "lines": [
            2,
            3
          ],
          "text": "Pad array with 1s at both ends to handle boundary cases. Original balloon indices become 1 to n"
        },
        {
          "lines": [
            4
          ],
          "text": "Create 2D DP table: dp[i][j] = max coins from bursting balloons in range [i, j]"
        },
        {
          "lines": [
            5
          ],
          "text": "Outer loop: iterate by interval length (smaller intervals first - bottom-up)"
        },
        {
          "lines": [
            6,
            7
          ],
          "text": "Middle loop: iterate over all possible start positions for current interval length"
        },
        {
          "lines": [
            8,
            9
          ],
          "text": "Inner loop: try each balloon k as the LAST to burst in interval. Key insight: when k is last, its neighbors are i-1 and j+1"
        },
        {
          "lines": [
            13
          ],
          "text": "Answer: max coins from bursting all balloons (range 1 to n)"
        }
      ],
      "quizzes": [
        {
          "question": "What DP pattern does Burst Balloons use?",
          "options": [
            "Linear DP",
            "Interval DP",
            "Tree DP",
            "Digit DP"
          ],
          "correct": 1
        },
        {
          "question": "What's the key insight for this problem?",
          "options": [
            "Burst smallest first",
            "Think about which to burst LAST",
            "Use greedy",
            "Sort balloons"
          ],
          "correct": 1
        },
        {
          "question": "Why think about 'last balloon' instead of 'first'?",
          "options": [
            "Faster execution",
            "Subproblems become independent",
            "Easier to code",
            "Less memory"
          ],
          "correct": 1
        },
        {
          "question": "What does dp[i][j] represent?",
          "options": [
            "Min coins",
            "Max coins in interval (i,j)",
            "Balloon count",
            "Burst order"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity?",
          "options": [
            "O(N)",
            "O(N²)",
            "O(N³)",
            "O(2^N)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "dp-5",
      "title": "Regular Expression Matching",
      "difficulty": "Medium",
      "tags": [
        "dp",
        "regex",
        "strings"
      ],
      "problem_statement": "Given an input string `s` and a pattern `p`, implement regular expression matching with support for `'.'` and `'*'` where:\n\n*   `'.'` Matches any single character.​​​​\n*   `'*'` Matches zero or more of the preceding element.\n\nThe matching should cover the **entire** input string (not partial).\n\n**Example 1:**\n```\nInput: s = \"aa\", p = \"a\"\nOutput: false\nExplanation: \"a\" does not match the entire string \"aa\".\n```\n\n**Example 2:**\n```\nInput: s = \"ab\", p = \".*\"\nOutput: true\nExplanation: \".*\" means \"zero or more (*) of any character (.)\".\n```",
      "explanation": {
        "understanding_the_problem": "This is a complex matching problem due to the special rules for '.' and especially '*'. The '*' character's ability to match 'zero or more' times is a strong indicator that we need to explore multiple possibilities, which makes it a great fit for Dynamic Programming.",
        "brute_force": "We can define a function `isMatch(s_idx, p_idx)` that checks if the substring of `s` starting at `s_idx` matches the sub-pattern of `p` starting at `p_idx`. This leads to a recursive structure.\n\n- If `p[p_idx+1]` is `'*'`, we have two choices: either we match zero instances of `p[p_idx]` and check `isMatch(s_idx, p_idx+2)`, or we match one instance (if `s[s_idx]` matches `p[p_idx]`) and check `isMatch(s_idx+1, p_idx)`.\n- Otherwise, we check if the current characters match and recurse on `isMatch(s_idx+1, p_idx+1)`.",
        "bottleneck": "A pure recursive solution would be very slow due to re-computing the same subproblems (e.g., `isMatch(5, 6)`) through different recursive paths. This is the classic 'overlapping subproblems' issue that DP solves.",
        "optimized_approach": "We use a 2D DP table. Let `dp[i][j]` be a boolean that is `true` if the first `i` characters of string `s` can be matched by the first `j` characters of pattern `p`.\n\nOur goal is to compute `dp[s.length][p.length]`. We fill the table based on the character `p[j-1]`:\n\n- **If `p[j-1]` is not `'*'`:** A match occurs only if the current characters match (`s[i-1] == p[j-1]` or `p[j-1] == '.'`) AND the previous prefixes matched (`dp[i-1][j-1]` was true).\n\n- **If `p[j-1]` is `'*'`:** This is the complex case. The `*` and its preceding character `p[j-2]` can be treated in two ways:\n    1.  **It matches zero elements:** In this case, we effectively ignore `p[j-2]*`. The result is simply `dp[i][j-2]`.\n    2.  **It matches one or more elements:** This is only possible if the current string character `s[i-1]` matches the pattern character `p[j-2]`. If it does, we are essentially extending the match for `*`, so the result depends on `dp[i-1][j]` (we consume a character from `s` but stay at the same pattern position `j` to allow for more matches by `*`).\n\n`dp[i][j]` is true if either of these conditions is met.",
        "algorithm_steps": "1.  **Initialization:**\n    a.  Create a `dp` table of size `(s.length + 1) x (p.length + 1)`.\n    b.  `dp[0][0] = true` (empty string matches empty pattern).\n    c.  Initialize the first row: `dp[0][j]` is true only if `p` can form an empty string. This happens for patterns like `a*`, `a*b*`, etc. So, if `p[j-1] == '*'`, then `dp[0][j] = dp[0][j-2]`.\n\n2.  **Main Loop:**\n    a.  Iterate `i` from 1 to `s.length`.\n    b.  Iterate `j` from 1 to `p.length`.\n    c.  Apply the state transition logic described above for when `p[j-1]` is `'*'` or not.\n\n3.  **Result:** Return `dp[s.length][p.length]`."
      },
      "code": "function isMatch(s, p) {\n  const m = s.length, n = p.length;\n  const dp = Array(m+1).fill().map(() => Array(n+1).fill(false));\n  dp[0][0] = true;\n  for (let j = 1; j <= n; j++) if (p[j-1] === '*') dp[0][j] = dp[0][j-2];\n  for (let i = 1; i <= m; i++) {\n    for (let j = 1; j <= n; j++) {\n      if (p[j-1] === '*') {\n        dp[i][j] = dp[i][j-2] || (matches(p[j-2], s[i-1]) && (dp[i-1][j] || dp[i][j-1]));\n      } else {\n        dp[i][j] = matches(p[j-1], s[i-1]) && dp[i-1][j-1];\n      }\n    }\n  }\n  return dp[m][n];\n}\nfunction matches(c1, c2) { return c1 === '.' || c1 === c2; }",
      "complexity": {
        "time": "O(M * N)",
        "space": "O(M * N)",
        "explanation_time": "Let M be the length of the string and N be the length of the pattern. We fill a 2D DP table of size M x N. Calculating the value for each cell takes constant time. This results in a time complexity of O(M * N).",
        "explanation_space": "We use a 2D DP table of size (M+1) x (N+1) to store the results of all subproblems. This requires O(M * N) space."
      },
      "annotations": [
        {
          "lines": [
            2,
            3
          ],
          "text": "Create 2D boolean DP table: dp[i][j] = true if s[0..i-1] matches p[0..j-1]"
        },
        {
          "lines": [
            4
          ],
          "text": "Base case: empty string matches empty pattern"
        },
        {
          "lines": [
            5
          ],
          "text": "Initialize first row: handle patterns like a*, a*b* that can match empty string (via * matching zero elements)"
        },
        {
          "lines": [
            6,
            7
          ],
          "text": "Main DP loops: fill table for all (i, j) combinations"
        },
        {
          "lines": [
            8,
            9
          ],
          "text": "Case 1 - Star: match zero (dp[i][j-2]) OR match one+ if char matches (dp[i-1][j])"
        },
        {
          "lines": [
            10,
            11
          ],
          "text": "Case 2 - No star: simple match. Current chars must match AND previous prefixes must match"
        },
        {
          "lines": [
            15
          ],
          "text": "Return whether entire string matches entire pattern"
        },
        {
          "lines": [
            17
          ],
          "text": "Helper: '.' matches any character, otherwise must be exact match"
        }
      ],
      "quizzes": [
        {
          "question": "What does dp[i][j] represent?",
          "options": [
            "Match count",
            "s[0..i-1] matches p[0..j-1]?",
            "Pattern length",
            "String index"
          ],
          "correct": 1
        },
        {
          "question": "How does '*' work in patterns?",
          "options": [
            "Match any char",
            "Match zero or more of preceding",
            "Match one char",
            "End of pattern"
          ],
          "correct": 1
        },
        {
          "question": "What does '.' match?",
          "options": [
            "Period only",
            "Any single character",
            "Zero chars",
            "Whitespace"
          ],
          "correct": 1
        },
        {
          "question": "When can pattern a* match empty string?",
          "options": [
            "Never",
            "When * matches zero elements",
            "Only if a exists",
            "Always"
          ],
          "correct": 1
        },
        {
          "question": "What's the complexity of this DP solution?",
          "options": [
            "O(N)",
            "O(N²)",
            "O(M × N)",
            "O(2^N)"
          ],
          "correct": 2
        }
      ]
    }
  ],
  "graphs": [
    {
      "id": "graphs-1",
      "title": "Word Ladder (Shortest Path)",
      "difficulty": "Medium",
      "tags": [
        "bfs",
        "shortest-path",
        "strings"
      ],
      "follow_up": {
        "scenario": "You need to find a path in a graph with millions of nodes (e.g., Social Network connections).",
        "trade_off": "Standard BFS explores too many nodes (exponential growth of frontier).",
        "strategy": "Bi-directional BFS. Start from both the source and destination simultaneously. Reduces time complexity from O(b^d) to O(b^(d/2)).",
        "answering_guide": "Mentioning <strong>'Bi-directional BFS'</strong> is the key differentiator between L4 and L5 here. Explain <em>why</em> it's faster (branching factor)."
      },
      "problem_statement": "A **transformation sequence** from word `beginWord` to word `endWord` using a dictionary `wordList` is a sequence of words `beginWord -> s1 -> s2 -> ... -> sk` such that:\n\n1.  Every adjacent pair of words differs by a single letter.\n2.  Every `si` for `1 <= i <= k` is in `wordList`. Note that `beginWord` does not need to be in `wordList`.\n3.  `sk == endWord`\n\nGiven two words, `beginWord` and `endWord`, and a dictionary `wordList`, return the **number of words** in the **shortest transformation sequence** from `beginWord` to `endWord`, or `0` if no such sequence exists.\n\n**Example 1:**\n```\nInput: beginWord = \"hit\", endWord = \"cog\", wordList = [\"hot\",\"dot\",\"dog\",\"lot\",\"log\",\"cog\"]\nOutput: 5\nExplanation: One shortest transformation sequence is \"hit\" -> \"hot\" -> \"dot\" -> \"dog\" -> \"cog\", which is 5 words long.\n```",
      "diagram": "graph TD\n    hit --> hot\n    hit --> dot\n    hot --> dot\n    hot --> lot\n    dot --> lot\n    dot --> dog\n    lot --> log\n    dog --> log\n    dog --> cog\n    log --> cog\n    style hit fill:#4ade80,stroke:#333,stroke-width:2px,color:#000\n    style cog fill:#f87171,stroke:#333,stroke-width:2px,color:#000\n    linkStyle default stroke:#a1a1aa,stroke-width:2px",
      "explanation": {
        "understanding_the_problem": "We are asked to find the *shortest* sequence of transformations to get from a starting word to an ending word. The only valid transformation is changing a single letter, and the resulting word must be in a given dictionary. This problem can be perfectly modeled as finding the shortest path in a graph.",
        "brute_force": "A simple way to visualize this is as a graph where every word is a node. An edge exists between two word-nodes if they are one letter apart. Our task is to find the shortest path from the `beginWord` node to the `endWord` node.\n\nA Depth-First Search (DFS) could explore paths. We'd start from the `beginWord`, find all connected words (one letter different), and recursively explore each of them, keeping track of the path length. We would need a `visited` set to avoid cycles (e.g., `hit` -> `hot` -> `hit`).",
        "bottleneck": "DFS explores paths deeply. It might follow a very long, winding path of transformations to its end before it even considers a different, much shorter path. To find the shortest path, DFS would have to explore *every possible path* between `beginWord` and `endWord`, which is highly inefficient.",
        "optimized_approach": "The problem asks for the **shortest path** in what is effectively an **unweighted graph** (each transformation costs 1). This is the classic use case for **Breadth-First Search (BFS)**.\n\nBFS explores the graph layer by layer. It starts at `beginWord` (layer 0), then finds all words that are 1 change away (layer 1), then all words 2 changes away (layer 2), and so on. Because of this layer-by-layer exploration, the very first time BFS reaches the `endWord`, it is guaranteed to have done so via a shortest possible path.",
        "algorithm_steps": "1.  **Preprocessing:** Add all words from the `wordList` to a `Set` for efficient O(1) lookups. This set will also double as our 'unvisited' set.\n2.  **Initialization:** Create a `Queue` and add the starting state: `[beginWord, 1]`, representing the word and the current path length.\n3.  **BFS Loop:** While the queue is not empty:\n    a.  Dequeue the current state: `[currentWord, currentLength]`.\n    b.  If `currentWord` is the `endWord`, we have found the shortest path. Return `currentLength`.\n    c.  **Generate Neighbors:** Find all possible next words by iterating through each character of the `currentWord` and replacing it with every letter from 'a' to 'z'.\n    d.  **Check and Enqueue:** For each `newWord` generated:\n        i.  If the `newWord` exists in our word set, it's a valid transformation.\n        ii. Add the new state `[newWord, currentLength + 1]` to the queue.\n        iii. **Crucially, remove the `newWord` from the word set.** This marks it as 'visited', preventing us from processing it again and getting stuck in cycles.\n4.  **No Path:** If the queue becomes empty and we never reached `endWord`, no such path exists. Return 0."
      },
      "code": "function ladderLength(beginWord, endWord, wordList) {\n  const wordSet = new Set(wordList);\n  if (!wordSet.has(endWord)) return 0;\n  const queue = [[beginWord, 1]];\n  wordSet.delete(beginWord);\n  while (queue.length) {\n    const [word, steps] = queue.shift();\n    if (word === endWord) return steps;\n    for (let i = 0; i < word.length; i++) {\n      for (let c = 0; c < 26; c++) {\n        const newWord = word.slice(0,i) + String.fromCharCode(97+c) + word.slice(i+1);\n        if (wordSet.has(newWord)) {\n          queue.push([newWord, steps+1]);\n          wordSet.delete(newWord);\n        }\n      }\n    }\n  }\n  return 0;\n}",
      "complexity": {
        "time": "O(N * L²)",
        "space": "O(N * L)",
        "explanation_time": "Let N be the number of words in the list and L be the length of the words. In the worst case, we visit every word (N). For each word, we iterate through its length (L) and try all 26 alphabet characters. In JavaScript, creating the `newWord` by slicing and concatenating strings takes O(L) time. This results in a complexity of O(N * L * 26 * L), which simplifies to O(N * L²).",
        "explanation_space": "The `wordSet` stores N words, each of average length L, contributing O(N * L) to space. The queue, in the worst case, could also hold a large fraction of the words, also contributing to the O(N * L) space complexity."
      },
      "annotations": [
        {
          "lines": [
            2
          ],
          "text": "Create a Set for O(1) lookups of valid words. Using Array would be too slow (O(N))."
        },
        {
          "lines": [
            4
          ],
          "text": "Initialize BFS queue with [current_word, level]. Level starts at 1."
        },
        {
          "lines": [
            5
          ],
          "text": "Mark start word as visited immediately to prevent cycles."
        },
        {
          "lines": [
            6
          ],
          "text": "Standard BFS loop: process nodes level by level."
        },
        {
          "lines": [
            9,
            10
          ],
          "text": "Try changing every character (i) to every letter (a-z) to find neighbors."
        },
        {
          "lines": [
            12
          ],
          "text": "If neighbor is in dictionary (and not visited), it's a valid next step."
        },
        {
          "lines": [
            14
          ],
          "text": "Crucial: Remove from set immediately to mark as visited. This guarantees shortest path."
        }
      ],
      "quizzes": [
        {
          "question": "How can we model this problem?",
          "options": [
            "Tree",
            "Graph with edges between similar words",
            "Array",
            "Stack"
          ],
          "correct": 1
        },
        {
          "question": "Why is DFS inefficient for finding shortest path?",
          "options": [
            "Too much memory",
            "Explores long paths first",
            "Can't handle cycles",
            "Too slow"
          ],
          "correct": 1
        },
        {
          "question": "Which algorithm finds shortest path in unweighted graphs?",
          "options": [
            "DFS",
            "Dijkstra",
            "BFS",
            "Bellman-Ford"
          ],
          "correct": 2
        },
        {
          "question": "Why use a Set instead of Array for the word list?",
          "options": [
            "Smaller memory",
            "O(1) lookups",
            "Easier to iterate",
            "Maintains order"
          ],
          "correct": 1
        },
        {
          "question": "What makes BFS guarantee shortest path?",
          "options": [
            "Random exploration",
            "Layer-by-layer exploration",
            "Depth priority",
            "Heuristics"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "graphs-2",
      "title": "Course Schedule II (Topological Sort)",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/course-schedule-ii/",
      "related": [
        {
          "id": "graphs-1",
          "title": "Word Ladder",
          "category": "graphs"
        },
        {
          "id": "graphs-3",
          "title": "Network Delay Time",
          "category": "graphs"
        }
      ],
      "tags": [
        "topological-sort",
        "bfs",
        "dag"
      ],
      "follow_up": {
        "scenario": "You are building a distributed build system (like Bazel) with thousands of targets.",
        "trade_off": "Serial execution is too slow. Finding a valid order isn't enough; we need speed.",
        "strategy": "Parallel Topological Sort (Kahn's with a Thread Pool). Process all nodes with `in-degree 0` in parallel. As tasks finish, decrement neighbors.",
        "answering_guide": "Connect this to <strong>'Build Systems'</strong>. It shows you understand practical applications of DAGs beyond just abstract graphs."
      },
      "problem_statement": "There are a total of `numCourses` courses you have to take, labeled from `0` to `numCourses - 1`. You are given an array `prerequisites` where `prerequisites[i] = [ai, bi]` indicates that you **must** take course `bi` first if you want to take course `ai`.\n\nReturn the ordering of courses you should take to finish all courses. If there are many valid answers, return **any** of them. If it is impossible to finish all courses, return **an empty array**.\n\n**Example 1:**\n```\nInput: numCourses = 4, prerequisites = [[1,0],[2,0],[3,1],[3,2]]\nOutput: [0,2,1,3]\nExplanation: There are a total of 4 courses to take. To take course 3 you should have finished both courses 1 and 2. Both courses 1 and 2 should be taken after you finished course 0.\nSo one correct course order is [0,1,2,3]. Another correct ordering is [0,2,1,3].\n```",
      "diagram": "graph TD\n    0((Course 0)) --> 1((Course 1))\n    0 --> 2((Course 2))\n    1 --> 3((Course 3))\n    2 --> 3\n    \n    classDef default fill:#fff,stroke:#333,stroke-width:2px;\n    classDef source fill:#dcfce7,stroke:#166534,stroke-width:2px; \n    classDef sink fill:#fee2e2,stroke:#991b1b,stroke-width:2px;\n    \n    class 0 source;\n    class 3 sink;",
      "explanation": {
        "understanding_the_problem": "This is a classic scheduling problem. We have a set of tasks (courses) and dependencies (prerequisites). The rule `[a, b]` means `b` must be completed before `a`. We need to find a linear sequence of all courses that respects these rules.\n\nThis problem is equivalent to finding a **Topological Sort** of a directed graph. A cycle in the prerequisites (e.g., you need course 1 for course 2, and course 2 for course 1) makes it impossible to finish.",
        "brute_force": "The problem can be modeled as a graph where courses are nodes and prerequisites are directed edges. For a prerequisite `[a, b]`, we draw an edge `b -> a`.\n\nA topological sort is a linear ordering of nodes where for every edge `u -> v`, node `u` comes before node `v`. The core idea is to always find a node with no incoming edges—this is a course with no prerequisites. We can place this course in our schedule.\n\nOnce we 'take' this course, we can remove it and its outgoing edges from the graph. This might create new nodes that now have zero incoming edges. We repeat this process until all courses are taken.",
        "bottleneck": "While the concept is sound, repeatedly scanning the graph to find a new node with zero in-degree after each removal is inefficient. We can do better by keeping track of the in-degrees as they change.",
        "optimized_approach": "A more efficient method is **Kahn's Algorithm**, which uses a BFS-like approach. It works by keeping track of the 'in-degree' of each node (the number of prerequisites it has). The algorithm processes nodes with an in-degree of 0, and as it 'completes' them, it decrements the in-degree of all subsequent courses, adding new zero-in-degree courses to a queue to be processed.",
        "algorithm_steps": "1.  **Build Graph and In-Degrees:**\n    a.  Create an adjacency list to represent the course dependencies (`graph`).\n    b.  Create an `in_degree` array, initialized to zeros, to count prerequisites for each course.\n    c.  For each prerequisite `[course, prereq]`, add an edge `prereq -> course` to the graph and increment `in_degree[course]`.\n\n2.  **Initialize Queue:**\n    a.  Create a queue.\n    b.  Add all courses with an in-degree of 0 to the queue. These are the courses with no prerequisites.\n\n3.  **Process Courses:**\n    a.  Initialize an empty list, `topological_order`, to store the result.\n    b.  While the queue is not empty, dequeue a course `u`.\n    c.  Add `u` to `topological_order`.\n    d.  For each neighbor `v` of `u` (i.e., for each course that depends on `u`):\n        i.  Decrement `in_degree[v]`.\n        ii. If `in_degree[v]` becomes 0, add `v` to the queue.\n\n4.  **Check for Cycles:**\n    a.  If `topological_order` contains `numCourses` elements, a valid ordering was found. Return it.\n    b.  Otherwise, the graph had a cycle, and not all courses could be processed. Return an empty array."
      },
      "code": "function findOrder(numCourses, prerequisites) {\n  const graph = Array(numCourses).fill().map(() => []);\n  const indegree = new Array(numCourses).fill(0);\n  for (let [course, pre] of prerequisites) {\n    graph[pre].push(course);\n    indegree[course]++;\n  }\n  const queue = [];\n  for (let i = 0; i < numCourses; i++) if (!indegree[i]) queue.push(i);\n  const res = [];\n  while (queue.length) {\n    const course = queue.shift();\n    res.push(course);\n    for (let nei of graph[course]) {\n      if (--indegree[nei] === 0) queue.push(nei);\n    }\n  }\n  return res.length === numCourses ? res : [];\n}",
      "complexity": {
        "time": "O(V + E)",
        "space": "O(V + E)",
        "explanation_time": "Let V be the number of courses and E be the number of prerequisites. Building the graph and in-degree map takes O(E) time. The initial scan for zero-in-degree nodes takes O(V). The main BFS loop processes each vertex and each edge exactly once, which takes O(V + E). The total time complexity is therefore O(V + E).",
        "explanation_space": "The space is dominated by the data structures used. The adjacency list (`graph`) requires O(V + E) space. The `indegree` array and the `queue` both require O(V) space. Therefore, the total space complexity is O(V + E)."
      },
      "annotations": [
        {
          "lines": [
            2
          ],
          "text": "Create Adjacency List to represent the graph (course dependencies)."
        },
        {
          "lines": [
            3
          ],
          "text": "Create In-degree array: counts how many prerequisites each course has."
        },
        {
          "lines": [
            4,
            5,
            6,
            7
          ],
          "text": "Build graph: Pre -> Course. Increment in-degree for the dependent course."
        },
        {
          "lines": [
            9
          ],
          "text": "Kahn's Algorithm: Initialize queue with courses having 0 prerequisites (ready to take)."
        },
        {
          "lines": [
            11
          ],
          "text": "Process courses in queue (BFS-like approach)."
        },
        {
          "lines": [
            14
          ],
          "text": "Decrement in-degree of neighbors. If 0, all prereqs are met -> add to queue."
        },
        {
          "lines": [
            17
          ],
          "text": "If result length equals numCourses, we found a valid order. Otherwise, there's a cycle."
        }
      ],
      "quizzes": [
        {
          "question": "What algorithm solves task scheduling with dependencies?",
          "options": [
            "BFS",
            "Topological Sort",
            "Dijkstra",
            "Binary Search"
          ],
          "correct": 1
        },
        {
          "question": "What does 'in-degree' of a node mean?",
          "options": [
            "Outgoing edges",
            "Number of prerequisites",
            "Node weight",
            "Distance from source"
          ],
          "correct": 1
        },
        {
          "question": "Which data structure does Kahn's algorithm use?",
          "options": [
            "Stack",
            "Queue",
            "Heap",
            "Tree"
          ],
          "correct": 1
        },
        {
          "question": "How do we detect a cycle in this approach?",
          "options": [
            "DFS recursion",
            "Result length < numCourses",
            "Stack overflow",
            "Negative edges"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity?",
          "options": [
            "O(N)",
            "O(N²)",
            "O(V + E)",
            "O(N log N)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "graphs-3",
      "title": "Network Delay Time (Dijkstra)",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/network-delay-time/",
      "related": [
        {
          "id": "graphs-2",
          "title": "Course Schedule II",
          "category": "graphs"
        },
        {
          "id": "graphs-4",
          "title": "Min Cost to Connect Points",
          "category": "graphs"
        }
      ],
      "tags": [
        "dijkstra",
        "shortest-path",
        "priority-queue"
      ],
      "problem_statement": "You are given a network of `n` nodes, labeled from `1` to `n`. You are also given `times`, a list of travel times as directed edges `times[i] = [u, v, w]`, where `u` is the source node, `v` is the target node, and `w` is the time it takes for a signal to travel from source to target.\n\nWe will send a signal from a given node `k`. Return the time it takes for all `n` nodes to receive the signal. If it is impossible for all `n` nodes to receive the signal, return `-1`.\n\n**Example 1:**\n```\nInput: times = [[2,1,1],[2,3,1],[3,4,1]], n = 4, k = 2\nOutput: 2\n```",
      "diagram": "graph LR\n    2((2)) -->|1| 1((1))\n    2 -->|1| 3((3))\n    3 -->|1| 4((4))\n    \n    classDef default fill:#fff,stroke:#333,stroke-width:2px;\n    classDef source fill:#dcfce7,stroke:#166534,stroke-width:2px;\n    \n    class 2 source;",
      "explanation": {
        "understanding_the_problem": "We have a network of nodes with weighted directed connections (travel times). A signal starts at a source node `K` and propagates through the network. The time for the signal to reach any given node is the shortest path time from `K` to that node.\n\nThe problem asks for the time it takes for the *entire network* to get the signal. This means we must find the shortest path from `K` to all other nodes, and the answer will be the *longest* of these shortest paths. If any node is unreachable, its shortest path time will be infinity, and it's impossible to alert the whole network.",
        "brute_force": "This is a classic **single-source shortest path** problem on a weighted, directed graph. The nodes are the network nodes, the edges are the travel times, and the source is `K`.\n\nSince the edge weights (times) are non-negative, this is a perfect scenario for **Dijkstra's Algorithm**. Dijkstra's is a greedy algorithm that finds the shortest paths from a single source to all other nodes in a graph.",
        "bottleneck": "A naive implementation of Dijkstra's might repeatedly scan all nodes to find the unvisited node with the smallest current distance. This would be slow, around O(V²). The key to an efficient implementation is using a data structure that can quickly provide the node with the minimum distance.",
        "optimized_approach": "The standard and efficient way to implement Dijkstra's is by using a **Min-Priority Queue**. The priority queue will store pairs of `[time, node]`, always ordered by the smallest time. This allows us to greedily and efficiently select the next node to visit—the one that is closest to the source among all nodes we've reached but not yet finalized.",
        "algorithm_steps": "1.  **Build the Graph:** Create an adjacency list where `graph[u]` stores a list of its neighbors and the corresponding travel times, like `[v, w]`.\n2.  **Initialization:**\n    a.  Create a `distances` array to store the shortest time from `K` to every node. Initialize all values to `Infinity` and set `distances[K] = 0`.\n    b.  Push the starting state `[0, K]` (representing `[time, node]`) into a min-priority queue.\n3.  **Process Nodes (Dijkstra's Loop):**\n    a.  While the priority queue is not empty, extract the state with the smallest time: `[currentTime, currentNode]`.\n    b.  If `currentTime` is already greater than `distances[currentNode]`, it's a stale, longer path. Ignore it and continue.\n    c.  For each neighbor `[neighborNode, travelTime]` of `currentNode`:\n        i.  If `currentTime + travelTime < distances[neighborNode]`, we've found a shorter path.\n        ii. Update `distances[neighborNode]` to this new, shorter time and push `[newTime, neighborNode]` to the priority queue.\n4.  **Find the Result:**\n    a.  After the loop, find the maximum value in the `distances` array.\n    b.  If this max value is `Infinity`, at least one node was unreachable, so return -1. Otherwise, return the max value."
      },
      "code": "function networkDelayTime(times, N, K) {\n  const adj = Array(N+1).fill().map(() => []);\n  for (let [u,v,w] of times) {\n    adj[u].push([v,w]);\n  }\n  const dist = new Array(N+1).fill(Infinity);\n  dist[K] = 0;\n  const pq = [[0, K]];\n  while (pq.length) {\n    const [d, u] = pq.shift();\n    if (d > dist[u]) continue;\n    for (let [v,w] of adj[u]) {\n      if (dist[v] > d + w) {\n        dist[v] = d + w;\n        pq.push([dist[v], v]);\n        pq.sort((a,b) => a[0] - b[0]);\n      }\n    }\n  }\n  const maxTime = Math.max(...dist.slice(1));\n  return maxTime === Infinity ? -1 : maxTime;\n}",
      "complexity": {
        "time": "O(E log V)",
        "space": "O(V + E)",
        "explanation_time": "Let V be the number of nodes and E be the number of edges. The complexity is dominated by the priority queue operations. In the worst case, we may add an entry to the priority queue for every edge in the graph. Each push or pop operation on a min-priority queue takes O(log V) time. Therefore, the total time complexity is O(E log V). (Note: The sample code simulates a PQ by sorting an array, which is less efficient than a true heap-based priority queue).",
        "explanation_space": "The adjacency list requires O(V + E) space. The `dist` array requires O(V) space, and the priority queue can hold up to V nodes, requiring O(V) space. This results in a total space complexity of O(V + E)."
      },
      "annotations": [
        {
          "lines": [
            2,
            3,
            4
          ],
          "text": "Build adjacency list: Node -> [[Neighbor, Weight]]."
        },
        {
          "lines": [
            6
          ],
          "text": "Initialize distances to Infinity. Source node distance is 0."
        },
        {
          "lines": [
            8
          ],
          "text": "Min-Priority Queue: stores [time, node]. Simulating with array + sort."
        },
        {
          "lines": [
            10
          ],
          "text": "Dijkstra's main loop: process closest node first."
        },
        {
          "lines": [
            11
          ],
          "text": "Optimization: If current path is longer than already found shortest path, skip."
        },
        {
          "lines": [
            13
          ],
          "text": "Relaxation: If path through u to v is shorter than current known path to v, update it."
        },
        {
          "lines": [
            15,
            16
          ],
          "text": "Push new shortest path to PQ. Sort to maintain min-heap property (simplification)."
        }
      ],
      "quizzes": [
        {
          "question": "What algorithm finds single-source shortest paths with non-negative weights?",
          "options": [
            "BFS",
            "DFS",
            "Dijkstra",
            "Bellman-Ford"
          ],
          "correct": 2
        },
        {
          "question": "What data structure makes Dijkstra efficient?",
          "options": [
            "Stack",
            "Array",
            "Min-Priority Queue",
            "Hash Table"
          ],
          "correct": 2
        },
        {
          "question": "Why can't Dijkstra handle negative edge weights?",
          "options": [
            "Memory overflow",
            "Greedy assumption breaks",
            "Too slow",
            "Can't represent them"
          ],
          "correct": 1
        },
        {
          "question": "What is 'relaxation' in Dijkstra?",
          "options": [
            "Removing edges",
            "Updating shorter path distance",
            "Adding nodes",
            "Sorting edges"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity with a heap?",
          "options": [
            "O(V)",
            "O(V²)",
            "O(E log V)",
            "O(V³)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "graphs-4",
      "title": "Min Cost to Connect Points (Prim's MST)",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/min-cost-to-connect-all-points/",
      "related": [
        {
          "id": "graphs-3",
          "title": "Network Delay Time",
          "category": "graphs"
        },
        {
          "id": "graphs-5",
          "title": "Pacific Atlantic Water Flow",
          "category": "graphs"
        }
      ],
      "tags": [
        "mst",
        "prim",
        "greedy"
      ],
      "problem_statement": "You are given an array `points` representing integer coordinates of some points on a 2D-plane, where `points[i] = [xi, yi]`.\n\nThe cost of connecting two points `[xi, yi]` and `[xj, yj]` is the **Manhattan distance** between them: `|xi - xj| + |yi - yj|`.\n\nReturn the minimum cost to make all points connected. All points are connected if there is exactly one simple path between any two points.\n\n**Example 1:**\n```\nInput: points = [[0,0],[2,2],[3,10],[5,2],[7,0]]\nOutput: 20\n```",
      "diagram": "graph TD\n    0(\"[0,0]\") ---|4| 1(\"[2,2]\")\n    1 ---|3| 3(\"[5,2]\")\n    3 ---|4| 4(\"[7,0]\")\n    1 ---|9| 2(\"[3,10]\")\n    \n    classDef default fill:#fff,stroke:#333,stroke-width:2px;\n    classDef mst fill:#dcfce7,stroke:#166534,stroke-width:2px;\n    \n    class 0,1,2,3,4 mst;",
      "explanation": {
        "understanding_the_problem": "We are given a set of points and asked to connect all of them with the minimum possible total cost. The rule that 'there is exactly one simple path between any two points' is the definition of a **tree**.\n\nSo, the problem is asking us to find a **Minimum Spanning Tree (MST)**. We can imagine this as a complete graph where every point is a node, and an edge exists between every pair of points, weighted by their Manhattan distance.",
        "brute_force": "There are two primary greedy algorithms for finding an MST: Prim's and Kruskal's.\n\n- **Kruskal's Algorithm:** Sorts all possible edges by weight and adds the smallest ones that don't form a cycle. This is great for sparse graphs.\n- **Prim's Algorithm:** Grows an MST from a single starting node, at each step adding the cheapest edge that connects a node in the MST to a node outside of it.\n\nSince our graph is **dense** (a complete graph where every point is connected to every other), Prim's algorithm is a very natural and efficient choice.",
        "bottleneck": "A naive implementation of Prim's might involve complex data structures. However, a straightforward version is quite elegant and easy to implement, especially when the graph is dense, making it a great approach for this problem.",
        "optimized_approach": "We will use Prim's algorithm. The core idea is to maintain a set of visited nodes (part of our growing MST) and, for every unvisited node, keep track of the minimum cost to connect it to *any* node in the visited set. At each step, we greedily pick the unvisited node with the smallest connection cost, add it to our MST, and then update the connection costs for all remaining unvisited nodes.",
        "algorithm_steps": "1.  **Initialization:**\n    a.  Create a `min_dist` array of size N (number of points), initialized to `Infinity`. This stores the minimum cost to connect each point to the growing MST. Pick a starting point (e.g., node 0) and set its distance to 0.\n    b.  Create a `visited` set to track points already included in the MST.\n    c.  Initialize `total_cost = 0`.\n\n2.  **Main Loop:** Repeat N times:\n    a.  **Select Next Node:** Find the unvisited node `u` that has the smallest value in the `min_dist` array. This is the cheapest point to add to our MST.\n    b.  **Add to MST:** If no such node is found (or its distance is infinity), stop. Otherwise, mark `u` as visited and add its `min_dist` value to `total_cost`.\n    c.  **Update Distances:** For every other unvisited node `v`, calculate the Manhattan distance from `u` to `v`. Update `min_dist[v]` to be the minimum of its current value and this new distance.\n\n3.  **Return Result:** After the loop finishes, `total_cost` will hold the weight of the MST."
      },
      "code": "function minCostConnectPoints(points) {\n  const n = points.length;\n  const visited = new Set();\n  let minCost = 0;\n  const dist = (a,b) => Math.abs(a[0]-b[0]) + Math.abs(a[1]-b[1]);\n  const minDist = new Array(n).fill(Infinity);\n  minDist[0] = 0;\n  for (let i = 0; i < n; i++) {\n    let u = -1;\n    for (let j = 0; j < n; j++) {\n      if (!visited.has(j) && (u === -1 || minDist[j] < minDist[u])) {\n        u = j;\n      }\n    }\n    visited.add(u);\n    minCost += minDist[u];\n    for (let j = 0; j < n; j++) {\n      if (!visited.has(j)) {\n        const d = dist(points[u], points[j]);\n        minDist[j] = Math.min(minDist[j], d);\n      }\n    }\n  }\n  return minCost;\n}",
      "complexity": {
        "time": "O(N²)",
        "space": "O(N)",
        "explanation_time": "Let N be the number of points. The main loop runs N times. Inside this loop, we have two nested loops: one to find the unvisited node with the minimum distance (which takes O(N) time) and another to update the distances to all other unvisited nodes (also O(N)). This results in a total time complexity of O(N * N) = O(N²).",
        "explanation_space": "We use an array `minDist` of size N and a set `visited` which can grow to size N. Both require O(N) space. Therefore, the total space complexity is O(N)."
      },
      "annotations": [
        {
          "lines": [
            4
          ],
          "text": "Helper to calculate Manhattan distance between two points."
        },
        {
          "lines": [
            5,
            6
          ],
          "text": "Prim's Algorithm: Track min cost to connect each node to MST. Start with node 0 (cost 0)."
        },
        {
          "lines": [
            7
          ],
          "text": "Main loop: add N points to the MST one by one."
        },
        {
          "lines": [
            9,
            10,
            11,
            12,
            13
          ],
          "text": "Greedy step: Find unvisited node 'u' with smallest connection cost to current MST."
        },
        {
          "lines": [
            14,
            15
          ],
          "text": "Add 'u' to MST and add its cost to total."
        },
        {
          "lines": [
            16,
            17,
            18,
            19,
            21
          ],
          "text": "Update step: Update connection costs for all remaining unvisited neighbors of 'u'."
        }
      ],
      "quizzes": [
        {
          "question": "What problem does MST solve?",
          "options": [
            "Shortest path",
            "Min cost to connect all nodes",
            "Maximum flow",
            "Cycle detection"
          ],
          "correct": 1
        },
        {
          "question": "Which algorithm grows MST from a single node?",
          "options": [
            "Kruskal's",
            "Prim's",
            "Dijkstra's",
            "Bellman-Ford"
          ],
          "correct": 1
        },
        {
          "question": "What greedy choice does Prim's make?",
          "options": [
            "Longest edge",
            "Cheapest edge to unvisited node",
            "Random edge",
            "Most connected node"
          ],
          "correct": 1
        },
        {
          "question": "What is Manhattan distance?",
          "options": [
            "|x₁-x₂| + |y₁-y₂|",
            "√((x₁-x₂)² + (y₁-y₂)²)",
            "max(|x₁-x₂|, |y₁-y₂|)",
            "min(|x₁-x₂|, |y₁-y₂|)"
          ],
          "correct": 0
        },
        {
          "question": "What's Prim's time complexity for dense graphs?",
          "options": [
            "O(N)",
            "O(N log N)",
            "O(N²)",
            "O(N³)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "graphs-5",
      "title": "Pacific Atlantic Water Flow",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/pacific-atlantic-water-flow/",
      "related": [
        {
          "id": "graphs-4",
          "title": "Min Cost to Connect Points",
          "category": "graphs"
        },
        {
          "id": "graphs-1",
          "title": "Word Ladder",
          "category": "graphs"
        }
      ],
      "tags": [
        "dfs",
        "multi-source",
        "bfs"
      ],
      "problem_statement": "There is an `m x n` rectangular island that borders both the **Pacific Ocean** and **Atlantic Ocean**. The **Pacific Ocean** touches the island's left and top edges, and the **Atlantic Ocean** touches the island's right and bottom edges.\n\nThe island is partitioned into a grid of square cells. You are given an `m x n` integer matrix `heights` where `heights[r][c]` represents the height above sea level of the cell at coordinate `(r, c)`.\n\nThe island receives a lot of rain, and the rain water can flow to neighboring cells directly north, south, east, and west if the neighboring cell's height is **less than or equal to** the current cell's height. Water can flow from any cell adjacent to an ocean into the ocean.\n\nReturn a **2D list** of grid coordinates `result` where `result[i] = [ri, ci]` denotes that rain water can flow from cell `(ri, ci)` to **both** the Pacific and Atlantic oceans.\n\n**Example 1:**\n```\nInput: heights = [[1,2,2,3,5],[3,2,3,4,4],[2,4,5,3,1],[6,7,1,4,5],[5,1,1,2,4]]\nOutput: [[0,4],[1,3],[1,4],[2,2],[3,0],[3,1],[4,0]]\n```",
      "explanation": {
        "understanding_the_problem": "Imagine the matrix is a continent, where each number represents the altitude of that piece of land. The edges of the continent are surrounded by two oceans: the Pacific (top and left edges) and the Atlantic (bottom and right edges).\n\nRain falls on the continent, and water can flow from any cell to a neighboring cell (up, down, left, or right) as long as the neighbor's altitude is the **same or lower**.\n\nOur goal is to find every cell on the continent from which water can flow to **both** the Pacific and the Atlantic oceans.",
        "brute_force": "The most intuitive way to solve this is to check each cell one by one. For every single cell `(r, c)` in the matrix, we could start a traversal (like DFS or BFS) and ask two questions:\n1. Can we reach the Pacific (top or left edge)?\n2. Can we reach the Atlantic (bottom or right edge)?\n\nIf the answer to both is 'yes,' we add the cell to our results.",
        "bottleneck": "This approach is very slow. If the matrix has `M * N` cells, we would be performing a full traversal for each of them. This leads to a time complexity of roughly `O((MN)²)` because we're repeating the same path calculations over and over.",
        "optimized_approach": "The crucial optimization comes from reversing our thinking. Instead of asking 'Where can water from this cell flow *to*?', we should ask:\n\n**'Which cells can be reached by water flowing *from* the oceans?'**\n\nIf we can find all the cells that can reach the Pacific, and all the cells that can reach the Atlantic, the answer is simply the intersection of those two sets.",
        "algorithm_steps": "This insight leads to a much more efficient, two-pass algorithm:\n\n1.  **Create two 'reachable' grids:** We'll use two boolean matrices, `can_reach_pacific` and `can_reach_atlantic`, to keep track of the cells reachable by each ocean.\n2.  **Start from the Pacific:** Begin a traversal (either BFS or DFS) simultaneously from all cells touching the Pacific border. As we traverse *inward*, we can only move from a cell to a neighbor if the neighbor's altitude is **greater than or equal to** our current cell's altitude (since we are flowing 'uphill' from the ocean). Mark every cell we visit in the `can_reach_pacific` grid.\n3.  **Start from the Atlantic:** Do the exact same thing for the Atlantic border, marking visited cells in the `can_reach_atlantic` grid.\n4.  **Find the Intersection:** Iterate through the entire matrix one last time. Any cell `(r, c)` that is marked `true` in **both** `can_reach_pacific` and `can_reach_atlantic` is our answer."
      },
      "code": "function pacificAtlantic(heights) {\n  const m = heights.length, n = heights[0].length;\n  const pacific = new Set(), atlantic = new Set();\n  const dirs = [[0,1],[0,-1],[1,0],[-1,0]];\n  const bfs = (starts, reachable) => {\n    const q = [...starts];\n    for (const [r,c] of starts) reachable.add(`${r},${c}`);\n    while (q.length) {\n      const [r,c] = q.shift();\n      for (const [dr,dc] of dirs) {\n        const nr = r + dr, nc = c + dc;\n        if (nr >= 0 && nr < m && nc >= 0 && nc < n &&\n            !reachable.has(`${nr},${nc}`) &&\n            heights[nr][nc] >= heights[r][c]) {\n          reachable.add(`${nr},${nc}`);\n          q.push([nr,nc]);\n        }\n      }\n    }\n  };\n  const pacificStarts = [];\n  for (let c = 0; c < n; c++) pacificStarts.push([0,c]);\n  for (let r = 0; r < m; r++) pacificStarts.push([r,0]);\n  bfs(pacificStarts, pacific);\n  const atlanticStarts = [];\n  for (let c = 0; c < n; c++) atlanticStarts.push([m-1,c]);\n  for (let r = 0; r < m; r++) atlanticStarts.push([r,n-1]);\n  bfs(atlanticStarts, atlantic);\n  const result = [];\n  for (let r = 0; r < m; r++) {\n    for (let c = 0; c < n; c++) {\n      if (pacific.has(`${r},${c}`) && atlantic.has(`${r},${c}`)) {\n        result.push([r,c]);\n      }\n    }\n  }\n  return result;\n}",
      "complexity": {
        "time": "O(MN)",
        "space": "O(MN)",
        "explanation_time": "The time complexity is O(M * N) because we visit each cell a constant number of times. The first pass (Pacific) visits each reachable cell once, the second pass (Atlantic) does the same, and the final pass iterates through all M*N cells to find the intersection. This gives us a linear time complexity relative to the number of cells.",
        "explanation_space": "The space complexity is O(M * N) because we use two additional boolean matrices/sets (`pacific` and `atlantic`) of the same size as the input grid to store the reachability information. The recursion stack (for DFS) or the queue (for BFS) can also take up to O(M*N) space in the worst-case scenario where all cells are reachable from an ocean."
      },
      "annotations": [
        {
          "lines": [
            3
          ],
          "text": "Use Sets to track reachable cells (storing 'row,col' strings)."
        },
        {
          "lines": [
            5
          ],
          "text": "Helper BFS: starts from ocean borders and flows 'uphill'."
        },
        {
          "lines": [
            6,
            7
          ],
          "text": "Add starting border cells queue and mark as reachable."
        },
        {
          "lines": [
            12,
            13,
            14
          ],
          "text": "Check bounds, not visited, and height condition (neighbor >= current)."
        },
        {
          "lines": [
            21,
            22
          ],
          "text": "Initialize starts: Top row and Left col for Pacific."
        },
        {
          "lines": [
            25,
            26
          ],
          "text": "Initialize starts: Bottom row and Right col for Atlantic."
        },
        {
          "lines": [
            31
          ],
          "text": "Intersection: If cell is in both sets, it can flow to both oceans."
        }
      ],
      "quizzes": [
        {
          "question": "What's the key insight to optimize this problem?",
          "options": [
            "Sort cells",
            "Think from oceans inward",
            "Use DP",
            "Binary search"
          ],
          "correct": 1
        },
        {
          "question": "What traversal technique is used here?",
          "options": [
            "Single-source BFS",
            "Multi-source BFS",
            "Dijkstra",
            "DFS only"
          ],
          "correct": 1
        },
        {
          "question": "How do we find cells that reach both oceans?",
          "options": [
            "Union of sets",
            "Intersection of sets",
            "Difference of sets",
            "XOR of sets"
          ],
          "correct": 1
        },
        {
          "question": "When flowing 'uphill' from ocean, we move to cells with height...?",
          "options": [
            "Lower",
            "Equal or higher",
            "Exactly equal",
            "Random"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity of the optimized solution?",
          "options": [
            "O((MN)²)",
            "O(MN)",
            "O(M + N)",
            "O(M log N)"
          ],
          "correct": 1
        }
      ]
    }
  ],
  "trees": [
    {
      "id": "trees-1",
      "title": "Range Sum Queries (Segment Tree)",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/range-sum-query-mutable/",
      "related": [
        {
          "id": "trees-4",
          "title": "Lazy Propagation",
          "category": "trees"
        },
        {
          "id": "trees-5",
          "title": "Fenwick Tree",
          "category": "trees"
        }
      ],
      "tags": [
        "segment-tree",
        "data-structures"
      ],
      "problem_statement": "Given an integer array `nums`, handle multiple queries of the following types:\n\n1.  **Update**: Update the value of an element in `nums`.\n2.  **Sum Range**: Calculate the sum of the elements of `nums` between indices `left` and `right` **inclusive** where `left <= right`.\n\nImplement the `SegmentTree` class:\n*   `SegmentTree(int[] nums)` Initializes the object with the integer array `nums`.\n*   `void update(int index, int val)` Updates the value of `nums[index]` to be `val`.\n*   `int sumRange(int left, int right)` Returns the sum of the elements of `nums` between indices `left` and `right` inclusive (i.e. `nums[left] + nums[left + 1] + ... + nums[right]`).\n\n**Example 1:**\n```\nInput\n[\"SegmentTree\", \"sumRange\", \"update\", \"sumRange\"]\n[[[1, 3, 5]], [0, 2], [1, 2], [0, 2]]\nOutput\n[null, 9, null, 8]\n\nExplanation\nSegmentTree segmentTree = new SegmentTree([1, 3, 5]);\nsegmentTree.sumRange(0, 2); // return 1 + 3 + 5 = 9\nsegmentTree.update(1, 2);   // nums = [1, 2, 5]\nsegmentTree.sumRange(0, 2); // return 1 + 2 + 5 = 8\n```",
      "explanation": {
        "understanding_the_problem": "A naive approach using a simple array would give us O(1) for updates but O(N) for range sum queries, which is too slow if we have many queries. We need a more advanced data structure that can balance the performance of both operations.",
        "brute_force": "The problem requires a way to handle range queries efficiently. This is the perfect use case for a **Segment Tree**. A Segment Tree is a binary tree where each node represents an interval (or segment) of the original array.\n\n- The **root** represents the entire array `[0, N-1]`.\n- Each **leaf** represents a single element `[i, i]`.\n- Each **internal node** stores aggregate information (in this case, the sum) for the union of its children's intervals.",
        "bottleneck": "The key is that a query for any range can be answered by combining the information from a small number of pre-computed nodes in the tree. Similarly, updating a single element only requires updating the nodes on the path from that leaf to the root.",
        "optimized_approach": "By storing pre-computed sums in the tree's nodes, a Segment Tree allows us to perform both point updates and range sum queries in O(log N) time.\n\n- **Query `(l, r)`:** We traverse the tree. If a node's interval is completely inside our query range, we use its value directly. If it partially overlaps, we recurse on its children. This covers our query range with a logarithmic number of nodes.\n- **Update `(pos, val)`:** We find the leaf corresponding to `pos` and update its value. Then, we walk back up to the root, updating the sum of each parent node along the way. This is a single path of length O(log N).",
        "algorithm_steps": "1.  **Build Tree:** We represent the tree using an array (size `4N` is safe). A recursive function `build(node, start, end)` populates the tree. If `start == end`, it's a leaf. Otherwise, it recursively builds the left and right children and sets its own value to their sum.\n\n2.  **Update Operation:** A recursive function `update(node, start, end, pos, val)` finds the leaf corresponding to `pos`. It updates the leaf's value and then updates the sums of all parent nodes on the recursion path back to the root.\n\n3.  **Query Operation:** A recursive function `query(node, start, end, l, r)` finds the answer for range `[l, r]`. \n    - If the node's range is outside `[l, r]`, return 0.\n    - If the node's range is completely inside `[l, r]`, return the node's value.\n    - Otherwise, the ranges partially overlap. Return the sum of recursive calls to the left and right children."
      },
      "code": "class SegmentTree {\n  constructor(arr) {\n    this.n = arr.length;\n    this.tree = new Array(4 * this.n);\n    this.build(1, 0, this.n - 1, arr);\n  }\n  build(node, l, r, arr) {\n    if (l === r) this.tree[node] = arr[l];\n    else {\n      const mid = Math.floor((l + r) / 2);\n      this.build(node * 2, l, mid, arr);\n      this.build(node * 2 + 1, mid + 1, r, arr);\n      this.tree[node] = this.tree[node * 2] + this.tree[node * 2 + 1];\n    }\n  }\n  update(pos, val, node = 1, l = 0, r = this.n - 1) {\n    if (l === r) this.tree[node] = val;\n    else {\n      const mid = Math.floor((l + r) / 2);\n      if (pos <= mid) this.update(pos, val, node * 2, l, mid);\n      else this.update(pos, val, node * 2 + 1, mid + 1, r);\n      this.tree[node] = this.tree[node * 2] + this.tree[node * 2 + 1];\n    }\n  }\n  query(qL, qR, node = 1, l = 0, r = this.n - 1) {\n    if (qL > r || qR < l) return 0;\n    if (qL <= l && r <= qR) return this.tree[node];\n    const mid = Math.floor((l + r) / 2);\n    return this.query(qL, qR, node * 2, l, mid) +\n           this.query(qL, qR, node * 2 + 1, mid + 1, r);\n  }\n}",
      "complexity": {
        "time": "O(log N) per query/update",
        "space": "O(N)",
        "explanation_time": "The tree is balanced, so its height is O(log N). Both `update` and `query` operations traverse the tree from the root to a leaf or a set of nodes, taking O(log N) time. The initial build takes O(N) time as it visits every node once.",
        "explanation_space": "We use an array to represent the tree. To avoid out-of-bounds issues with indexing, an array of size 4*N is sufficient for a balanced binary tree of N leaves. This results in O(N) space complexity."
      },
      "annotations": [
        {
          "lines": [
            4,
            5
          ],
          "text": "Allocate array of size 4N. This is a safe upper bound for a segment tree."
        },
        {
          "lines": [
            7,
            8
          ],
          "text": "Build: Leaf node (l == r) holds the array value."
        },
        {
          "lines": [
            12,
            13
          ],
          "text": "Internal node: sum of left and right children. This is the 'merge' step."
        },
        {
          "lines": [
            16,
            21
          ],
          "text": "Update: find leaf, update it, then update all parents on the way up."
        },
        {
          "lines": [
            25
          ],
          "text": "Query: Range outside node range -> return 0 (identity for sum)."
        },
        {
          "lines": [
            26
          ],
          "text": "Query: Range completely covers node range -> return pre-calculated answer."
        },
        {
          "lines": [
            28,
            29
          ],
          "text": "Query: Partial overlap -> recurse on both sides and sum the results."
        }
      ],
      "quizzes": [
        {
          "question": "What operations does a Segment Tree efficiently support?",
          "options": [
            "Only point queries",
            "Range queries and point updates",
            "Only sorting",
            "Random access only"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity for query and update?",
          "options": [
            "O(N)",
            "O(1)",
            "O(log N)",
            "O(N²)"
          ],
          "correct": 2
        },
        {
          "question": "What does each node in the tree represent?",
          "options": [
            "Single element",
            "Aggregate of an interval",
            "Pointer to child",
            "Hash value"
          ],
          "correct": 1
        },
        {
          "question": "How do we handle partial overlap during query?",
          "options": [
            "Skip node",
            "Recurse on both children",
            "Return 0",
            "Use lazy tag"
          ],
          "correct": 1
        },
        {
          "question": "Why use array size 4N for the tree?",
          "options": [
            "For padding",
            "Safe upper bound for complete binary tree",
            "Memory alignment",
            "Faster access"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "trees-2",
      "title": "Distance Queries (LCA with Binary Lifting)",
      "difficulty": "Medium",
      "related": [
        {
          "id": "trees-1",
          "title": "Segment Tree",
          "category": "trees"
        },
        {
          "id": "trees-3",
          "title": "Tree Diameter",
          "category": "trees"
        }
      ],
      "tags": [
        "lca",
        "binary-lifting",
        "trees"
      ],
      "problem_statement": "You are given a tree with `n` nodes labeled from `0` to `n - 1`, and an array of `edges` where `edges[i] = [ai, bi]` indicates that there is an undirected edge between the two nodes `ai` and `bi` in the tree.\n\nYou are also given an array `queries` where `queries[i] = [u, v]`. For each query, find the **distance** between node `u` and node `v`.\n\nThe **distance** between two nodes is the number of edges on the unique simple path between them.\n\n**Example 1:**\n```\nInput: n = 6, edges = [[0,1],[0,2],[1,3],[1,4],[2,5]], queries = [[3,5],[4,5]]\nOutput: [3, 3]\nExplanation: \nThe distance between 3 and 5 is 3 (3 -> 1 -> 0 -> 2 -> 5).\nThe distance between 4 and 5 is 3 (4 -> 1 -> 0 -> 2 -> 5).\n```",
      "diagram": "graph TD\n    0((0))\n    1((1))\n    2((2))\n    3((3))\n    4((4))\n    5((5))\n    \n    0 --- 1\n    0 --- 2\n    1 --- 3\n    1 --- 4\n    2 --- 5\n    \n    %% Binary Lifting Paths (Example for nodes 3 and 5)\n    3 -.->|2^0| 1\n    3 -.->|2^1| 0\n    5 -.->|2^0| 2\n    5 -.->|2^1| 0\n    \n    classDef default fill:#fff,stroke:#333,stroke-width:2px;",
      "explanation": {
        "understanding_the_problem": "A naive approach of running a separate BFS/DFS for each query to find the path length would be too slow if there are many queries. A more efficient method relies on finding the **Lowest Common Ancestor (LCA)** of the two nodes.\n\nThe path from node `u` to `v` can be seen as the path from `u` up to `lca(u,v)` and then down to `v`. This gives us a formula for the distance: `dist(u,v) = depth(u) + depth(v) - 2 * depth(lca(u,v))`. The problem now becomes: how to find the LCA efficiently?",
        "brute_force": "To find the LCA of `u` and `v`, we could first bring them to the same depth by moving the deeper node up one parent at a time. Then, we move both nodes up one parent at a time until they meet. This is simple but can take O(N) time per query in the worst case (for a tall, skinny tree).",
        "bottleneck": "The O(N) time per query is the bottleneck. We need a way to move nodes up the tree much faster than one step at a time. This is where Binary Lifting comes in.",
        "optimized_approach": "**Binary Lifting** is a technique that allows us to answer LCA queries in O(log N) time after an O(N log N) preprocessing step. The core idea is to precompute, for each node, its ancestor at various powers-of-2 distances (1st parent, 2nd, 4th, 8th, etc.).\n\nWe build a table `up[k][u]` that stores the `2^k`-th ancestor of node `u`. This table can be built with the recurrence: `up[k][u] = up[k-1][up[k-1][u]]` (the 2^k-th ancestor is the 2^(k-1)-th ancestor of the 2^(k-1)-th ancestor).\n\nWith this table, we can jump up the tree by large distances (any distance `d` can be represented as a sum of powers of 2), allowing us to find the LCA very quickly.",
        "algorithm_steps": "1.  **Preprocessing (O(N log N)):**\n    a.  Run a DFS from the root to compute `depth[u]` and the direct parent `up[0][u]` for every node.\n    b.  Fill the `up` table using the binary lifting recurrence described above.\n\n2.  **LCA Query (O(log N)):**\n    a.  **Equalize Depths:** Bring the deeper node up until it's at the same depth as the other node. This is done in O(log N) by jumping up by powers of 2.\n    b.  **Check for Ancestor:** If the nodes are now the same, one was an ancestor of the other, so we've found the LCA.\n    c.  **Simultaneous Lifting:** If not, lift both nodes up simultaneously by powers of 2, from largest to smallest (`k = logN...0`). We take a jump `up[k]` only if their ancestors are **not** the same. This ensures they land just below the LCA.\n    d.  The LCA is then the direct parent of either of the current nodes.\n\n3.  **Distance Query:** Once the LCA is found, apply the distance formula."
      },
      "code": "class LCA {\n  constructor(n, edges, root = 0) {\n    this.log = Math.ceil(Math.log2(n)) + 1;\n    this.parent = Array.from({length: this.log}, () => new Array(n).fill(-1));\n    this.depth = new Array(n).fill(0);\n    this.adj = Array(n).fill().map(() => []);\n    for (let [u,v] of edges) {\n      this.adj[u].push(v); this.adj[v].push(u);\n    }\n    this.dfs(root, -1);\n    this.preprocess();\n  }\n  dfs(u, p) {\n    this.parent[0][u] = p;\n    for (let v of this.adj[u]) if (v !== p) {\n      this.depth[v] = this.depth[u] + 1;\n      this.dfs(v, u);\n    }\n  }\n  preprocess() {\n    for (let k=1; k<this.log; k++)\n      for (let v=0; v<this.parent[0].length; v++)\n        if (this.parent[k-1][v] !== -1)\n          this.parent[k][v] = this.parent[k-1][this.parent[k-1][v]];\n  }\n  getLCA(u, v) {\n    if (this.depth[u] < this.depth[v]) [u,v] = [v,u];\n    let diff = this.depth[u] - this.depth[v];\n    for (let k=0; k<this.log; k++)\n      if (diff & (1<<k)) u = this.parent[k][u];\n    if (u === v) return u;\n    for (let k=this.log-1; k>=0; k--)\n      if (this.parent[k][u] !== this.parent[k][v]) {\n        u = this.parent[k][u]; v = this.parent[k][v];\n      }\n    return this.parent[0][u];\n  }\n  getDistance(u, v) {\n    const lca = this.getLCA(u, v);\n    return this.depth[u] + this.depth[v] - 2 * this.depth[lca];\n  }\n}",
      "complexity": {
        "time": "O(N log N) preprocess, O(log N) per query",
        "space": "O(N log N)",
        "explanation_time": "The initial DFS to compute depths and parents is O(N). Building the binary lifting table `up` takes O(N log N). Each LCA query then takes O(log N) because it involves a few loops that iterate up to log N times.",
        "explanation_space": "The adjacency list, depth array, etc., take O(N) space. The dominant factor is the `up` table, which has dimensions of roughly (log N) x N, resulting in O(N log N) space complexity."
      },
      "annotations": [
        {
          "lines": [
            2
          ],
          "text": "Calculate max height (log N) for binary lifting table."
        },
        {
          "lines": [
            3
          ],
          "text": "Up table: parent[k][u] is the 2^k-th ancestor of u."
        },
        {
          "lines": [
            10,
            15
          ],
          "text": "DFS: Compute depths and fill 0-th ancestors (direct parents)."
        },
        {
          "lines": [
            17,
            18
          ],
          "text": "Preprocess: Build table. 2^k-th ancestor is (2^(k-1))-th of (2^(k-1))-th."
        },
        {
          "lines": [
            21
          ],
          "text": "LCA Query: Ensure 'u' is deeper than 'v'."
        },
        {
          "lines": [
            23,
            24
          ],
          "text": "Lift 'u' to the same depth as 'v' using binary jumps."
        },
        {
          "lines": [
            27,
            28,
            29
          ],
          "text": "Lift both up simultaneously by largest powers of 2 as long as ancestors differ."
        },
        {
          "lines": [
            31
          ],
          "text": "After loop, they are just below LCA, so return parent."
        }
      ],
      "quizzes": [
        {
          "question": "What does LCA stand for?",
          "options": [
            "Last Common Ancestor",
            "Lowest Common Ancestor",
            "Linear Chain Analysis",
            "Longest Connected Arc"
          ],
          "correct": 1
        },
        {
          "question": "How is distance calculated using LCA?",
          "options": [
            "depth(u) * depth(v)",
            "depth(u) + depth(v) - 2*depth(lca)",
            "depth(lca)",
            "max(depth(u), depth(v))"
          ],
          "correct": 1
        },
        {
          "question": "What does up[k][u] store?",
          "options": [
            "k-th child",
            "2^k-th ancestor of u",
            "k ancestors",
            "Distance to root"
          ],
          "correct": 1
        },
        {
          "question": "What's the key idea of binary lifting?",
          "options": [
            "Sort nodes",
            "Jump by powers of 2",
            "Hash ancestors",
            "Split tree"
          ],
          "correct": 1
        },
        {
          "question": "What's the preprocessing complexity?",
          "options": [
            "O(N)",
            "O(N log N)",
            "O(N²)",
            "O(log N)"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "trees-3",
      "title": "Tree Diameter",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/tree-diameter/",
      "related": [
        {
          "id": "trees-2",
          "title": "LCA Binary Lifting",
          "category": "trees"
        },
        {
          "id": "graphs-3",
          "title": "Network Delay Time",
          "category": "graphs"
        }
      ],
      "tags": [
        "tree-dp",
        "dfs",
        "diameter"
      ],
      "problem_statement": "Given the `root` of a binary tree (or a general tree represented by edges), return *the length of the **diameter** of the tree*.\n\nThe **diameter** of a tree is the length of the longest path between any two nodes in a tree. This path may or may not pass through the root.\n\nThe length of a path between two nodes is represented by the number of edges between them.\n\n**Example 1:**\n```\nInput: edges = [[0,1],[0,2],[1,4],[1,5]]\nOutput: 3\nExplanation: 3 is the length of the path [4,1,0,2] or [5,1,0,2].\n```",
      "diagram": "graph TD\n    0((0)) --- 1((1))\n    0 --- 2((2))\n    1 --- 4((4))\n    1 --- 5((5))\n    \n    classDef default fill:#fff,stroke:#333,stroke-width:2px;\n    classDef path fill:#fca5a5,stroke:#b91c1c,stroke-width:2px;\n    \n    class 4,1,0,2 path;\n    linkStyle 0,1,2 stroke:#ef4444,stroke-width:4px;",
      "explanation": {
        "understanding_the_problem": "We need to find the two nodes in the tree that are the farthest apart and return the number of edges between them. Since it's a tree, there's only one unique path between any two nodes.",
        "brute_force": "A naive approach would be to run a graph traversal (like BFS or DFS) starting from every single node in the tree. For each start node `u`, we would find the node `v` farthest from it and record that distance. The final answer would be the maximum distance found across all starting nodes. This would require N traversals, each taking O(N) time, for a total of O(N²), which is inefficient.",
        "bottleneck": "The O(N²) approach is slow because it re-computes many of the same path lengths repeatedly. There are two much more efficient O(N) solutions.",
        "optimized_approach": "**Method 1: Two Traversals (BFS/DFS)**\nThis method relies on a neat property: one of the endpoints of any tree diameter will always be the node that is farthest from an arbitrary starting node.\n1. Pick any node `s` and find the node `u` farthest from it.\n2. Start from `u` and find the node `v` farthest from it.\n3. The distance between `u` and `v` is the diameter.\n\n**Method 2: DP on Trees (as in the code)**\nThis is a single-pass DFS approach. For any node `u`, the longest path that passes through `u` is the sum of the two longest 'downward' paths starting from `u` into its subtrees. We can use DFS to compute this.\n\nThe function `dfs(node)` will return the length of the longest downward path starting at `node`. While doing so, it also calculates the two longest downward paths for each node (`max1`, `max2`) and uses their sum (`max1 + max2`) to update a global `diameter` variable.",
        "algorithm_steps": "The provided code uses the DP on Trees method:\n\n1.  **Initialization:**\n    a.  Build an adjacency list for the tree.\n    b.  Initialize a global `diameter` variable to 0.\n\n2.  **Define DFS function `dfs(currentNode, parentNode)`:**\n    a.  This function will return the longest single downward path from `currentNode`.\n    b.  Inside, initialize `max1 = 0` and `max2 = 0`.\n    c.  For each `neighbor` of `currentNode` (that isn't its parent), recursively call `dfs(neighbor, currentNode)`. This gives the longest path from the neighbor down. Add 1 to it to get the path length from `currentNode`.\n    d.  Use this result to update `max1` and `max2`.\n    e.  At the end of the neighbor loop, a potential diameter passing through `currentNode` is `max1 + max2`. Update the global `diameter = max(diameter, max1 + max2)`.\n    f.  Return `max1` to be used by the parent's calculation.\n\n3.  **Execution:** Call `dfs` on an arbitrary root (e.g., node 0) and return the final `diameter`."
      },
      "code": "function treeDiameter(n, edges) {\n  const adj = Array(n).fill().map(() => []);\n  for (let [u,v] of edges) {\n    adj[u].push(v); adj[v].push(u);\n  }\n  let diameter = 0;\n  function dfs(u, p) {\n    let max1 = 0, max2 = 0;\n    for (let v of adj[u]) if (v !== p) {\n      const h = dfs(v, u) + 1;\n      if (h > max1) { max2 = max1; max1 = h; }\n      else if (h > max2) max2 = h;\n    }\n    diameter = Math.max(diameter, max1 + max2);\n    return max1;\n  }\n  dfs(0, -1);\n  return diameter;\n}",
      "complexity": {
        "time": "O(N)",
        "space": "O(N)",
        "explanation_time": "Both the two-traversal method and the DP on Trees method run in O(N) time. They are based on DFS or BFS, which visit each node and edge in the tree exactly once.",
        "explanation_space": "We need O(N) space for the adjacency list. The recursion stack for the DFS will also be O(H) where H is the height of the tree. In the worst case of a skewed tree, H can be N, so the space complexity is O(N)."
      },
      "annotations": [
        {
          "lines": [
            2,
            3,
            4
          ],
          "text": "Build adjacency list for undirected tree."
        },
        {
          "lines": [
            6
          ],
          "text": "DFS: returns length of longest path STARTING at 'u' and going DOWN."
        },
        {
          "lines": [
            9
          ],
          "text": "Recursively solve for neighbor. Add 1 for the edge u->v."
        },
        {
          "lines": [
            10,
            11
          ],
          "text": "Track top 2 longest paths branching from current node."
        },
        {
          "lines": [
            14
          ],
          "text": "Global diameter update: The longest path THROUGH u is max1 + max2."
        }
      ],
      "quizzes": [
        {
          "question": "What is tree diameter?",
          "options": [
            "Node count",
            "Longest path between any two nodes",
            "Tree height",
            "Edge weight sum"
          ],
          "correct": 1
        },
        {
          "question": "What's the two-BFS/DFS method?",
          "options": [
            "Sort nodes",
            "Find farthest from any node, then farthest from that",
            "Binary search",
            "Hash nodes"
          ],
          "correct": 1
        },
        {
          "question": "In the DP method, what do max1 and max2 track?",
          "options": [
            "Min paths",
            "Two longest downward paths",
            "Node values",
            "Edge weights"
          ],
          "correct": 1
        },
        {
          "question": "How is diameter updated at each node?",
          "options": [
            "max1 * max2",
            "max1 + max2",
            "max(max1, max2)",
            "min(max1, max2)"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity?",
          "options": [
            "O(N²)",
            "O(N log N)",
            "O(N)",
            "O(2^N)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "trees-4",
      "title": "Range Add, Range Sum (Lazy Propagation)",
      "difficulty": "Medium",
      "related": [
        {
          "id": "trees-1",
          "title": "Segment Tree",
          "category": "trees"
        },
        {
          "id": "trees-5",
          "title": "Fenwick Tree",
          "category": "trees"
        }
      ],
      "tags": [
        "segment-tree",
        "lazy-propagation"
      ],
      "follow_up": {
        "scenario": "Range updates on a high-frequency trading ledger (millions of rows).",
        "trade_off": "Updating every leaf node is O(N), which is too slow for real-time streams.",
        "strategy": "Lazy Propagation. Defer updates to children until absolutely necessary (during a query or subsequent update).",
        "answering_guide": "The keyword is <strong>'Deferred Execution'</strong>. Explain that we trade slightly more complex logic for massive <strong>'Write Speed'</strong> gains by caching updates."
      },
      "problem_statement": "You have an array of integers `nums`. You need to handle multiple queries of the following types efficiently:\n\n1.  **Range Add**: Add a value `val` to all elements `nums[i]` where `left <= i <= right`.\n2.  **Range Sum**: Calculate the sum of elements `nums[i]` where `left <= i <= right`.\n\nImplement the `LazySegmentTree` class:\n*   `LazySegmentTree(int[] nums)` Initializes with `nums`.\n*   `void rangeAdd(int left, int right, int val)` Adds `val` to the subarray `nums[left...right]`.\n*   `int rangeSum(int left, int right)` Returns the sum of the subarray `nums[left...right]`.\n\n**Example 1:**\n```\nInput\n[\"LazySegmentTree\", \"rangeAdd\", \"rangeSum\", \"rangeAdd\", \"rangeSum\"]\n[[[1, 1, 1, 1, 1]], [0, 4, 1], [0, 4], [2, 3, 10], [0, 4]]\nOutput\n[null, null, 10, null, 30]\nExplanation\n// nums starts as [1, 1, 1, 1, 1]\n// rangeAdd(0, 4, 1) -> nums becomes [2, 2, 2, 2, 2]\n// rangeSum(0, 4) -> 10\n// rangeAdd(2, 3, 10) -> nums becomes [2, 2, 12, 12, 2]\n// rangeSum(0, 4) -> 30\n```",
      "explanation": {
        "understanding_the_problem": "A standard segment tree can handle point updates and range queries in O(log N) time. However, if we try to use it for a range update, we would have to update every leaf in the range, which could take O(N log N) time. We need a way to make range updates efficient as well.",
        "brute_force": "The bottleneck is applying updates to a large range. The key idea to solve this is **Lazy Propagation**. Instead of immediately applying an update to all affected children in the tree, we will be 'lazy'. We'll update a high-level node that covers a large part of the range and leave a 'note' (a lazy tag) on it. This note signifies that all its children need this update eventually.",
        "bottleneck": "Without lazy propagation, a single range update that covers most of the array would be equivalent to performing N point updates, defeating the purpose of the segment tree's logarithmic performance.",
        "optimized_approach": "We add a `lazy` array, parallel to our `tree` array. `lazy[node]` stores a pending update for that node's interval.\n\n- **During a `rangeAdd`:** When we find a node fully contained in the update range, we update its `sum` and add to its `lazy` tag. We don't go down to its children.\n- **During a `rangeSum` (or any traversal):** Before we access a node's children, we must first check if the node has a lazy tag. If it does, we 'push' the update down to its children, updating their `sum` and `lazy` values. Then, we clear the current node's lazy tag.\n\nThis `pushDown` operation ensures that updates are propagated correctly and only when needed, maintaining the O(log N) performance for all operations.",
        "algorithm_steps": "1.  **Data Structures:** Augment the standard Segment Tree with a `lazy` array of the same size, initialized to zeros.\n\n2.  **`pushDown(node, ...)` function:** This is the core of the technique. It applies the `lazy[node]` value to its two children's `sum` and `lazy` fields, then resets `lazy[node]` to 0.\n\n3.  **`rangeAdd(l, r, val)` function:**\n    a.  Follows the standard segment tree traversal.\n    b.  Before recursing to children, it calls `pushDown` on the current node.\n    c.  If a node is fully contained in the update range, update its `sum` and `lazy` tag and return.\n    d.  On the way back up the recursion, update parent sums based on their children.\n\n4.  **`rangeSum(l, r)` function:**\n    a.  Follows the standard segment tree query logic.\n    b.  Crucially, before recursing to children, it calls `pushDown` on the current node to ensure its information is up-to-date."
      },
      "code": "class LazySegmentTree {\n  constructor(arr) {\n    this.n = arr.length;\n    this.tree = new Array(4 * this.n).fill(0);\n    this.lazy = new Array(4 * this.n).fill(0);\n    this.build(1, 0, this.n - 1, arr);\n  }\n  build(node, l, r, arr) {\n    if (l === r) this.tree[node] = arr[l];\n    else {\n      const mid = Math.floor((l + r) / 2);\n      this.build(node*2, l, mid, arr);\n      this.build(node*2+1, mid+1, r, arr);\n      this.tree[node] = this.tree[node*2] + this.tree[node*2+1];\n    }\n  }\n  apply(node, l, r, val) {\n    this.tree[node] += val * (r - l + 1);\n    this.lazy[node] += val;\n  }\n  push(node, l, r) {\n    if (this.lazy[node] !== 0) {\n      const mid = Math.floor((l + r) / 2);\n      this.apply(node*2, l, mid, this.lazy[node]);\n      this.apply(node*2+1, mid+1, r, this.lazy[node]);\n      this.lazy[node] = 0;\n    }\n  }\n  rangeAdd(qL, qR, val, node=1, l=0, r=this.n-1) {\n    if (qL > r || qR < l) return;\n    if (qL <= l && r <= qR) {\n      this.apply(node, l, r, val);\n      return;\n    }\n    this.push(node, l, r);\n    const mid = Math.floor((l + r) / 2);\n    this.rangeAdd(qL, qR, val, node*2, l, mid);\n    this.rangeAdd(qL, qR, val, node*2+1, mid+1, r);\n    this.tree[node] = this.tree[node*2] + this.tree[node*2+1];\n  }\n  rangeSum(qL, qR, node=1, l=0, r=this.n-1) {\n    if (qL > r || qR < l) return 0;\n    if (qL <= l && r <= qR) return this.tree[node];\n    this.push(node, l, r);\n    const mid = Math.floor((l + r) / 2);\n    return this.rangeSum(qL, qR, node*2, l, mid) +\n           this.rangeSum(qL, qR, node*2+1, mid+1, r);\n  }\n}",
      "complexity": {
        "time": "O(log N) per operation",
        "space": "O(N)",
        "explanation_time": "Thanks to lazy propagation, range updates no longer need to touch every node in the range. Both `rangeAdd` and `rangeSum` operations only need to traverse a logarithmic number of nodes down the tree. This gives them an amortized time complexity of O(log N).",
        "explanation_space": "We need an array for the tree itself (O(N)) and an additional array of the same size for the lazy tags (O(N)). This results in a total space complexity of O(N)."
      },
      "annotations": [
        {
          "lines": [
            5
          ],
          "text": "Lazy array: stores pending additions for children."
        },
        {
          "lines": [
            17,
            18
          ],
          "text": "Apply: Update node sum AND add to its lazy tag."
        },
        {
          "lines": [
            21
          ],
          "text": "Push: Propagate lazy value to children only when needed."
        },
        {
          "lines": [
            23,
            24
          ],
          "text": "Update children and reset current lazy tag to 0."
        },
        {
          "lines": [
            31
          ],
          "text": "Range Add: If fully covered, update node & lazy, then STOP. Don't recurse."
        },
        {
          "lines": [
            34
          ],
          "text": "Crucial: Push pending updates before recursing."
        },
        {
          "lines": [
            47
          ],
          "text": "Query: Push before checking children to ensure fresh data."
        }
      ],
      "quizzes": [
        {
          "question": "What is 'lazy propagation'?",
          "options": [
            "Slow updates",
            "Defer updates to children until needed",
            "Skip nodes",
            "Parallel processing"
          ],
          "correct": 1
        },
        {
          "question": "When do we 'push down' lazy values?",
          "options": [
            "Never",
            "Before accessing children",
            "After query",
            "Only on update"
          ],
          "correct": 1
        },
        {
          "question": "Why use lazy propagation for range updates?",
          "options": [
            "Simpler code",
            "Avoid updating all N elements",
            "Better caching",
            "Less memory"
          ],
          "correct": 1
        },
        {
          "question": "What does the lazy array store?",
          "options": [
            "Final values",
            "Pending additions for children",
            "Node indices",
            "Hash codes"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity per operation?",
          "options": [
            "O(N)",
            "O(1)",
            "O(log N)",
            "O(N log N)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "trees-5",
      "title": "Fenwick Tree (Binary Indexed Tree)",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/range-sum-query-mutable/",
      "related": [
        {
          "id": "trees-1",
          "title": "Segment Tree",
          "category": "trees"
        },
        {
          "id": "trees-4",
          "title": "Lazy Propagation",
          "category": "trees"
        }
      ],
      "tags": [
        "fenwick-tree",
        "bit",
        "data-structures"
      ],
      "follow_up": {
        "scenario": "Live leaderboard with millions of players and constant score updates.",
        "trade_off": "Segment Tree uses 4N space (memory overhead). We need something cache-friendly.",
        "strategy": "Fenwick Tree (BIT). Uses O(N) space and relies on bit manipulation for O(log N) traversal. Implicit bounds.",
        "answering_guide": "Highlight <strong>'Space Efficiency'</strong> and <strong>'Bit Manipulation'</strong>. Mentioning that BIT is easier to implement (fewer lines of code) than Segment Tree often impresses."
      },
      "problem_statement": "Given an integer array `nums`, handle multiple queries of the following types:\n\n1.  **Update**: Add a value `delta` to the element at `index`.\n2.  **Prefix Sum**: Calculate the sum of the elements of `nums` between indices `0` and `index` **inclusive**.\n\nImplement the `FenwickTree` class:\n*   `FenwickTree(int n)` Initializes a tree with `n+1` size.\n*   `void update(int index, int delta)` Adds `delta` to `nums[index]`.\n*   `int query(int index)` Returns the prefix sum up to `index`.\n\n**Example 1:**\n```\nInput\n[\"FenwickTree\", \"update\", \"query\", \"update\", \"query\"]\n[[5], [2, 1], [4], [2, 3], [4]]\nOutput\n[null, null, 1, null, 4]\n```",
      "explanation": {
        "understanding_the_problem": "This problem requires efficient point updates and prefix sum queries. A standard Segment Tree can solve this in O(log N) time, but a Fenwick Tree (also known as a Binary Indexed Tree or BIT) provides the same time complexity while being significantly simpler to implement and requiring less space.",
        "brute_force": "A naive array gives O(1) updates but O(N) prefix sums. A pre-calculated prefix sum array gives O(1) queries but O(N) updates. We need to balance these, which is what a BIT does.",
        "bottleneck": "The core idea of a Fenwick Tree is that any prefix sum can be calculated by summing up a few pre-calculated, non-overlapping sub-range sums. The genius of the data structure lies in how it maps indices to these sub-ranges using bit manipulation.",
        "optimized_approach": "A BIT uses an array where `tree[i]` stores the sum of a specific range of elements from the original array. The range `tree[i]` is responsible for is determined by the least significant bit (LSB) of `i`.\n\n- **`update(index, delta)`:** When a value at `index` changes, we only need to update the `tree` entries that include this index in their range. Using bit manipulation (`i += i & -i`), we can 'jump' up to parent ranges that are affected, taking O(log N) time.\n- **`query(index)`:** To get a prefix sum, we do the reverse. We start at `tree[index]` and add its value. Then we jump to the next relevant sub-range (`i -= i & -i`) and add its value, and so on. This also takes O(log N) time.",
        "algorithm_steps": "Note: Fenwick Trees are typically implemented using 1-based indexing.\n\n1.  **`update(index, delta)`:**\n    a.  Start at `i = index + 1`.\n    b.  While `i` is within the bounds of the tree array, add `delta` to `tree[i]`.\n    c.  Move to the next affected index by jumping to the parent range: `i += i & -i`. The expression `i & -i` isolates the least significant bit.\n\n2.  **`query(index)`:**\n    a.  Start at `i = index + 1`.\n    b.  Initialize `sum = 0`.\n    c.  While `i > 0`, add `tree[i]` to `sum`.\n    d.  Move to the next sub-range by removing the LSB: `i -= i & -i`.\n    e.  Return `sum`."
      },
      "code": "class FenwickTree {\n  constructor(n) {\n    this.n = n;\n    this.tree = new Array(n + 1).fill(0);\n  }\n  update(i, delta) {\n    for (++i; i <= this.n; i += i & -i) {\n      this.tree[i] += delta;\n    }\n  }\n  query(i) {\n    let sum = 0;\n    for (++i; i > 0; i -= i & -i) {\n      sum += this.tree[i];\n    }\n    return sum;\n  }\n  rangeQuery(l, r) {\n    return this.query(r) - (l > 0 ? this.query(l - 1) : 0);\n  }\n}",
      "complexity": {
        "time": "O(log N) per operation",
        "space": "O(N)",
        "explanation_time": "Both the `update` and `query` operations involve a loop that 'jumps' through the tree array. The number of jumps is determined by the number of set bits in the index, which is at most O(log N).",
        "explanation_space": "The Fenwick Tree requires a single array of size N+1 to store its state, resulting in O(N) space complexity. This is more space-efficient than a standard Segment Tree, which typically requires 4N space."
      },
      "annotations": [
        {
          "lines": [
            4
          ],
          "text": "Tree array is 1-indexed for bit manipulation magic."
        },
        {
          "lines": [
            7
          ],
          "text": "Update: Start at index+1. Loop until end of array."
        },
        {
          "lines": [
            8
          ],
          "text": "i += i & -i: Jumps to the next parent range coverage."
        },
        {
          "lines": [
            13
          ],
          "text": "Query: Start at index+1. Loop backwards to 0."
        },
        {
          "lines": [
            14
          ],
          "text": "i -= i & -i: Jumps to the previous non-overlapping sub-range."
        },
        {
          "lines": [
            19
          ],
          "text": "Range Sum [L, R] = PrefixSum(R) - PrefixSum(L-1)."
        }
      ],
      "quizzes": [
        {
          "question": "What is another name for Fenwick Tree?",
          "options": [
            "Segment Tree",
            "Binary Indexed Tree",
            "AVL Tree",
            "Red-Black Tree"
          ],
          "correct": 1
        },
        {
          "question": "What bit operation finds the LSB?",
          "options": [
            "i | -i",
            "i ^ -i",
            "i & -i",
            "i >> 1"
          ],
          "correct": 2
        },
        {
          "question": "Why is Fenwick Tree 1-indexed?",
          "options": [
            "Convention",
            "LSB trick works properly",
            "Faster access",
            "Less memory"
          ],
          "correct": 1
        },
        {
          "question": "What's the advantage over Segment Tree?",
          "options": [
            "Faster queries",
            "Simpler code and less space",
            "More operations",
            "Better for strings"
          ],
          "correct": 1
        },
        {
          "question": "How to get range sum [L, R]?",
          "options": [
            "query(R) + query(L)",
            "query(R) - query(L-1)",
            "query(L+R)",
            "query(R) * query(L)"
          ],
          "correct": 1
        }
      ]
    }
  ],
  "strings": [
    {
      "id": "strings-1",
      "title": "KMP Pattern Matching",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/find-the-index-of-the-first-occurrence-in-a-string/",
      "related": [
        {
          "id": "strings-2",
          "title": "Rabin-Karp Rolling Hash",
          "category": "strings"
        },
        {
          "id": "strings-3",
          "title": "Z-Algorithm",
          "category": "strings"
        }
      ],
      "tags": [
        "kmp",
        "strings",
        "pattern-matching"
      ],
      "follow_up": {
        "scenario": "Intrusion Detection System (IDS) filtering network packets against signatures.",
        "trade_off": "Naive search is vulnerable to algorithmic complexity attacks. Checking one by one is too slow.",
        "strategy": "Use Aho-Corasick for *multiple* pattern matching (an extension of KMP). It builds a Trie of patterns with failure links.",
        "answering_guide": "In security/networking, pure KMP is rarely used alone. Mention <strong>'Aho-Corasick'</strong> for *multiple* patterns (like virus signatures)."
      },
      "problem_statement": "Given two strings `text` and `pattern`, return **all** indices in `text` where `pattern` starts.\n\n**Example 1:**\n```\nInput: text = \"abxabcabcaby\", pattern = \"abcaby\"\nOutput: [6]\n```\n**Example 2:**\n```\nInput: text = \"aaaaa\", pattern = \"aa\"\nOutput: [0, 1, 2, 3]\n```",
      "explanation": {
        "understanding_the_problem": "The goal is to find a small pattern string within a larger text string efficiently. A naive solution would check for a match at every possible starting position in the text, which is slow.",
        "brute_force": "The naive approach is to slide the pattern over the text one position at a time. At each position, you compare the pattern with the corresponding substring of the text. This has a time complexity of O((N-M+1) * M) \u2248 O(N*M), where N is the length of the text and M is the length of the pattern.",
        "bottleneck": "The bottleneck is that when a mismatch occurs, the naive approach shifts the pattern by only one position. We have information from the partial match we just made that could allow us to shift the pattern much further.",
        "optimized_approach": "The Knuth-Morris-Pratt (KMP) algorithm achieves O(N+M) time by using a precomputed 'Longest Proper Prefix which is also Suffix' (LPS) array. This `lps` array tells us, after a mismatch, the length of the longest proper prefix of the pattern that has already been matched, which is also a suffix of that matched part. This allows us to 'slide' the pattern forward intelligently without re-checking characters we know will match.",
        "algorithm_steps": "1.  **Preprocessing (O(M)):** Build the `lps` array for the `pattern`. `lps[i]` stores the length of the longest proper prefix of `pattern[0...i]` which is also a suffix of `pattern[0...i]`.\n2.  **Searching (O(N)):** Iterate through the `text` with pointer `i` and `pattern` with pointer `j`. If characters match, increment both. If they mismatch, instead of resetting `j` to 0, we consult the `lps` array (`j = lps[j-1]`) to find the next best position to continue matching from, avoiding redundant comparisons."
      },
      "code": "function buildLPS(pattern) {\n  const m = pattern.length;\n  const lps = new Array(m).fill(0);\n  let len = 0;\n  for (let i = 1; i < m; i++) {\n    while (len > 0 && pattern[i] !== pattern[len]) len = lps[len - 1];\n    if (pattern[i] === pattern[len]) len++;\n    lps[i] = len;\n  }\n  return lps;\n}\n\nfunction kmpSearch(text, pattern) {\n  const n = text.length, m = pattern.length;\n  const lps = buildLPS(pattern);\n  const result = [];\n  let i = 0, j = 0;\n  while (i < n) {\n    if (pattern[j] === text[i]) {\n      i++; j++;\n    }\n    if (j === m) {\n      result.push(i - j);\n      j = lps[j - 1];\n    } else if (i < n && pattern[j] !== text[i]) {\n      if (j !== 0) j = lps[j - 1];\n      else i++;\n    }\n  }\n  return result;\n}",
      "complexity": {
        "time": "O(N + M)",
        "space": "O(M)",
        "explanation_time": "The algorithm has two parts: building the LPS array, which takes O(M) time, and searching the text, which takes O(N) time because the text pointer `i` never moves backward. This gives a total linear time complexity.",
        "explanation_space": "We need to store the LPS array, which is the same size as the pattern, resulting in O(M) space."
      },
      "annotations": [
        {
          "lines": [
            6
          ],
          "text": "LPS mismatch case: backtrack `len` using previous LPS values."
        },
        {
          "lines": [
            7
          ],
          "text": "Match case: extend the current prefix length."
        },
        {
          "lines": [
            16
          ],
          "text": "Search loop: i scans text, j scans pattern."
        },
        {
          "lines": [
            19,
            20
          ],
          "text": "Full match found! Record index and use LPS to prepare for next match."
        },
        {
          "lines": [
            22
          ],
          "text": "Mismatch after some matches: Smart jump. Set j to LPS[j-1]."
        }
      ],
      "quizzes": [
        {
          "question": "What does LPS stand for in KMP?",
          "options": [
            "Longest Prefix String",
            "Longest Proper Prefix also Suffix",
            "Last Pattern Start",
            "Linear Pattern Search"
          ],
          "correct": 1
        },
        {
          "question": "What is the time complexity of KMP?",
          "options": [
            "O(N²)",
            "O(N*M)",
            "O(N+M)",
            "O(N log N)"
          ],
          "correct": 2
        },
        {
          "question": "What makes KMP faster than naive search?",
          "options": [
            "Sorting first",
            "Avoiding redundant comparisons using LPS",
            "Parallel processing",
            "Binary search"
          ],
          "correct": 1
        },
        {
          "question": "What happens on mismatch after partial match?",
          "options": [
            "Start over",
            "Use LPS to skip ahead",
            "Reverse string",
            "Swap characters"
          ],
          "correct": 1
        },
        {
          "question": "Is the LPS array built in preprocessing?",
          "options": [
            "No, during search",
            "Yes, O(M) time",
            "Not needed",
            "Built once per text"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "strings-2",
      "title": "Rabin-Karp Rolling Hash",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/repeated-dna-sequences/",
      "related": [
        {
          "id": "strings-1",
          "title": "KMP Pattern Matching",
          "category": "strings"
        },
        {
          "id": "strings-3",
          "title": "Z-Algorithm",
          "category": "strings"
        }
      ],
      "tags": [
        "hashing",
        "strings",
        "pattern-matching"
      ],
      "follow_up": {
        "scenario": "Plagiarism detection in large codebases or documents.",
        "trade_off": "Exact string matching is expensive for massive documents.",
        "strategy": "Content-Defined Chunking (CDC) with Rolling Hash to fingerprint files. Compare fingerprints instead of full text.",
        "answering_guide": "Focus on <strong>'Fingerprinting'</strong>. Rolling hash is the basis for 'rsync' and 'deduplication' systems. Mentioning <strong>'rsync'</strong> is a pro move."
      },
      "problem_statement": "Given two strings `text` and `pattern`, return **all** indices in `text` where `pattern` starts.\n\n**Example 1:**\n```\nInput: text = \"abxabcabcaby\", pattern = \"abcaby\"\nOutput: [6]\n```",
      "explanation": {
        "understanding_the_problem": "This algorithm provides an alternative to KMP. Instead of comparing strings character by character, it compares hash values. The core idea is to compute a hash for the pattern and then efficiently compute hashes for all substrings of the text that have the same length as the pattern.",
        "brute_force": "A naive hashing approach would be to calculate the hash of the pattern, then iterate through all N-M+1 substrings of the text, calculate the hash for each, and compare. However, calculating the hash for each substring from scratch would take O(M) time, leading to the same O(N*M) complexity as the naive string search.",
        "bottleneck": "The bottleneck is re-calculating the hash for each sliding window from scratch.",
        "optimized_approach": "The Rabin-Karp algorithm uses a **Rolling Hash**. A rolling hash function allows us to calculate the hash value of the next window in O(1) time, given the hash of the current window. When we slide the window one position to the right, we mathematically subtract the contribution of the character leaving the window and add the contribution of the character entering it.\n\nWhen the hash of the pattern matches the hash of the current text window, it's a *potential* match. We must then do a character-by-character comparison to confirm, as different strings can sometimes have the same hash (a 'collision').",
        "algorithm_steps": "1.  Choose a prime modulus and a base for the polynomial rolling hash.\n2.  Calculate the hash of the `pattern` and the hash of the first window of the `text`.\n3.  Iterate from left to right across the text. If the hashes match, perform a direct string comparison to confirm.\n4.  'Roll' the hash of the text window to the next position in O(1) time by removing the leftmost character's contribution and adding the rightmost character's."
      },
      "code": "function rabinKarp(text, pattern, base=256, mod=10**9+7) {\n  const n = text.length, m = pattern.length;\n  if (m > n) return [];\n  let hashPattern = 0, hashText = 0;\n  let h = 1;\n  for (let i = 0; i < m; i++) {\n    hashPattern = (hashPattern * base + pattern.charCodeAt(i)) % mod;\n    hashText = (hashText * base + text.charCodeAt(i)) % mod;\n    if (i < m - 1) h = (h * base) % mod;\n  }\n  const result = [];\n  for (let i = 0; i <= n - m; i++) {\n    if (hashPattern === hashText) {\n      if (text.slice(i, i+m) === pattern) result.push(i);\n    }\n    if (i < n - m) {\n      hashText = ((hashText - text.charCodeAt(i) * h) * base + text.charCodeAt(i+m)) % mod;\n      if (hashText < 0) hashText += mod;\n    }\n  }\n  return result;\n}",
      "complexity": {
        "time": "O(N + M) average",
        "space": "O(M)",
        "explanation_time": "On average, with a good hash function, collisions are rare. The algorithm runs in O(N+M) time. In the worst case (many hash collisions), the complexity can degrade to O(N*M) because of the repeated character-by-character checks.",
        "explanation_space": "We only need to store the pattern and a few variables for the hashes, so the space is O(M)."
      },
      "annotations": [
        {
          "lines": [
            8
          ],
          "text": "Initial window hash calculation."
        },
        {
          "lines": [
            13
          ],
          "text": "Hash match: Potential collision, verify with exact string comparison."
        },
        {
          "lines": [
            17
          ],
          "text": "Rolling hash: Remove leading char, add new trailing char in O(1)."
        },
        {
          "lines": [
            18
          ],
          "text": "Correction: Handle negative result from JS modulo operator."
        }
      ],
      "quizzes": [
        {
          "question": "What is a 'rolling hash'?",
          "options": [
            "A sorted hash",
            "Hash updated in O(1) per window slide",
            "A circular buffer",
            "Encrypted hash"
          ],
          "correct": 1
        },
        {
          "question": "What happens when hashes match?",
          "options": [
            "Guaranteed match",
            "Verify with string comparison",
            "Skip to next",
            "Return immediately"
          ],
          "correct": 1
        },
        {
          "question": "Why might we get false positives?",
          "options": [
            "Bug in code",
            "Hash collisions",
            "Wrong base",
            "Pattern too long"
          ],
          "correct": 1
        },
        {
          "question": "What's the average time complexity?",
          "options": [
            "O(N²)",
            "O(N*M)",
            "O(N+M)",
            "O(log N)"
          ],
          "correct": 2
        },
        {
          "question": "What's the worst case (many collisions)?",
          "options": [
            "O(N)",
            "O(N+M)",
            "O(N*M)",
            "O(log N)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "strings-3",
      "title": "Z-Algorithm for Pattern Finding",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/find-the-index-of-the-first-occurrence-in-a-string/",
      "related": [
        {
          "id": "strings-1",
          "title": "KMP Pattern Matching",
          "category": "strings"
        },
        {
          "id": "strings-4",
          "title": "Suffix Array Construction",
          "category": "strings"
        }
      ],
      "tags": [
        "z-algorithm",
        "strings",
        "prefix-function"
      ],
      "problem_statement": "Given two strings `text` and `pattern`, return **all** indices in `text` where `pattern` starts.\n\n**Example 1:**\n```\nInput: text = \"abxabcabcaby\", pattern = \"abcaby\"\nOutput: [6]\n```",
      "explanation": {
        "understanding_the_problem": "The Z-Algorithm is another linear-time pattern searching algorithm. It's based on creating a 'Z-array' from a combined string.",
        "brute_force": "The core of the algorithm is the Z-array. For a string `S`, `Z[i]` is the length of the longest substring starting from `S[i]` which is also a prefix of `S`. A naive way to compute this array would be to iterate `i` from 1 to N-1 and, for each `i`, manually compare `S[i...]` with `S[0...]`, which would take O(N²) time.",
        "bottleneck": "The O(N²) Z-array construction is the bottleneck. A clever linear-time construction is needed.",
        "optimized_approach": "To find a `pattern` in `text`, we construct a new string `S = pattern + '$' + text`, where `$` is a special character not present in either string. Then, we compute the Z-array for `S` in linear time.\n\nThe key is that if `Z[i]` for some index `i` in the `text` part of `S` is equal to the length of the `pattern`, it means the substring of `S` starting at `i` is identical to the prefix of `S` (which is the `pattern`). Thus, we have found a match.\n\nThe Z-array itself can be computed in O(N+M) time by maintaining a 'Z-box' `[l, r]`, which is the interval with the rightmost endpoint that matches a prefix. When computing `Z[i]`, if `i` is inside this box, we can use previously computed Z-values to get a head start, avoiding redundant comparisons.",
        "algorithm_steps": "1.  Create the concatenated string `S = pattern + '$' + text`.\n2.  Compute the Z-array for `S` using the optimized linear-time algorithm.\n3.  Iterate through the Z-array. If `Z[i]` equals the length of the pattern, then a match starts at index `i - pattern.length - 1` in the original text."
      },
      "code": "function computeZ(s) {\n  const n = s.length;\n  const Z = new Array(n).fill(0);\n  Z[0] = 0;\n  let l = 0, r = 0;\n  for (let i = 1; i < n; i++) {\n    if (i <= r) {\n      Z[i] = Math.min(r - i + 1, Z[i - l]);\n    }\n    while (i + Z[i] < n && s[Z[i]] === s[i + Z[i]]) {\n      Z[i]++;\n    }\n    if (i + Z[i] - 1 > r) {\n      l = i;\n      r = i + Z[i] - 1;\n    }\n  }\n  return Z;\n}\n\nfunction zSearch(pattern, text, special = '$') {\n  const combined = pattern + special + text;\n  const Z = computeZ(combined);\n  const m = pattern.length;\n  const result = [];\n  for (let i = m + 1; i < combined.length; i++) {\n    if (Z[i] === m) {\n      result.push(i - m - 1);\n    }\n  }\n  return result;\n}",
      "complexity": {
        "time": "O(N + M)",
        "space": "O(N + M)",
        "explanation_time": "The Z-array construction algorithm cleverly reuses information to achieve a single pass over the combined string, giving a linear time complexity.",
        "explanation_space": "We need to store the combined string and the Z-array, which both have a size of N + M + 1."
      },
      "annotations": [
        {
          "lines": [
            6
          ],
          "text": "Optimization: Inside Z-box [l,r], use pre-computed mirror value."
        },
        {
          "lines": [
            9
          ],
          "text": "Naive expansion: Extend match beyond known Z-box."
        },
        {
          "lines": [
            12
          ],
          "text": "Update Z-box: Expand [l, r] boundary to cover new match."
        },
        {
          "lines": [
            20
          ],
          "text": "Check Z-value: If length equals pattern length, exact match found."
        }
      ],
      "quizzes": [
        {
          "question": "What does Z[i] represent?",
          "options": [
            "Distance to end",
            "Longest prefix also substring at i",
            "Character code",
            "Match count"
          ],
          "correct": 1
        },
        {
          "question": "How do we use Z-algorithm for pattern matching?",
          "options": [
            "Direct comparison",
            "Concatenate pattern+'$'+text",
            "Sort strings",
            "Hash values"
          ],
          "correct": 1
        },
        {
          "question": "What is a 'Z-box'?",
          "options": [
            "Special character",
            "Interval matching prefix used to optimize",
            "Data structure",
            "Output array"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity?",
          "options": [
            "O(N²)",
            "O(N*M)",
            "O(N+M)",
            "O(N log N)"
          ],
          "correct": 2
        },
        {
          "question": "When is Z[i] = pattern length?",
          "options": [
            "Always",
            "When pattern matches at position i",
            "Never",
            "At end of text"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "strings-4",
      "title": "Suffix Array Construction",
      "difficulty": "Medium",
      "related": [
        {
          "id": "strings-3",
          "title": "Z-Algorithm",
          "category": "strings"
        },
        {
          "id": "strings-5",
          "title": "Manacher's Algorithm",
          "category": "strings"
        }
      ],
      "tags": [
        "suffix-array",
        "strings",
        "sorting"
      ],
      "problem_statement": "Given a string `s`, return the **suffix array** of `s`.\n\nA **suffix array** is a sorted array of all suffixes of `s`. It is an array of integers representing the starting indices of all suffixes of `s` required to list them in **lexicographical order**.\n\n**Example 1:**\n```\nInput: s = \"banana\"\nOutput: [5, 3, 1, 0, 4, 2]\nExplanation: \nSuffixes: [\"a\", \"ana\", \"anana\", \"banana\", \"na\", \"nana\"]\nIndices: [5, 3, 1, 0, 4, 2]\n```",
      "explanation": {
        "understanding_the_problem": "We need to create an array of integers that represent the starting indices of all suffixes of a string, such that if we were to list the suffixes in that order, they would be lexicographically sorted.",
        "brute_force": "The most straightforward way is to actually generate all N suffixes of the string and then use a standard sorting algorithm. Generating all suffixes takes O(N²) time and space. Sorting them would then take O(N * N log N) because each comparison can take up to O(N) time. This is far too slow.",
        "bottleneck": "The sorting step is the bottleneck, specifically the expensive O(N) string comparisons.",
        "optimized_approach": "A common and much faster approach is the **'Doubling'** or **'Prefix-Doubling'** algorithm, which has a complexity of O(N log² N). Instead of sorting the full suffixes at once, we sort them based on prefixes of exponentially increasing lengths (1, 2, 4, 8, ...).\n\nIn each step `k`, we sort the suffixes based on their first `2^k` characters. The key trick is that a prefix of length `2^k` can be seen as a pair of two prefixes of length `2^(k-1)`, whose relative order (rank) we already computed in the previous step. This turns the expensive string comparison into a simple pair-of-integers comparison, which is much faster.",
        "algorithm_steps": "1.  **Step k=0:** Sort all suffixes based on their first character. Assign an equivalence class (rank) to each suffix.\n2.  **Iteration:** For `k` from 1 up to `log N`:\n    a.  Create pairs of ranks for each suffix, representing the first `2^k` characters: `(rank of first 2^(k-1) chars, rank of second 2^(k-1) chars)`.\n    b.  Sort the suffixes based on these pairs.\n    c.  Re-calculate the equivalence classes (ranks) based on the new sorted order.\n3.  **Result:** After `log N` iterations, the suffixes are fully sorted, and the suffix array is constructed."
      },
      "code": "function buildSuffixArray(s) {\n  const n = s.length;\n  const sa = Array.from({length: n}, (_, i) => i);\n  const rank = s.split('').map(c => c.charCodeAt(0));\n  const tmp = new Array(n).fill(0);\n  for (let k = 1; k < n; k *= 2) {\n    sa.sort((a, b) => {\n      const ra = rank[a], rb = rank[b];\n      if (ra === rb) {\n        const rak = rank[a+k] || -1, rbk = rank[b+k] || -1;\n        return rak - rbk;\n      }\n      return ra - rb;\n    });\n    tmp[sa[0]] = 0;\n    for (let i = 1; i < n; i++) {\n      const prev = sa[i-1], curr = sa[i];\n      const prevRank = rank[prev] + ',' + (rank[prev+k] || -1);\n      const currRank = rank[curr] + ',' + (rank[curr+k] || -1);\n      tmp[curr] = tmp[prev] + (prevRank < currRank ? 1 : 0);\n    }\n    [rank, tmp] = [tmp, rank];\n  }\n  return sa;\n}",
      "complexity": {
        "time": "O(N log² N)",
        "space": "O(N)",
        "explanation_time": "The main loop runs log(N) times. Inside the loop, the dominant operation is sorting, which takes O(N log N) time. This gives a total time complexity of O(N log² N). (Note: More advanced O(N) algorithms like SA-IS exist but are much more complex).",
        "explanation_space": "We need space for the suffix array itself, plus a couple of arrays for ranks, all of which are O(N)."
      },
      "annotations": [
        {
          "lines": [
            6
          ],
          "text": "Main loop: Logarithmic steps, doubling prefix length k each time."
        },
        {
          "lines": [
            8
          ],
          "text": "Sorting: Compare suffixes using pair (current rank, next rank)."
        },
        {
          "lines": [
            21
          ],
          "text": "Re-rank: Assign new equivalence classes based on sorted order."
        }
      ],
      "quizzes": [
        {
          "question": "What is a suffix array?",
          "options": [
            "Character frequencies",
            "Sorted indices of all suffixes",
            "Hash values",
            "Prefix lengths"
          ],
          "correct": 1
        },
        {
          "question": "What's the key idea of prefix-doubling?",
          "options": [
            "Sort by increasing prefix lengths",
            "Use hash tables",
            "Compare all pairs",
            "Recursive divide"
          ],
          "correct": 0
        },
        {
          "question": "How many iterations does doubling take?",
          "options": [
            "N",
            "N/2",
            "log N",
            "N²"
          ],
          "correct": 2
        },
        {
          "question": "What's the time complexity of doubling algorithm?",
          "options": [
            "O(N)",
            "O(N log N)",
            "O(N log² N)",
            "O(N²)"
          ],
          "correct": 2
        },
        {
          "question": "What are suffix arrays used for?",
          "options": [
            "Only sorting",
            "Pattern search, LCP, compression",
            "Just visualization",
            "Memory allocation"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "strings-5",
      "title": "Manacher's (Longest Palindrome)",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/longest-palindromic-substring/",
      "related": [
        {
          "id": "strings-4",
          "title": "Suffix Array Construction",
          "category": "strings"
        },
        {
          "id": "dp-1",
          "title": "Edit Distance",
          "category": "dp"
        }
      ],
      "tags": [
        "manacher",
        "palindrome",
        "strings"
      ],
      "problem_statement": "Given a string `s`, return the **longest palindromic substring** in `s`.\n\n**Example 1:**\n```\nInput: s = \"babad\"\nOutput: \"bab\"\nExplanation: \"aba\" is also a valid answer.\n```",
      "explanation": {
        "understanding_the_problem": "We need to find the longest substring (contiguous part of the string) that reads the same forwards and backwards.",
        "brute_force": "A common approach is 'Expand from Center'. We can iterate through every possible center of a palindrome. A center can be a single character (for odd-length palindromes) or the space between two characters (for even-length). From each center, we expand outwards as long as the characters match. This takes O(N²) time.",
        "bottleneck": "The 'Expand from Center' approach is O(N²) because it re-scans the same characters many times. For example, when checking the palindrome 'abacaba', we will re-scan the 'aba' part multiple times.",
        "optimized_approach": "Manacher's algorithm is a clever linear-time O(N) solution. It avoids redundant comparisons by exploiting the symmetric nature of palindromes. The algorithm maintains the center `C` and right boundary `R` of the palindrome found so far that reaches the farthest to the right.\n\nWhen we are at a new position `i`, if `i` is within the current rightmost palindrome `[L, R]`, we can use the information from its 'mirror' position on the other side of `C` to get an initial, guaranteed-minimum radius for the palindrome at `i`. We only need to perform character comparisons to try and expand *beyond* this initial radius.",
        "algorithm_steps": "1.  **Preprocessing:** Transform the string to handle odd and even length palindromes uniformly. For example, `s = \"aba\"` becomes `t = \"#a#b#a#\"`.\n2.  **Initialization:** Create a `P` array of the same size as `t`, where `P[i]` will store the radius of the palindrome centered at `t[i]`. Initialize `C` (center) and `R` (right boundary) to 0.\n3.  **Main Loop:** Iterate `i` through the transformed string `t`.\n    a.  If `i < R`, use the mirror property to initialize `P[i]` to a minimum guaranteed value.\n    b.  Attempt to expand the palindrome centered at `i` by comparing characters.\n    c.  If the palindrome at `i` expands beyond `R`, update `C` and `R` to this new palindrome's boundaries.\n4.  **Result:** Find the maximum value in the `P` array. This corresponds to the radius of the longest palindromic substring. Convert this radius back to a length in the original string."
      },
      "code": "function manacher(s) {\n  const t = '#' + s.split('').join('#') + '#';\n  const n = t.length;\n  const P = new Array(n).fill(0);\n  let C = 0, R = 0;\n  for (let i = 1; i < n - 1; i++) {\n    const mirror = 2*C - i;\n    if (i < R) {\n      P[i] = Math.min(R - i, P[mirror]);\n    }\n    while (i + 1 + P[i] < n && i - 1 - P[i] >= 0 &&\n           t[i + 1 + P[i]] === t[i - 1 - P[i]]) {\n      P[i]++;\n    }\n    if (i + P[i] > R) {\n      C = i;\n      R = i + P[i];\n    }\n  }\n  let maxLen = 0, centerIndex = 0;\n  for (let i = 1; i < n - 1; i++) {\n    if (P[i] > maxLen) {\n      maxLen = P[i];\n      centerIndex = i;\n    }\n  }\n  const start = Math.floor((centerIndex - 1 - maxLen) / 2);\n  return s.slice(start, start + maxLen);\n}",
      "complexity": {
        "time": "O(N)",
        "space": "O(N)",
        "explanation_time": "Although there is a `while` loop inside the main `for` loop, the right boundary `R` only ever moves forward through the string. The total number of character comparisons performed by the expansion `while` loop across the entire algorithm is amortized to O(N).",
        "explanation_space": "We need to store the transformed string `t` and the palindrome radius array `P`, both of which are proportional to the length of the original string `s`."
      },
      "annotations": [
        {
          "lines": [
            2
          ],
          "text": "Transform string: 'aba' -> '#a#b#a#' to handle even/odd lengths."
        },
        {
          "lines": [
            7
          ],
          "text": "Mirror property: Find index 'mirror' symmetric to 'i' around 'C'."
        },
        {
          "lines": [
            8
          ],
          "text": "Optimization: Use mirror's P value to skip known palindromic region."
        },
        {
          "lines": [
            10
          ],
          "text": "Expand: Check characters beyond the known radius."
        },
        {
          "lines": [
            13
          ],
          "text": "Update: New palindrome goes beyond R, so move center C."
        }
      ],
      "quizzes": [
        {
          "question": "How does Manacher handle odd and even length palindromes?",
          "options": [
            "Separate loops",
            "Transform string with #",
            "Ignore even",
            "Binary search"
          ],
          "correct": 1
        },
        {
          "question": "What's the key optimization in Manacher's?",
          "options": [
            "Sorting",
            "Using palindrome mirror property",
            "Hashing",
            "Divide and conquer"
          ],
          "correct": 1
        },
        {
          "question": "What does P[i] store?",
          "options": [
            "Character code",
            "Palindrome radius at i",
            "String length",
            "Index of mirror"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity of Manacher's?",
          "options": [
            "O(N²)",
            "O(N log N)",
            "O(N)",
            "O(2^N)"
          ],
          "correct": 2
        },
        {
          "question": "What makes the inner while loop O(N) total?",
          "options": [
            "Fixed iterations",
            "Right boundary R only moves forward",
            "Early termination",
            "Tail recursion"
          ],
          "correct": 1
        }
      ]
    }
  ],
  "math": [
    {
      "id": "math-1",
      "title": "Modular Exponentiation",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/powx-n/",
      "related": [
        {
          "id": "math-2",
          "title": "Sieve of Eratosthenes",
          "category": "math"
        },
        {
          "id": "math-3",
          "title": "Extended Euclidean Algorithm",
          "category": "math"
        }
      ],
      "tags": [
        "modular",
        "exponentiation",
        "fast-power"
      ],
      "problem_statement": "Calculate (a^b) mod m efficiently, where a, b, and m can be large numbers.",
      "explanation": {
        "understanding_the_problem": "We need to compute the remainder of a number `a` raised to a large power `b` when divided by `m`. A standard loop or `Math.pow` would be too slow if `b` is large, and would also lead to overflow issues with massive intermediate numbers.",
        "brute_force": "The naive approach is to use a loop to multiply `a` by itself `b` times, taking the modulus at each step to prevent overflow. This has a time complexity of O(b), which is not feasible for large `b` (e.g., 10^18).",
        "bottleneck": "The O(b) complexity is the bottleneck. We need a way to calculate powers much faster.",
        "optimized_approach": "The solution is **Binary Exponentiation** (also known as Exponentiation by Squaring). This O(log b) algorithm is based on the principle that any exponent `b` can be written as a sum of powers of 2. For example, `a^13 = a^(8+4+1) = a^8 * a^4 * a^1`.\n\nWe can find the terms `a^1, a^2, a^4, a^8, ...` by repeatedly squaring `a`. Then, we only multiply the terms that correspond to the set bits in the binary representation of `b`.",
        "algorithm_steps": "1.  Initialize `result = 1`.\n2.  Reduce `a = a % m`.\n3.  Loop while `b > 0`:\n    a.  If `b` is odd (i.e., the last bit is 1), it means the current power of `a` is part of our product. So, multiply `result = (result * a) % m`.\n    b.  In any case, we need the next power of `a`, so we square it: `a = (a * a) % m`.\n    c.  Right-shift `b` by one (`b = floor(b / 2)`), effectively moving to its next bit."
      },
      "code": "function modPow(a, b, mod) {\n  let result = 1;\n  a = a % mod;\n  while (b > 0) {\n    if (b % 2 === 1) {\n      result = (result * a) % mod;\n    }\n    a = (a * a) % mod;\n    b = Math.floor(b / 2);\n  }\n  return result;\n}",
      "complexity": {
        "time": "O(log b)",
        "space": "O(1)",
        "explanation_time": "The loop runs as many times as there are bits in the exponent `b`, which is approximately log₂(b). Each step inside the loop is a constant time operation.",
        "explanation_space": "The algorithm uses only a few variables to store its state, requiring constant O(1) space."
      },
      "annotations": [
        {
          "lines": [
            2,
            3
          ],
          "text": "Modulus Op: Reduce 'a' immediately. 'result' initialized to 1 (identity)."
        },
        {
          "lines": [
            5
          ],
          "text": "Bit Set: If current bit is 1 (b is odd), multiply current power of 'a' into result."
        },
        {
          "lines": [
            8
          ],
          "text": "Squaring: Move to next power of 2 (a^2, a^4, etc.) by squaring base."
        },
        {
          "lines": [
            9
          ],
          "text": "Shift: Move to next bit of exponent (divide by 2)."
        }
      ],
      "quizzes": [
        {
          "question": "What technique gives O(log b) for a^b?",
          "options": [
            "Linear loop",
            "Binary exponentiation",
            "Recursion only",
            "Matrix multiplication"
          ],
          "correct": 1
        },
        {
          "question": "Why square the base each iteration?",
          "options": [
            "Randomization",
            "Move to next power of 2",
            "Reduce memory",
            "Handle negatives"
          ],
          "correct": 1
        },
        {
          "question": "When do we multiply result by current base?",
          "options": [
            "Always",
            "When b is odd (bit is 1)",
            "When b is even",
            "Never"
          ],
          "correct": 1
        },
        {
          "question": "Why take mod at each step?",
          "options": [
            "Faster",
            "Prevent overflow",
            "Required by algorithm",
            "Improves precision"
          ],
          "correct": 1
        },
        {
          "question": "What's the space complexity?",
          "options": [
            "O(N)",
            "O(log b)",
            "O(1)",
            "O(b)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "math-2",
      "title": "Sieve of Eratosthenes",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/count-primes/",
      "related": [
        {
          "id": "math-1",
          "title": "Modular Exponentiation",
          "category": "math"
        },
        {
          "id": "math-3",
          "title": "Extended Euclidean Algorithm",
          "category": "math"
        }
      ],
      "tags": [
        "sieve",
        "primes",
        "factorization"
      ],
      "problem_statement": "Given an integer N, find all prime numbers up to N.",
      "explanation": {
        "understanding_the_problem": "We need an efficient way to generate a list of all primes up to a given limit N.",
        "brute_force": "A simple method is to iterate through each number `i` from 2 to N and, for each `i`, check if it's prime. A primality test for `i` involves checking for divisors from 2 up to `sqrt(i)`. This approach has a time complexity of roughly O(N * sqrt(N)), which is too slow for large N.",
        "bottleneck": "The naive approach performs redundant checks. For example, it will check that 12 is divisible by 2, 3, 4, and 6, when just finding it's divisible by 2 is enough to know it's not prime.",
        "optimized_approach": "The **Sieve of Eratosthenes** is a highly efficient algorithm that works by iteratively marking as composite the multiples of each prime. Instead of checking for factors, it eliminates multiples.\n\nThe key idea is that if we find a number `p` that has not been marked as composite yet, it must be a prime. We can then mark all of its multiples (`2p, 3p, 4p, ...`) as not prime. We can further optimize this by starting to mark multiples from `p*p`, because any smaller multiple `k*p` (where `k < p`) would have already been marked by the prime factor `k`.",
        "algorithm_steps": "1.  Create a boolean array `isPrime` of size `N+1`, and initialize all entries from 2 to N as `true`.\n2.  Iterate from `p = 2` up to `sqrt(N)`.\n3.  If `isPrime[p]` is `true`, then `p` is a prime number.\n4.  For this prime `p`, iterate through its multiples starting from `p*p` and mark them as not prime: `isPrime[j] = false` for `j = p*p, p*p+p, ...` up to N.\n5.  Finally, iterate through the `isPrime` array and collect all indices that are still marked as `true`."
      },
      "code": "function sieve(N) {\n  const isPrime = new Array(N + 1).fill(true);\n  isPrime[0] = isPrime[1] = false;\n  for (let p = 2; p * p <= N; p++) {\n    if (isPrime[p]) {\n      for (let multiple = p * p; multiple <= N; multiple += p) {\n        isPrime[multiple] = false;\n      }\n    }\n  }\n  const primes = [];\n  for (let i = 2; i <= N; i++) {\n    if (isPrime[i]) primes.push(i);\n  }\n  return primes;\n}",
      "complexity": {
        "time": "O(N log log N)",
        "space": "O(N)",
        "explanation_time": "The time complexity is a result of the harmonic series of prime reciprocals. For each prime p, we do N/p work. The sum over all primes gives O(N log log N), which is very close to linear.",
        "explanation_space": "We need a boolean array of size N to store the primality information for each number."
      },
      "annotations": [
        {
          "lines": [
            4
          ],
          "text": "Skip Composite: If isPrime[p] is false, we already processed its factors."
        },
        {
          "lines": [
            5
          ],
          "text": "Optimization: Start marking multiples from p*p. Smaller multiples (k*p where k < p) are already handled."
        },
        {
          "lines": [
            6
          ],
          "text": "Mark: Set multiple to false (composite)."
        }
      ],
      "quizzes": [
        {
          "question": "What's the main idea of the Sieve?",
          "options": [
            "Check each number",
            "Mark multiples of primes as composite",
            "Factorize all numbers",
            "Sort primes"
          ],
          "correct": 1
        },
        {
          "question": "Why start marking multiples from p*p?",
          "options": [
            "Faster to compute",
            "Smaller multiples already marked",
            "Prevents overflow",
            "Mathematical requirement"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity?",
          "options": [
            "O(N)",
            "O(N log N)",
            "O(N log log N)",
            "O(N²)"
          ],
          "correct": 2
        },
        {
          "question": "What does isPrime[i] = true mean?",
          "options": [
            "i is composite",
            "i is prime (not yet marked)",
            "i is even",
            "i is odd"
          ],
          "correct": 1
        },
        {
          "question": "What's the space complexity?",
          "options": [
            "O(1)",
            "O(log N)",
            "O(N)",
            "O(N²)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "math-3",
      "title": "Extended Euclidean Algorithm",
      "difficulty": "Medium",
      "related": [
        {
          "id": "math-1",
          "title": "Modular Exponentiation",
          "category": "math"
        },
        {
          "id": "math-4",
          "title": "Chinese Remainder Theorem",
          "category": "math"
        }
      ],
      "tags": [
        "gcd",
        "modular-inverse",
        "number-theory"
      ],
      "problem_statement": "Given two integers `a` and `b`, find integers `x` and `y` such that `ax + by = gcd(a, b)`. This is a fundamental algorithm, often used to find the modular multiplicative inverse.",
      "explanation": {
        "understanding_the_problem": "The standard Euclidean algorithm finds the `gcd(a, b)`. The extended version does that and also finds the integer coefficients `x` and `y` that satisfy Bézout's identity.",
        "brute_force": "The standard Euclidean algorithm is based on the recurrence `gcd(a, b) = gcd(b, a % b)`. The extended version is an augmentation of this process. It keeps track of the coefficients at each recursive step.",
        "bottleneck": "There isn't a 'brute force' in the typical sense. The algorithm itself is the efficient solution.",
        "optimized_approach": "The algorithm is recursive. The base case is when `b = 0`, where `gcd(a, 0) = a`. The equation is `a*x + 0*y = a`, so `x=1, y=0` is a trivial solution.\n\nFor the recursive step, we call the function on `(b, a % b)`. This gives us `g, x1, y1` such that `b*x1 + (a % b)*y1 = g`. We then substitute `a % b = a - floor(a/b) * b` back into this equation and rearrange the terms to find the new `x` and `y` that correspond to `a` and `b`.",
        "algorithm_steps": "1.  **Base Case:** If `b` is 0, return `[a, 1, 0]` representing `g, x, y`.\n2.  **Recursive Step:** Call `extendedGCD(b, a % b)` to get `[g, x1, y1]`.\n3.  The new `x` will be `y1`.\n4.  The new `y` will be `x1 - floor(a / b) * y1`.\n5.  Return `[g, x, y]`.\n\n**Modular Inverse:** To find `a^-1 mod m`, we solve `ax + my = 1`. The `x` value returned by `extendedGCD(a, m)` is the modular inverse (if the gcd is 1)."
      },
      "code": "function extendedGCD(a, b) {\n  if (b === 0) {\n    return [a, 1, 0];\n  }\n  const [g, x1, y1] = extendedGCD(b, a % b);\n  return [g, y1, x1 - Math.floor(a / b) * y1];\n}\n\nfunction modInverse(a, mod) {\n  const [g, x, y] = extendedGCD(a, mod);\n  if (g !== 1) {\n    return -1;\n  }\n  return ((x % mod) + mod) % mod;\n}",
      "complexity": {
        "time": "O(log(min(a,b)))",
        "space": "O(log(min(a,b)))",
        "explanation_time": "The number of recursive calls is the same as the standard Euclidean algorithm, which is logarithmic in the size of the smaller input.",
        "explanation_space": "The space complexity is determined by the depth of the recursion stack, which is also logarithmic."
      },
      "annotations": [
        {
          "lines": [
            2
          ],
          "text": "Base Case: gcd(a, 0) = a. Solution: 1*a + 0*0 = a."
        },
        {
          "lines": [
            5
          ],
          "text": "Recursive Call: Solve for gcd(b, a % b). Returns [g, x1, y1]."
        },
        {
          "lines": [
            6
          ],
          "text": "Update Coefficients: x = y1, y = x1 - floor(a/b)*y1. (Based on b*x1 + (a%b)*y1 = g)"
        }
      ],
      "quizzes": [
        {
          "question": "What equation does Extended Euclidean solve?",
          "options": [
            "ax = b mod m",
            "ax + by = gcd(a,b)",
            "a^b mod m",
            "ab = c"
          ],
          "correct": 1
        },
        {
          "question": "What is the base case when b = 0?",
          "options": [
            "x=0, y=1",
            "x=1, y=0",
            "x=a, y=b",
            "No solution"
          ],
          "correct": 1
        },
        {
          "question": "How is modular inverse found?",
          "options": [
            "Division",
            "Solve ax + my = 1 for x",
            "Exponentiation",
            "Random search"
          ],
          "correct": 1
        },
        {
          "question": "When does modular inverse exist?",
          "options": [
            "Always",
            "When gcd(a,m) = 1",
            "When a > m",
            "When m is prime"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity?",
          "options": [
            "O(N)",
            "O(log(min(a,b)))",
            "O(ab)",
            "O(1)"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "math-4",
      "title": "Chinese Remainder Theorem",
      "difficulty": "Medium",
      "related": [
        {
          "id": "math-3",
          "title": "Extended Euclidean Algorithm",
          "category": "math"
        },
        {
          "id": "math-5",
          "title": "FFT",
          "category": "math"
        }
      ],
      "tags": [
        "crt",
        "modular",
        "number-theory"
      ],
      "problem_statement": "Solve a system of simultaneous congruences. Given `x \u2261 a_i (mod m_i)` for `i = 1...k`, find `x`. The standard version assumes all `m_i` are pairwise coprime.",
      "explanation": {
        "understanding_the_problem": "We are looking for a single number `x` that leaves a specific remainder `a_i` when divided by a specific number `m_i`, for a whole set of `(a_i, m_i)` pairs.",
        "brute_force": "One could start at `a_1` and keep adding `m_1` (`x = a_1, a_1+m_1, a_1+2m_1, ...`) and for each resulting number, check if it satisfies all the other congruences. This is extremely slow.",
        "bottleneck": "The search space is huge. We need a constructive method to build the solution.",
        "optimized_approach": "The theorem guarantees a unique solution for `x` modulo `M` (where `M` is the product of all `m_i`). A practical way to find this solution is to solve the system one congruence at a time and merge the results.\n\nStart with the first congruence, `x \u2261 a_1 (mod m_1)`. This tells us `x` must be of the form `a_1 + k*m_1`. We substitute this into the second congruence and solve for `k`. This gives us a new, combined congruence `x \u2261 a_new (mod m_1*m_2)`. We can repeat this process, merging one congruence at a time, until we have the final answer.",
        "algorithm_steps": "1.  Start with the first solution `ans = a_1`, `mod = m_1`.\n2.  For each subsequent congruence `x \u2261 a_i (mod m_i)`:\n    a. We need to find `k` such that `ans + k*mod \u2261 a_i (mod m_i)`.\n    b. This can be solved for `k` using the modular inverse, which we find with the Extended Euclidean Algorithm.\n    c. Once `k` is found, update the answer: `ans = ans + k*mod`.\n    d. Update the combined modulus: `mod = mod * m_i`.\n3. Repeat for all congruences."
      },
      "code": "function solveCRT(a, m) {\n  function merge(a1, m1, a2, m2) {\n    const [g, p, q] = extendedGCD(m1, m2);\n    if ((a2 - a1) % g !== 0) {\n      return [-1, 0];\n    }\n    const lcm = (m1 / g) * m2;\n    const x = (a1 + ((a2 - a1) / g * p % (m2 / g)) * m1) % lcm;\n    return [(x % lcm + lcm) % lcm, lcm];\n  }\n  let [ans, mod] = [a[0], m[0]];\n  for (let i = 1; i < a.length; i++) {\n    [ans, mod] = merge(ans, mod, a[i], m[i]);\n    if (ans === -1) return [-1, 0];\n  }\n  return ans;\n}",
      "complexity": {
        "time": "O(k * log M)",
        "space": "O(k)",
        "explanation_time": "Where `k` is the number of equations and `M` is the product of the moduli. Each of the `k-1` merge steps involves a call to the Extended Euclidean Algorithm, which is logarithmic in the size of the moduli.",
        "explanation_space": "The inputs are stored in arrays of size k."
      },
      "annotations": [
        {
          "lines": [
            3
          ],
          "text": "Modular Inverse: Solve for 'p' in Bezout equation relative to moduli."
        },
        {
          "lines": [
            7
          ],
          "text": "Merge: Combine two congruences into one. New x satisfies both."
        },
        {
          "lines": [
            12
          ],
          "text": "Iterative Merge: Combine current answer with next congruence."
        }
      ],
      "quizzes": [
        {
          "question": "What does CRT solve?",
          "options": [
            "Polynomial equations",
            "System of congruences",
            "Prime factorization",
            "Matrix equations"
          ],
          "correct": 1
        },
        {
          "question": "What condition is needed for standard CRT?",
          "options": [
            "All m_i equal",
            "All m_i pairwise coprime",
            "All a_i prime",
            "m_i sorted"
          ],
          "correct": 1
        },
        {
          "question": "How does the iterative approach work?",
          "options": [
            "Solve all at once",
            "Merge two congruences at a time",
            "Binary search",
            "Random sampling"
          ],
          "correct": 1
        },
        {
          "question": "What algorithm is used in the merge step?",
          "options": [
            "Sieve",
            "Extended Euclidean",
            "FFT",
            "Binary exponentiation"
          ],
          "correct": 1
        },
        {
          "question": "Is the solution unique?",
          "options": [
            "No",
            "Yes, modulo product of all m_i",
            "Only for primes",
            "Depends on a_i"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "math-5",
      "title": "Fast Fourier Transform (FFT)",
      "difficulty": "Medium",
      "related": [
        {
          "id": "math-4",
          "title": "Chinese Remainder Theorem",
          "category": "math"
        },
        {
          "id": "math-1",
          "title": "Modular Exponentiation",
          "category": "math"
        }
      ],
      "tags": [
        "fft",
        "polynomial-multiplication",
        "number-theory"
      ],
      "problem_statement": "Multiply two polynomials of degree N efficiently.",
      "explanation": {
        "understanding_the_problem": "Multiplying two polynomials A(x) and B(x) of degree N using the standard 'long multiplication' method takes O(N²) time. FFT is a highly advanced algorithm that can accomplish this in O(N log N) time.",
        "brute_force": "The O(N²) schoolbook method is the naive approach.",
        "bottleneck": "The O(N²) complexity is too slow for problems involving large polynomial multiplication, such as finding all possible sums of two large sets of numbers (which can be modeled as polynomial multiplication).",
        "optimized_approach": "The core idea of FFT is to change the representation of the polynomials. A polynomial can be represented by its coefficients or by its values at a set of points (point-value form). Multiplying two polynomials in point-value form is trivial: you just multiply their values at each point, which takes O(N) time.\n\n1. **Evaluation:** The FFT algorithm evaluates a polynomial at a specific set of `2N` points (the complex 'roots of unity') in O(N log N) time.\n2. **Pointwise Multiplication:** Multiply the values at each point. O(N).\n3. **Interpolation:** The Inverse FFT algorithm converts the resulting point-values back into the coefficient representation of the final polynomial, also in O(N log N) time.",
        "algorithm_steps": "1.  Given two polynomials A(x) and B(x), convert them to point-value form by applying FFT. This is a recursive, divide-and-conquer algorithm.\n2.  Perform pointwise multiplication of the results.\n3.  Apply the Inverse FFT to the new point-value pairs to get the coefficients of the resulting polynomial C(x)."
      },
      "code": "function fft(a, invert = false) {\n  const n = a.length;\n  const rev = new Array(n).fill(0);\n  const bits = Math.log2(n);\n  for (let i = 0; i < n; i++) {\n    rev[i] = (rev[i >> 1] >> 1) | (i & 1) << (bits - 1);\n  }\n  for (let i = 0; i < n; i++) {\n    if (i < rev[i]) [a[i], a[rev[i]]] = [a[rev[i]], a[i]];\n  }\n  for (let len = 2; len <= n; len *= 2) {\n    const ang = 2 * Math.PI / len * (invert ? -1 : 1);\n    const wlen = [Math.cos(ang), Math.sin(ang)];\n    for (let i = 0; i < n; i += len) {\n      let u = [1, 0];\n      for (let j = 0; j < len/2; j++) {\n        const v = [a[i+j+len/2][0] * u[0] - a[i+j+len/2][1] * u[1],\n                 a[i+j+len/2][0] * u[1] + a[i+j+len/2][1] * u[0]];\n        [a[i+j], a[i+j+len/2]] = [[a[i+j][0] + v[0], a[i+j][1] + v[1]],\n                                         [a[i+j][0] - v[0], a[i+j][1] - v[1]]];\n        u = [u[0] * wlen[0] - u[1] * wlen[1],\n              u[0] * wlen[1] + u[1] * wlen[0]];\n      }\n    }\n  }\n  if (invert) {\n    for (let i = 0; i < n; i++) {\n      a[i] = [a[i][0] / n, a[i][1] / n];\n    }\n  }\n  return a;\n}",
      "complexity": {
        "time": "O(N log N)",
        "space": "O(N)",
        "explanation_time": "The FFT algorithm is a divide-and-conquer algorithm. The recurrence relation is T(N) = 2*T(N/2) + O(N), which solves to O(N log N).",
        "explanation_space": "The algorithm requires space to hold the input arrays and various temporary values, proportional to the degree of the polynomials, resulting in O(N) space."
      },
      "annotations": [
        {
          "lines": [
            5
          ],
          "text": "Bit-Reversal Permutation: Reorder array so recursion can be done iteratively."
        },
        {
          "lines": [
            10
          ],
          "text": "Roots of Unity: Calculate complex angle for current level of recursion."
        },
        {
          "lines": [
            15,
            16
          ],
          "text": "Butterfly: Update pairs in place using the 'twiddle factor' wlen."
        }
      ],
      "quizzes": [
        {
          "question": "What problem does FFT solve efficiently?",
          "options": [
            "Sorting",
            "Polynomial multiplication",
            "Graph traversal",
            "String matching"
          ],
          "correct": 1
        },
        {
          "question": "What's the key insight of FFT?",
          "options": [
            "Sort coefficients",
            "Convert to point-value form",
            "Use hashing",
            "Binary search"
          ],
          "correct": 1
        },
        {
          "question": "What are 'roots of unity'?",
          "options": [
            "Prime numbers",
            "Complex numbers where z^n = 1",
            "Square roots",
            "Fibonacci numbers"
          ],
          "correct": 1
        },
        {
          "question": "What's the time complexity of FFT?",
          "options": [
            "O(N²)",
            "O(N)",
            "O(N log N)",
            "O(N³)"
          ],
          "correct": 2
        },
        {
          "question": "How does inverse FFT differ?",
          "options": [
            "Different algorithm",
            "Negate angle and divide by N",
            "Only for integers",
            "Uses recursion"
          ],
          "correct": 1
        }
      ]
    }
  ],
  "geometry": [
    {
      "id": "geometry-1",
      "title": "Point in Polygon Test",
      "difficulty": "Medium",
      "related": [
        {
          "id": "geometry-2",
          "title": "Line Segment Intersection",
          "category": "geometry"
        },
        {
          "id": "geometry-3",
          "title": "Convex Hull",
          "category": "geometry"
        }
      ],
      "tags": [
        "geometry",
        "polygon",
        "point-in-polygon"
      ],
      "problem_statement": "Given a point P and a simple polygon (no self-intersections), determine if P is inside, outside, or on the boundary of the polygon.",
      "explanation": {
        "understanding_the_problem": "We need to determine if a given point lies inside a polygon. This is a fundamental problem in computational geometry.",
        "brute_force": "The **Ray Casting Algorithm** is a standard and robust method. The idea is to draw a ray (a semi-infinite line) from the point in any fixed direction (e.g., horizontally to the right) and count how many times it intersects with the edges of the polygon.",
        "bottleneck": "The main challenge is correctly handling all edge cases, such as the ray passing directly through a vertex or overlapping with a horizontal edge of the polygon. The intersection logic must be precise.",
        "optimized_approach": "If the number of intersections is **odd**, the point is **inside**. If the number of intersections is **even**, the point is **outside**. This works because each time the ray crosses an edge, it flips from being inside to outside or vice-versa. Starting from the 'outside' (infinity), an odd number of flips means it must end up 'inside'.",
        "algorithm_steps": "1.  Initialize an intersection counter to 0.\n2.  Iterate through each edge of the polygon (from vertex `i` to `i+1`).\n3.  For each edge, determine if the horizontal ray starting from our point `P` intersects it. This involves checking if the `y`-coordinates of the edge's endpoints are on opposite sides of the ray, and if the `x`-coordinate of the intersection point is to the right of `P`.\n4.  If a valid intersection occurs, increment the counter.\n5.  After checking all edges, if the counter is odd, return `true`; otherwise, return `false`."
      },
      "code": "function pointInPolygon(P, polygon) {\n  const [px, py] = P;\n  let inside = false;\n  for (let i = 0, j = polygon.length - 1; i < polygon.length; j = i++) {\n    const [xi, yi] = polygon[i];\n    const [xj, yj] = polygon[j];\n    const intersect = ((yi > py) !== (yj > py)) &&\n                     (px < (xj - xi) * (py - yi) / (yj - yi) + xi);\n    if (intersect) inside = !inside;\n  }\n  return inside;\n}",
      "complexity": {
        "time": "O(N)",
        "space": "O(1)",
        "explanation_time": "For each query, we must iterate through all N edges of the polygon once.",
        "explanation_space": "The algorithm uses only a few variables to store state, requiring constant space."
      },
      "annotations": [
        {
          "lines": [
            4
          ],
          "text": "Ray Casting: For each edge (i, j), check intersection with horizontal ray from point P."
        },
        {
          "lines": [
            7
          ],
          "text": "Y-Condition: Checks if point P's y-coord is between edge endpoints' y-coords."
        },
        {
          "lines": [
            8
          ],
          "text": "X-Condition: Checks if the edge intersection, at P's y-level, is to the right of P."
        },
        {
          "lines": [
            9
          ],
          "text": "Parity Flip: If intersect, toggle 'inside' state (odd=in, even=out)."
        }
      ],
      "quizzes": [
        {
          "question": "What is the standard algorithm for Point in Polygon?",
          "options": [
            "Ray Casting",
            "Dijkstra",
            "Convex Hull",
            "Rotating Calipers"
          ],
          "correct": 0
        },
        {
          "question": "If a ray has 3 intersections, is the point inside?",
          "options": [
            "No",
            "Yes (Odd)",
            "Maybe",
            "Only for convex"
          ],
          "correct": 1
        },
        {
          "question": "What direction do we typically cast the ray?",
          "options": [
            "Random",
            "Horizontal (Right)",
            "Vertical (Up)",
            "Diagonal"
          ],
          "correct": 1
        },
        {
          "question": "How do we handle the ray passing through a vertex?",
          "options": [
            "Ignore it",
            "Count as 0.5",
            "Careful inequality checks (epsilon)",
            "Restart with new ray"
          ],
          "correct": 2
        },
        {
          "question": "What is the time complexity per query?",
          "options": [
            "O(1)",
            "O(log N)",
            "O(N)",
            "O(N²)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "geometry-2",
      "title": "Line Segment Intersection",
      "difficulty": "Medium",
      "related": [
        {
          "id": "geometry-1",
          "title": "Point in Polygon",
          "category": "geometry"
        },
        {
          "id": "geometry-3",
          "title": "Convex Hull",
          "category": "geometry"
        }
      ],
      "tags": [
        "geometry",
        "line-intersection",
        "orientation"
      ],
      "problem_statement": "Determine if two line segments, `(p1, q1)` and `(p2, q2)`, intersect.",
      "explanation": {
        "understanding_the_problem": "We need to check if two finite line segments cross each other.",
        "brute_force": "Solving the linear equations for the lines and checking if the intersection point lies within the bounds of both segments is possible, but it involves division and floating-point arithmetic, which can be slow and prone to precision errors.",
        "bottleneck": "Division and floating-point numbers are best avoided in computational geometry if possible.",
        "optimized_approach": "A much more robust method uses the concept of **orientation**. The orientation of an ordered triplet of points `(p, q, r)` can be determined by the sign of the cross-product of the vectors `(q-p)` and `(r-q)`. The orientation can be collinear, clockwise, or counter-clockwise.\n\nTwo segments `(p1, q1)` and `(p2, q2)` intersect if and only if the endpoints of each segment are on opposite sides of the other segment. We can check this with orientation tests.",
        "algorithm_steps": "Two segments intersect if and only if one of these two conditions is met:\n\n1.  **General Case:** The orientation of `(p1, q1, p2)` and `(p1, q1, q2)` are different, AND the orientation of `(p2, q2, p1)` and `(p2, q2, p2)` are different. This means the segments properly cross each other.\n\n2.  **Special Case (Collinear):** The orientations are all collinear, and the segments overlap. This requires an additional check to see if, for example, `p2` lies on the 1D segment `p1q1`."
      },
      "code": "function onSegment(p, q, r) {\n  const min_x = Math.min(p[0], r[0]), max_x = Math.max(p[0], r[0]);\n  const min_y = Math.min(p[1], r[1]), max_y = Math.max(p[1], r[1]);\n  return (q[0] >= min_x && q[0] <= max_x && q[1] >= min_y && q[1] <= max_y);\n}\n\nfunction orientation(p, q, r) {\n  const val = (q[1] - p[1]) * (r[0] - q[0]) - (q[0] - p[0]) * (r[1] - q[1]);\n  if (val > 0) return 1;\n  if (val < 0) return 2;\n  return 0;\n}\n\nfunction segmentsIntersect(p1, p2, p3, p4) {\n  const o1 = orientation(p1, p2, p3);\n  const o2 = orientation(p1, p2, p4);\n  const o3 = orientation(p3, p4, p1);\n  const o4 = orientation(p3, p4, p2);\n  if (o1 !== o2 && o3 !== o4) return true;\n  if (o1 === 0 && onSegment(p1, p3, p2)) return true;\n  if (o2 === 0 && onSegment(p1, p4, p2)) return true;\n  if (o3 === 0 && onSegment(p3, p1, p4)) return true;\n  if (o4 === 0 && onSegment(p3, p2, p4)) return true;\n  return false;\n}",
      "complexity": {
        "time": "O(1)",
        "space": "O(1)",
        "explanation_time": "The check involves a constant number of orientation tests and comparisons, regardless of the coordinates.",
        "explanation_space": "No extra space is needed."
      },
      "annotations": [
        {
          "lines": [
            7
          ],
          "text": "Cross Product: Calculates Z-component. Sign indicates orientation (CW/CCW/Collinear)."
        },
        {
          "lines": [
            16
          ],
          "text": "General Case: Segments straddle each other iff orientations differ for both pairs."
        },
        {
          "lines": [
            17
          ],
          "text": "Collinear Case: If 3 points are collinear, check if point lies ON segment."
        }
      ],
      "quizzes": [
        {
          "question": "What is the key geometric primitive used?",
          "options": [
            "Distance",
            "Slope",
            "Orientation (Cross Product)",
            "Midpoint"
          ],
          "correct": 2
        },
        {
          "question": "How many orientation tests are needed for the general case?",
          "options": [
            "1",
            "2",
            "3",
            "4"
          ],
          "correct": 3
        },
        {
          "question": "When do segments intersect in general?",
          "options": [
            "Slopes are different",
            "Lengths are equal",
            "Endpoints straddle each other",
            "They are perpendicular"
          ],
          "correct": 2
        },
        {
          "question": "Why avoid using equation y = mx + c?",
          "options": [
            "Harder to code",
            "Division by zero (vertical lines) & precision",
            "Slower",
            "Memory usage"
          ],
          "correct": 1
        },
        {
          "question": "What is the time complexity?",
          "options": [
            "O(1)",
            "O(log N)",
            "O(N)",
            "O(N²)",
            "O(log N)"
          ],
          "correct": 0
        }
      ]
    },
    {
      "id": "geometry-3",
      "title": "Convex Hull (Monotone Chain)",
      "difficulty": "Medium",
      "leetcode_url": "https://leetcode.com/problems/erect-the-fence/",
      "related": [
        {
          "id": "geometry-2",
          "title": "Line Segment Intersection",
          "category": "geometry"
        },
        {
          "id": "geometry-5",
          "title": "Rotating Calipers",
          "category": "geometry"
        }
      ],
      "tags": [
        "convex-hull",
        "geometry",
        "sorting"
      ],
      "problem_statement": "Given a set of N points, find the smallest convex polygon that contains all of them. This polygon is the Convex Hull.",
      "explanation": {
        "understanding_the_problem": "Imagine placing a rubber band around a set of nails on a board. The shape the rubber band forms is the convex hull. We need to find the vertices of this shape.",
        "brute_force": "There are several algorithms. The one presented here is **Andrew's Algorithm**, also known as the Monotone Chain algorithm. It's efficient and relatively easy to implement.",
        "bottleneck": "The main challenge is determining which points are on the 'outer edge' of the set.",
        "optimized_approach": "The Monotone Chain algorithm works by building the upper and lower hulls of the point set separately and then combining them.\n\nIt relies on the fact that if we process the points in sorted order (by x-coordinate), the hull vertices will be added in order. For each new point, we check if it maintains the convexity of the hull we're building. If adding the new point creates a 'right turn' (for the lower hull) or a 'left turn' (for the upper hull), it means the previous point we added was actually inside the hull, and we must pop it off.",
        "algorithm_steps": "1.  **Sort:** Sort all points lexicographically (first by x-coordinate, then by y-coordinate). This takes O(N log N).\n2.  **Build Lower Hull:** Iterate through the sorted points. Maintain a list for the lower hull. For each point, check if adding it creates a non-left turn with the last two points on the hull. If so, the previous point is not part of the convex hull, so pop it. Keep popping until a left turn is made. Then, add the current point.\n3.  **Build Upper Hull:** Do the same thing, but iterate through the sorted points in reverse order.\n4.  **Combine:** Concatenate the lower and upper hulls (removing duplicate start/end points) to get the final convex hull."
      },
      "code": "function convexHull(points) {\n  points.sort((a, b) => a[0] === b[0] ? a[1] - b[1] : a[0] - b[0]);\n  const cross = (o, a, b) => (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0]);\n  const buildHull = (pts) => {\n    const hull = [];\n    for (const p of pts) {\n      while (hull.length >= 2 && cross(hull[hull.length-2], hull[hull.length-1], p) <= 0) {\n        hull.pop();\n      }\n      hull.push(p);\n    }\n    return hull;\n  };\n  const lower = buildHull(points);\n  const upper = buildHull([...points].reverse());\n  const hull = [...lower.slice(0, -1), ...upper.slice(0, -1)];\n  return hull;\n}",
      "complexity": {
        "time": "O(N log N)",
        "space": "O(N)",
        "explanation_time": "The algorithm is dominated by the initial sort of the points, which takes O(N log N). The subsequent passes to build the hulls each take O(N) time.",
        "explanation_space": "In the worst case, all points could be on the convex hull, so we need O(N) space to store the result."
      },
      "annotations": [
        {
          "lines": [
            2
          ],
          "text": "Sort: Crucial step. Sort by X, then Y, to process points in a 'monotone' order."
        },
        {
          "lines": [
            7
          ],
          "text": "Cross Product Check: If <= 0, we made a right/straight turn (for lower hull), so pop (keep expanding left)."
        },
        {
          "lines": [
            13
          ],
          "text": "Reverse Logic: Building the upper hull works identically but processes points in reverse sorted order."
        }
      ],
      "quizzes": [
        {
          "question": "What is the Convex Hull?",
          "options": [
            "Largest circle inside",
            "Smallest convex polygon containing all points",
            "Average point",
            "Shortest path"
          ],
          "correct": 1
        },
        {
          "question": "What is the first step of Andrew's Algorithm?",
          "options": [
            "Find center",
            "Sort points by X",
            "Random shuffle",
            "Find max Y"
          ],
          "correct": 1
        },
        {
          "question": "When do we pop a point from the hull?",
          "options": [
            "When we make a right turn (non-convex)",
            "When we make a left turn",
            "Never",
            "Every time"
          ],
          "correct": 0
        },
        {
          "question": "What is the time complexity?",
          "options": [
            "O(N)",
            "O(N²)",
            "O(N log N)",
            "O(N!)"
          ],
          "correct": 2
        },
        {
          "question": "Why build lower and upper hulls separately?",
          "options": [
            "It's mandatory",
            "Handles vertical lines better",
            "Simplifies the turn logic (monotone chain)",
            "Faster"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "geometry-4",
      "title": "Closest Pair of Points",
      "difficulty": "Medium",
      "related": [
        {
          "id": "geometry-3",
          "title": "Convex Hull",
          "category": "geometry"
        },
        {
          "id": "geometry-1",
          "title": "Point in Polygon",
          "category": "geometry"
        }
      ],
      "tags": [
        "divide-and-conquer",
        "geometry",
        "closest-pair"
      ],
      "problem_statement": "Given N points in a plane, find the pair of points with the smallest Euclidean distance between them.",
      "explanation": {
        "understanding_the_problem": "We need to find the minimum distance among all possible pairs of points.",
        "brute_force": "The naive approach is to calculate the distance between every pair of points and keep track of the minimum. This involves N*(N-1)/2 pairs, leading to an O(N²) time complexity.",
        "bottleneck": "The O(N²) complexity is too slow for a large number of points.",
        "optimized_approach": "A classic **Divide and Conquer** algorithm can solve this in O(N log N) time.\n\n1.  **Divide:** Sort the points by x-coordinate and split them into two equal halves, `Left` and `Right`, by a vertical line.\n2.  **Conquer:** Recursively find the closest pair distance in `Left` (let's call it `d_L`) and in `Right` (`d_R`). Let `d = min(d_L, d_R)`.\n3.  **Combine:** The closest pair is either `d`, OR it's a 'split pair' where one point is in `Left` and the other is in `Right`. A split pair can only exist if both points are within a `2d`-width 'strip' around the dividing line. A key geometric property proves that for any point in this strip, we only need to check a small, constant number of subsequent points (after sorting them by y-coordinate) to see if there's a closer split pair. This combine step can be done in O(N).",
        "algorithm_steps": "1.  Sort points by x-coordinate.\n2.  Recursively solve for the left and right halves.\n3.  Take the minimum distance `d` from the two halves.\n4.  Create a 'strip' of points within distance `d` of the center line.\n5.  Sort the strip by y-coordinate.\n6.  Iterate through the strip and for each point, check its distance against a constant number of its neighbors. Update `d` if a closer pair is found.\n7.  Return `d`."
      },
      "code": "function dist(p1, p2) {\n  return (p1[0]-p2[0])**2 + (p1[1]-p2[1])**2;\n}\n\nfunction closestPair(points) {\n  const n = points.length;\n  if (n <= 3) return bruteForce(points);\n  points.sort((a, b) => a[0] - b[0]);\n  const mid = Math.floor(n / 2);\n  const midX = points[mid][0];\n  const left = points.slice(0, mid);\n  const right = points.slice(mid);\n  const [dl, pl] = closestPair(left);\n  const [dr, pr] = closestPair(right);\n  let d = Math.min(dl, dr);\n  const strip = points.filter(p => Math.abs(p[0] - midX) < d);\n  strip.sort((a, b) => a[1] - b[1]);\n  for (let i = 0; i < strip.length; i++) {\n    for (let j = i + 1; j < Math.min(i + 7, strip.length); j++) {\n      if (dist(strip[i], strip[j]) < d) {\n        d = dist(strip[i], strip[j]);\n        // In a real implementation, you'd update the pair of points here\n      }\n    }\n  }\n  return d < dl ? [d, pl] : (dr < dl ? [dr, pr] : [dl, pl]);\n}\n\nfunction bruteForce(points) {\n  let minDist = Infinity;\n  let pair = [];\n  for (let i = 0; i < points.length; i++) {\n    for (let j = i + 1; j < points.length; j++) {\n      const d = dist(points[i], points[j]);\n      if (d < minDist) {\n        minDist = d;\n        pair = [points[i], points[j]];\n      }\n    }\n  }\n  return [minDist, pair];\n}",
      "complexity": {
        "time": "O(N log N)",
        "space": "O(N)",
        "explanation_time": "The recurrence relation for the algorithm is T(N) = 2*T(N/2) + O(N) (for the strip processing), which solves to O(N log N).",
        "explanation_space": "The space complexity depends on the implementation of the recursion and sorting, but is typically O(N) or O(N log N)."
      },
      "annotations": [
        {
          "lines": [
            7
          ],
          "text": "Divide: Split sorted points into left and right halves."
        },
        {
          "lines": [
            12
          ],
          "text": "Conquer: Recursively find min dist in both halves. Let d = min(dl, dr)."
        },
        {
          "lines": [
            13
          ],
          "text": "Filter Strip: Only points within distance 'd' of midline can form a closer pair spanning the cut."
        },
        {
          "lines": [
            16
          ],
          "text": "Magic Constant: Geometry guarantees we only need to check next 7 points in sorted Y strip."
        }
      ],
      "quizzes": [
        {
          "question": "What is the time complexity of the Divide and Conquer approach?",
          "options": [
            "O(N²)",
            "O(N log N)",
            "O(N)",
            "O(N!)"
          ],
          "correct": 1
        },
        {
          "question": "Why is brute force O(N²)?",
          "options": [
            "Checks all pairs",
            "Sorting takes N²",
            "Complex math",
            "Memory allocation"
          ],
          "correct": 0
        },
        {
          "question": "What is the 'strip'?",
          "options": [
            "Points far from center",
            "Points within distance 'd' of dividing line",
            "Outliers",
            "Points on X-axis"
          ],
          "correct": 1
        },
        {
          "question": "Why do we only check 7 neighbors in the strip?",
          "options": [
            "It's a heuristic",
            "Geometric pigeonhole principle limit",
            "To save time (approximate)",
            "Hardware limit"
          ],
          "correct": 1
        },
        {
          "question": "What sorting is required?",
          "options": [
            "Only by X",
            "Only by Y",
            "By X initially, and Y for the strip",
            "No sorting"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "geometry-5",
      "title": "Rotating Calipers (Diameter)",
      "difficulty": "Medium",
      "related": [
        {
          "id": "geometry-3",
          "title": "Convex Hull",
          "category": "geometry"
        }
      ],
      "tags": [
        "rotating-calipers",
        "convex-hull",
        "geometry"
      ],
      "problem_statement": "Find the diameter of a set of points, which is the largest distance between any two points in the set.",
      "explanation": {
        "understanding_the_problem": "The diameter of a set of points is determined by two points on its convex hull. Therefore, the first step is always to compute the convex hull of the points.",
        "brute_force": "Once the convex hull is found (an O(N log N) operation), a naive approach would be to check the distance between every pair of vertices on the hull. If the hull has H vertices, this would take O(H²) time.",
        "bottleneck": "The O(H²) check is unnecessary. We can do better.",
        "optimized_approach": "The **Rotating Calipers** algorithm finds the diameter of a convex polygon in O(H) time. The algorithm is based on the property that the diameter is the greatest distance between a pair of **antipodal points**. Antipodal points are two points on the polygon such that you can draw parallel lines of support through them.\n\nImagine two parallel lines 'clamping' the polygon. We 'rotate' these lines around the polygon, keeping them tangent to it. The points they touch form antipodal pairs. We can find all such pairs in a single linear scan around the hull.",
        "algorithm_steps": "1.  Compute the convex hull of the point set. Let the vertices be `h_0, h_1, ..., h_{k-1}`.\n2.  Find an initial antipodal pair. A simple one is the point with the minimum y-coordinate and the point with the maximum y-coordinate.\n3.  Start with two 'calipers' (pointers), `p` and `q`, at this antipodal pair.\n4.  In a loop, advance one of the calipers to the next vertex. The choice of which to advance is based on which caliper has the smaller angle to its next edge. At each step, calculate the distance between the points `h_p` and `h_q` and update the maximum distance found.\n5.  Continue until the calipers have rotated all the way around the polygon (a full 180 degrees)."
      },
      "code": "function rotatingCalipers(hull) {\n  const n = hull.length;\n  if (n < 2) return 0;\n  if (n === 2) return Math.sqrt(dist(hull[0], hull[1]));\n  \n  let i = 0, j = 1;\n  // Find initial antipodal pair (e.g., min/max y)\n  for(let k=0; k<n; k++) {\n    if(hull[k][1] > hull[j][1]) j = k;\n    if(hull[k][1] < hull[i][1]) i = k;\n  }\n\n  let maxDistSq = 0;\n  let p = i, q = j;\n\n  for (let k = 0; k < 2 * n; k++) {\n    maxDistSq = Math.max(maxDistSq, dist(hull[p], hull[q]));\n    // cross product to find which caliper to rotate\n    const cross_product = (hull[(p + 1) % n][0] - hull[p][0]) * (hull[q][1] - hull[(q + 1) % n][1]) - \n                        (hull[(p + 1) % n][1] - hull[p][1]) * (hull[q][0] - hull[(q + 1) % n][0]);\n    if (cross_product > 0) {\n      q = (q + 1) % n;\n    } else {\n      p = (p + 1) % n;\n    }\n  }\n  return Math.sqrt(maxDistSq);\n}\n\nfunction dist(a, b) {\n  return (a[0]-b[0])**2 + (a[1]-b[1])**2;\n}",
      "complexity": {
        "time": "O(N log N)",
        "space": "O(N)",
        "explanation_time": "The complexity is dominated by the initial convex hull computation, which is O(N log N). The rotating calipers algorithm itself runs in O(H) time, where H is the number of vertices on the hull (H <= N).",
        "explanation_space": "We need O(H) or O(N) space to store the vertices of the convex hull."
      },
      "annotations": [
        {
          "lines": [
            7
          ],
          "text": "Init Calipers: Start with min-Y and max-Y vertices as the antipodal pair."
        },
        {
          "lines": [
            16
          ],
          "text": "Caliper Rotation: Use cross product to decide which caliper advances to next vertex to maintain parallel support lines."
        },
        {
          "lines": [
            20
          ],
          "text": "Update: Advance 'q' or 'p' based on which edge is closer to parallel."
        }
      ],
      "quizzes": [
        {
          "question": "What does this algorithm calculate?",
          "options": [
            "Area",
            "Diameter (max distance)",
            "Perimeter",
            "Center of mass"
          ],
          "correct": 1
        },
        {
          "question": "What is the prerequisite step?",
          "options": [
            "Sort by angle",
            "Compute Convex Hull",
            "Triangulate",
            "Find centroid"
          ],
          "correct": 1
        },
        {
          "question": "What are 'antipodal points'?",
          "options": [
            "Opposite points supporting parallel tangents",
            "Points with same x-coord",
            "Points with same y-coord",
            "Adjacent points"
          ],
          "correct": 0
        },
        {
          "question": "What is the time complexity (after hull)?",
          "options": [
            "O(H)",
            "O(H log H)",
            "O(H²)",
            "O(1)"
          ],
          "correct": 0
        },
        {
          "question": "How do we decide which caliper to rotate?",
          "options": [
            "Randomly",
            "Smallest angle to next edge",
            "Largest angle",
            "Fixed order"
          ],
          "correct": 1
        }
      ]
    }
  ],
  "design": [
    {
      "id": "design-1",
      "title": "LRU Cache",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/lru-cache/",
      "follow_up": {
        "scenario": "Your cache is used by 1000 concurrent threads. The global lock is a bottleneck.",
        "trade_off": "Sharding the cache (segments) reduces contention but approximates LRU accuracy globally.",
        "strategy": "Use a `ConcurrentHashMap` with segmented locks (like Java 7) or a Clock algorithm (Second Chance) to avoid full locking.",
        "answering_guide": "Acknowledge the lock contention issue immediately. Proposing <strong>'segmentation' (sharding)</strong> is the standard L5 move here. Mentioning <strong>'Clock Algorithm'</strong> shows depth."
      },
      "related": [
        {
          "id": "design-4",
          "title": "Design Hit Counter",
          "category": "design"
        },
        {
          "id": "design-5",
          "title": "Design Add and Search Words",
          "category": "design"
        }
      ],
      "tags": [
        "design",
        "hash-table",
        "doubly-linked-list"
      ],
      "problem_statement": "Design a data structure that follows the constraints of a **[Least Recently Used (LRU) cache](https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU)**.\n\nImplement the `LRUCache` class:\n*   `LRUCache(int capacity)` Initialize the LRU cache with positive size `capacity`.\n*   `int get(int key)` Return the value of the `key` if the key exists, otherwise return `-1`.\n*   `void put(int key, int value)` Update the value of the `key` if the `key` exists. Otherwise, add the `key-value` pair to the cache. If the number of keys exceeds the `capacity` from this operation, **evict** the least recently used key.\n\nThe functions `get` and `put` must each run in `O(1)` average time complexity.\n\n**Example 1:**\n```\nInput\n[\"LRUCache\", \"put\", \"put\", \"get\", \"put\", \"get\", \"put\", \"get\", \"get\", \"get\"]\n[[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]\nOutput\n[null, null, null, 1, null, -1, null, -1, 3, 4]\n```",
      "explanation": {
        "understanding_the_problem": "We need a data structure that can store key-value pairs up to a certain `capacity`. When we access or add an item, it becomes the 'most recently used'. When we need to make space, we evict the 'least recently used' item. All operations must be very fast (O(1)).",
        "brute_force": "A simple hash map gives O(1) `get` and `put`, but it doesn't track usage order. A linked list can track order (by moving nodes to the front), but `get` would take O(N) to find the item. We need to combine the strengths of both.",
        "bottleneck": "No single standard data structure provides both O(1) key-based lookup and O(1) reordering. The challenge is to link these two capabilities.",
        "optimized_approach": "The optimal solution combines a **Hash Map** and a **Doubly Linked List**.\n\n- The **Hash Map** stores keys and maps them to nodes in the linked list (`key -> Node`). This gives us the O(1) lookup.\n- The **Doubly Linked List** stores the actual key-value pairs in its nodes and maintains the usage order. The head of the list is the most recently used item, and the tail is the least recently used. A doubly linked list is crucial because it allows O(1) removal of any node *if we have a direct pointer to it* (which the hash map provides).",
        "algorithm_steps": "1.  **`get(key)`:**\n    a.  Look up the key in the hash map. If it doesn't exist, return -1.\n    b.  If it exists, we get a direct pointer to the linked list node. Move this node to the head of the list to mark it as most recently used.\n    c.  Return the node's value.\n\n2.  **`put(key, value)`:**\n    a.  Check if the key exists in the map.\n    b.  If yes, update the node's value and move it to the head of the list.\n    c.  If no, create a new node. If the cache is at full capacity, evict the tail of the list and remove its key from the hash map. Then, add the new node to the head of the list and add its `(key, node)` pair to the map."
      },
      "code": "class LRUCache {\n  constructor(capacity) {\n    this.capacity = capacity;\n    this.cache = new Map();\n    this.head = { key: 0, val: 0 };\n    this.tail = { key: 0, val: 0 };\n    this.head.next = this.tail;\n    this.tail.prev = this.head;\n  }\n  \n  addNode(node) {\n    node.prev = this.head;\n    node.next = this.head.next;\n    this.head.next.prev = node;\n    this.head.next = node;\n  }\n  \n  removeNode(node) {\n    node.prev.next = node.next;\n    node.next.prev = node.prev;\n  }\n  \n  moveToHead(node) {\n    this.removeNode(node);\n    this.addNode(node);\n  }\n  \n  popTail() {\n    const last = this.tail.prev;\n    this.removeNode(last);\n    return last;\n  }\n  \n  get(key) {\n    const node = this.cache.get(key);\n    if (node) {\n      this.moveToHead(node);\n      return node.val;\n    }\n    return -1;\n  }\n  \n  put(key, value) {\n    const node = this.cache.get(key);\n    if (node) {\n      node.val = value;\n      this.moveToHead(node);\n    } else {\n      const newNode = { key, val: value };\n      if (this.cache.size >= this.capacity) {\n        const tail = this.popTail();\n        this.cache.delete(tail.key);\n      }\n      this.cache.set(key, newNode);\n      this.addNode(newNode);\n    }\n  }\n}",
      "complexity": {
        "time": "O(1)",
        "space": "O(capacity)",
        "explanation_time": "All `get` and `put` operations involve a hash map lookup/insertion (O(1)) and a few pointer re-assignments in the doubly linked list (O(1)).",
        "explanation_space": "The hash map and the linked list both store up to the `capacity` number of items."
      },
      "annotations": [
        {
          "lines": [
            6,
            7
          ],
          "text": "Sentinel Nodes: Head and Tail dummies simplify add/remove logic (no null checks)."
        },
        {
          "lines": [
            11,
            12
          ],
          "text": "Add Node: Always add to front (right after head). Most recently used."
        },
        {
          "lines": [
            23
          ],
          "text": "Pop Tail: Remove from back (right before tail). Least recently used."
        },
        {
          "lines": [
            30
          ],
          "text": "Get: If found, move to head to refresh 'recently used' status."
        }
      ],
      "quizzes": [
        {
          "question": "What combination of data structures is optimal?",
          "options": [
            "Array + List",
            "HashMap + Doubly Linked List",
            "Stack + Queue",
            "Two Heaps"
          ],
          "correct": 1
        },
        {
          "question": "Why is a Doubly Linked List needed?",
          "options": [
            "To sort keys",
            "O(1) removal of any node given a pointer",
            "Less memory",
            "To handle collisions"
          ],
          "correct": 1
        },
        {
          "question": "Which item is evicted when full?",
          "options": [
            "Head (Most Recently Used)",
            "Tail (Least Recently Used)",
            "Random",
            "Middle"
          ],
          "correct": 1
        },
        {
          "question": "What is the time complexity of put/get?",
          "options": [
            "O(1)",
            "O(log N)",
            "O(N)",
            "O(N log N)"
          ],
          "correct": 0
        },
        {
          "question": "Why use dummy head/tail nodes?",
          "options": [
            "Required by language",
            "Avoid null checks/edge cases",
            "Store metadata",
            "Increase capacity"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "design-2",
      "title": "Design Twitter",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/design-twitter/",
      "follow_up": {
        "scenario": "A celebrity like Justin Bieber tweets. Pushing to 100M followers' feeds takes too long (Fan-out on Write).",
        "trade_off": "Push model gives fast reads but slow writes for celebrities. Pull model is safer for celebrities.",
        "strategy": "Hybrid Approach: Use Push for normal users and Pull for celebrities. When loading feed, merge pushed tweets with fetched celebrity tweets.",
        "answering_guide": "Start by contrasting Push vs. Pull models. The <strong>'Hybrid'</strong> approach is the gold standard answer. Explicitly mention the term <strong>'Fan-out on Write'</strong>."
      },
      "related": [
        {
          "id": "design-4",
          "title": "Design Hit Counter",
          "category": "design"
        },
        {
          "id": "design-1",
          "title": "LRU Cache",
          "category": "design"
        }
      ],
      "tags": [
        "design",
        "heap",
        "merge-k-lists"
      ],
      "problem_statement": "Design a simplified Twitter where users can post tweets, follow/unfollow others, and get a news feed of the 10 most recent tweets from their own posts and the posts of people they follow.\n\nImplement the `Twitter` class:\n*   `Twitter()` Initializes your twitter object.\n*   `void postTweet(int userId, int tweetId)` Composes a new tweet with ID `tweetId` by the user `userId`. Each call to this function will be made with a unique `tweetId`.\n*   `List<Integer> getNewsFeed(int userId)` Retrieves the `10` most recent tweet IDs in the user's news feed. Each item in the news feed must be posted by users who the user followed or by the user themself. Tweets must be ordered from most recent to least recent.\n*   `void follow(int followerId, int followeeId)` The user with ID `followerId` started following the user with ID `followeeId`.\n*   `void unfollow(int followerId, int followeeId)` The user with ID `followerId` started unfollowing the user with ID `followeeId`.\n\n**Example 1:**\n```\nInput\n[\"Twitter\", \"postTweet\", \"getNewsFeed\", \"follow\", \"postTweet\", \"getNewsFeed\", \"unfollow\", \"getNewsFeed\"]\n[[], [1, 5], [1], [1, 2], [2, 6], [1], [1, 2], [1]]\nOutput\n[null, null, [5], null, null, [6, 5], null, [5]]\n```",
      "explanation": {
        "understanding_the_problem": "We need to manage user relationships (who follows whom) and user content (tweets). The most complex operation is `getNewsFeed`, which requires fetching content from multiple users and finding the most recent items.",
        "brute_force": "1.  Store follow relationships in a `Map<userId, Set<userId>>`.\n2.  Store tweets in a `Map<userId, List<Tweet>>`, where each tweet has a global timestamp.\n3.  For `getNewsFeed`, gather all tweets from the user and everyone they follow into a single list, sort this large list by timestamp, and take the top 10. ",
        "bottleneck": "The `getNewsFeed` operation is very inefficient. If a user follows `k` people and they each have `T` tweets, sorting the combined list of `k*T` tweets is slow and not scalable.",
        "optimized_approach": "The `getNewsFeed` problem is a classic **'Merge k-Sorted Lists'** problem, since each user's tweet list is already sorted by time. A **Max-Heap** is the ideal data structure for this.\n\nInstead of combining all tweets and sorting, we can maintain a max-heap of size `k+1` (one for each followed user's most recent tweet, plus the current user's). We extract the most recent tweet (the max element) from the heap, add it to our result, and then add the next tweet from the same user to the heap. We repeat this 10 times.",
        "algorithm_steps": "1.  **Data Structures:**\n    - `follows: Map<userId, Set<userId>>`\n    - `tweets: Map<userId, List<[timestamp, tweetId]>>`\n    - `time: global integer` for timestamps.\n2.  **`postTweet(userId, tweetId)`:** Add `[time++, tweetId]` to the user's tweet list. O(1).\n3.  **`follow(follower, followee)`:** Add `followee` to `follower`'s set. O(1).\n4.  **`getNewsFeed(userId)` (Optimized):**\n    a.  Create a max-heap.\n    b.  For the user and each person they follow, add a pointer to their most recent tweet to the heap.\n    c.  Loop 10 times: extract the max (most recent) tweet from the heap, add it to the result list, and then add the next tweet from that same user back into the heap."
      },
      "code": "class Twitter {\n  constructor() {\n    this.tweets = new Map();\n    this.following = new Map();\n    this.timestamp = 0;\n  }\n  \n  postTweet(userId, tweetId) {\n    if (!this.tweets.has(userId)) this.tweets.set(userId, []);\n    this.tweets.get(userId).push([this.timestamp++, tweetId]);\n  }\n  \n  getNewsFeed(userId) {\n    const feeds = [];\n    const userTweets = this.tweets.get(userId) || [];\n    feeds.push(...userTweets);\n    \n    const followees = this.following.get(userId) || new Set();\n    for (const followeeId of followees) {\n      const followeeTweets = this.tweets.get(followeeId) || [];\n      feeds.push(...followeeTweets);\n    }\n    \n    feeds.sort((a, b) => b[0] - a[0]);\n    return feeds.slice(0, 10).map(tweet => tweet[1]);\n  }\n  \n  follow(followerId, followeeId) {\n    if (followerId === followeeId) return;\n    if (!this.following.has(followerId)) {\n      this.following.set(followerId, new Set());\n    }\n    this.following.get(followerId).add(followeeId);\n  }\n  \n  unfollow(followerId, followeeId) {\n    if (this.following.has(followerId)) {\n      this.following.get(followerId).delete(followeeId);\n    }\n  }\n}",
      "complexity": {
        "time": "O(T*k log(T*k)) for feed (naive), O(1) others",
        "space": "O(Tweets + Follows)",
        "explanation_time": "The provided code uses the naive sorting approach, where `k` is the number of followees and `T` is their average number of tweets. The complexity is dominated by sorting the collected feed. A heap-based approach would improve this significantly to O(k log k + 10 log k).",
        "explanation_space": "We need to store all tweets and all follow relationships."
      },
      "annotations": [
        {
          "lines": [
            3
          ],
          "text": "Data Store: Map<UserId, List<Tweet>> for O(1) access to a user's tweets."
        },
        {
          "lines": [
            16
          ],
          "text": "Aggregation: Collect tweets from user and everyone they follow."
        },
        {
          "lines": [
            21
          ],
          "text": "Sort: Naive approach sorts all gathered tweets. Heap merge would be better here."
        }
      ],
      "quizzes": [
        {
          "question": "What is the core algorithmic problem in getNewsFeed?",
          "options": [
            "Graph traversal",
            "Merge k Sorted Lists",
            "Dynamic Programming",
            "shortest path"
          ],
          "correct": 1
        },
        {
          "question": "Which data structure optimizes merging tweets?",
          "options": [
            "Stack",
            "Queue",
            "Min-Heap",
            "Max-Heap"
          ],
          "correct": 3
        },
        {
          "question": "Why is 'Pull Model' (fetch on read) used here?",
          "options": [
            "Simpler for small scale",
            "Faster reads",
            "Uses more storage",
            "Harder to implement"
          ],
          "correct": 0
        },
        {
          "question": "How do we handle a user following thousands of people?",
          "options": [
            "Pull model fails (latency)",
            "It just works",
            "Use larger heap",
            "Cache everything"
          ],
          "correct": 0
        },
        {
          "question": "What is the time complexity of getNewsFeed (optimized)?",
          "options": [
            "O(T * k)",
            "O(1)",
            "O(k log k)",
            "O(N²)"
          ],
          "correct": 2
        }
      ]
    },
    {
      "id": "design-3",
      "title": "Design Search Autocomplete System",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/design-search-autocomplete-system/",
      "follow_up": {
        "scenario": "The Trie is too large (100GB) to fit in RAM of a single machine.",
        "trade_off": "Storing on disk adds latency. Sharding by prefix adds complexity.",
        "strategy": "Shard by prefix (e.g., A-M on Server 1). Store only top-k hot queries in RAM; keep full tail on disk (SSD) or use a distributed KV store.",
        "answering_guide": "Focus on data partitioning. <strong>'Sharding by prefix'</strong> is the key phrase. Also mention <strong>'Hot vs. Cold data separation'</strong> to show optimization awareness."
      },
      "related": [
        {
          "id": "design-5",
          "title": "Design Add and Search Words",
          "category": "design"
        },
        {
          "id": "design-2",
          "title": "Design Twitter",
          "category": "design"
        }
      ],
      "tags": [
        "design",
        "trie",
        "heap"
      ],
      "problem_statement": "Design a search autocomplete system for a search engine. Users may input a sentence (at least one word and end with a special character `'#'`).\n\nFor each character they type except `'#'`, you return the **top 3 historical hot sentences** that have prefix the same as the part of sentence already typed. Here are the specific rules:\n\n1.  The hot degree for a sentence is defined as the number of times a user typed the exactly same sentence before.\n2.  The returned top 3 hot sentences should be sorted by hot degree (The first is the hottest one). If several sentences have the same degree of hotness, you need to use ASCII-code order (smaller one appears first).\n3.  If less than 3 hot sentences exist, then just return as many as you can.\n4.  When the input is a special character, it means the sentence ends, and in this case, you need to return an empty list.\n\n**Example 1:**\n```\nInput\n[\"AutocompleteSystem\", \"input\", \"input\", \"input\", \"input\"]\n[[[\"i love you\", \"island\", \"ironman\", \"i love leetcode\"], [5, 3, 2, 2]], [\"i\"], [\" \"], [\"a\"], [\"#\"]]\nOutput\n[null, [\"i love you\", \"island\", \"i love leetcode\"], [\"i love you\", \"i love leetcode\"], [], []]\n```",
      "explanation": {
        "understanding_the_problem": "We need to perform prefix-based searches and return a ranked list of results. This is a classic application for a **Trie (Prefix Tree)**.",
        "brute_force": "A naive approach would be to iterate through all known sentences every time the user types a character, check if each sentence starts with the current prefix, and then sort all matches to find the top 3. This would be extremely slow.",
        "bottleneck": "Scanning the entire list of sentences for every single keystroke is the clear bottleneck.",
        "optimized_approach": "A **Trie** is the perfect data structure. Each node represents a character, and a path from the root to a node represents a prefix. The key is what we store in the nodes.\n\nEach node in our Trie will store a map or list of all complete sentences that pass through it, along with their hotness scores. When a user types a prefix, we traverse the Trie to the node corresponding to that prefix. We then only need to sort the sentences stored at that specific node, which is a much smaller set than all sentences.",
        "algorithm_steps": "1.  **Trie Node Structure:** Each node contains `children` pointers and a `sentences` map (`sentence -> hotness`).\n2.  **Constructor:** Build the initial Trie from the given sentences. For each sentence, traverse the Trie, creating nodes as needed. At each node along the path, add the sentence and its hotness score to that node's `sentences` map.\n3.  **`input(c)`:**\n    a.  If `c == '#'`, the user finished typing. Add the current sentence to our data (and update its hotness in all relevant Trie nodes). Reset the state.\n    b.  Otherwise, append `c` to the current prefix and traverse to the next node in the Trie.\n    c.  Retrieve the `sentences` map from the current node.\n    d.  Sort the items in the map based on hotness (desc) and then lexicographically (asc).\n    e.  Return the top 3."
      },
      "code": "class AutocompleteSystem {\n  constructor(sentences, times) {\n    this.root = {};\n    this.currentNode = this.root;\n    this.currentSentence = '';\n    \n    for (let i = 0; i < sentences.length; i++) {\n      this.addSentence(sentences[i], times[i]);\n    }\n  }\n  \n  addSentence(sentence, count) {\n    let node = this.root;\n    for (const char of sentence) {\n      if (!node[char]) node[char] = {};\n      node = node[char];\n      if (!node.sentences) node.sentences = new Map();\n      node.sentences.set(sentence, (node.sentences.get(sentence) || 0) + count);\n    }\n  }\n  \n  input(c) {\n    if (c === '#') {\n      this.addSentence(this.currentSentence, 1);\n      this.currentSentence = '';\n      this.currentNode = this.root;\n      return [];\n    }\n    \n    this.currentSentence += c;\n    if (!this.currentNode || !this.currentNode[c]) {\n      this.currentNode = null;\n      return [];\n    }\n    \n    this.currentNode = this.currentNode[c];\n    const candidates = Array.from(this.currentNode.sentences.entries());\n    candidates.sort((a, b) => {\n      if (a[1] !== b[1]) return b[1] - a[1];\n      return a[0].localeCompare(b[0]);\n    });\n    \n    return candidates.slice(0, 3).map(item => item[0]);\n  }\n}",
      "complexity": {
        "time": "O(S*L + P + C*logC)",
        "space": "O(S*L)",
        "explanation_time": "Let S be the number of sentences, L be their average length, P be the prefix length, and C be the number of candidates for a prefix. Build time is O(S*L). Each input takes O(P) to traverse the trie, then O(C log C) to sort the candidates. ",
        "explanation_space": "The space is dominated by the Trie, which in the worst case stores every character of every sentence, leading to O(S*L) space."
      },
      "annotations": [
        {
          "lines": [
            2,
            10
          ],
          "text": "Trie Structure: Root node stores children chars and a map of full sentences passing through it."
        },
        {
          "lines": [
            13
          ],
          "text": "Hotness Map: Each node stores {sentence: frequency} for all words in its subtree."
        },
        {
          "lines": [
            32
          ],
          "text": "Traversal: Follow path down the Trie based on input characters."
        },
        {
          "lines": [
            40
          ],
          "text": "Ranking: Sort candidates at the constant-time node lookup level."
        }
      ],
      "quizzes": [
        {
          "question": "What is the primary data structure used?",
          "options": [
            "Hash Table",
            "Trie (Prefix Tree)",
            "Binary Search Tree",
            "Linked List"
          ],
          "correct": 1
        },
        {
          "question": "Where do we store the 'hot' sentences?",
          "options": [
            "Only at leaves",
            "In every node of the path",
            "In a separate database",
            "In the root only"
          ],
          "correct": 1
        },
        {
          "question": "How do we handle the '#' character?",
          "options": [
            "Ignore it",
            "It marks the end of a sentence (commit)",
            "It's a wildcard",
            "It deletes the last char"
          ],
          "correct": 1
        },
        {
          "question": "What optimizes the 'top 3' query?",
          "options": [
            "Sorting all sentences every time",
            "Storing a small sorted list at each node",
            "Using a global heap",
            "Binary search"
          ],
          "correct": 1
        },
        {
          "question": "What happens if two sentences have the same hotness?",
          "options": [
            "Random order",
            "Lexicographical (ASCII) order",
            "Newest first",
            "Oldest first"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "design-4",
      "title": "Design Hit Counter",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/design-hit-counter/",
      "follow_up": {
        "scenario": "You have multiple application servers. Hits are coming to all of them.",
        "trade_off": "Local counters are fast but don't show global state. Central Redis is a bottleneck.",
        "strategy": "Aggregate locally for 1 second, then push sum to a central Redis/Service. Precision is traded for scalability (eventual consistency).",
        "answering_guide": "Highlight the trade-off between Consistency and Availability. <strong>'Local Aggregation'</strong> (buffering) is the winning strategy here."
      },
      "related": [
        {
          "id": "design-2",
          "title": "Design Twitter",
          "category": "design"
        },
        {
          "id": "design-1",
          "title": "LRU Cache",
          "category": "design"
        }
      ],
      "tags": [
        "design",
        "sliding-window",
        "circular-buffer"
      ],
      "problem_statement": "Design a hit counter that counts the number of hits received in the past `5` minutes (i.e., the past `300` seconds).\n\nYour system should accept a `timestamp` parameter (in seconds granularity), and you may assume that calls are being made to the system in chronological order (i.e., `timestamp` is monotonically increasing). Several hits may arrive roughly at the same time.\n\nImplement the `HitCounter` class:\n*   `HitCounter()` Initializes the object of the hit counter system.\n*   `void hit(int timestamp)` Records a hit that happened at `timestamp`.\n*   `int getHits(int timestamp)` Returns the number of hits in the past 5 minutes from `timestamp` (i.e., the past `300` seconds).\n\n**Example 1:**\n```\nInput\n[\"HitCounter\", \"hit\", \"hit\", \"hit\", \"getHits\", \"hit\", \"getHits\", \"getHits\"]\n[[], [1], [2], [3], [4], [300], [300], [301]]\nOutput\n[null, null, null, null, 3, null, 4, 3]\n```",
      "explanation": {
        "understanding_the_problem": "We need to count events within a rolling time window. The key constraints are the fixed size of the window (300 seconds) and the potential for a very high number of hits, making it infeasible to store every single hit.",
        "brute_force": "A simple but unscalable solution is to store a list of timestamps for every hit. When `getHits` is called, we would iterate through the list, remove timestamps older than 300 seconds, and return the list's size. This would be slow and consume unbounded memory.",
        "bottleneck": "Storing every hit timestamp leads to unbounded memory usage and slow `getHits` calls if the hit rate is high.",
        "optimized_approach": "The **Bucketing** or **Circular Array** approach is ideal. Since the time window is fixed, we can use an array of 300 'buckets', where each bucket counts the hits for a specific second. We use the timestamp modulo 300 to map a time to a bucket index.\n\nTo handle the fact that bucket `i` could represent second `i`, `i+300`, `i+600`, etc., we need to store the timestamp along with the count. When we access a bucket, we check if its stored timestamp matches the current timestamp's second. If not, it's an old bucket that we can reset.",
        "algorithm_steps": "1.  **Data Structures:**\n    - `hits[300]`: An array to store the hit counts.\n    - `timestamps[300]`: A parallel array to store the timestamp corresponding to each bucket.\n\n2.  **`hit(timestamp)`:**\n    a.  Calculate the bucket index: `idx = timestamp % 300`.\n    b.  If `timestamps[idx]` matches the current `timestamp`, just increment `hits[idx]`.\n    c.  If not, it's a stale bucket. Reset it: `timestamps[idx] = timestamp` and `hits[idx] = 1`.\n\n3.  **`getHits(timestamp)`:**\n    a.  Initialize `total = 0`.\n    b.  Iterate through all 300 buckets.\n    c.  For each bucket `i`, if `timestamp - timestamps[i] < 300`, add `hits[i]` to `total`.\n    d.  Return `total`."
      },
      "code": "class HitCounter {\n  constructor() {\n    this.times = new Array(300).fill(0);\n    this.hits = new Array(300).fill(0);\n  }\n  \n  hit(timestamp) {\n    const idx = timestamp % 300;\n    if (this.times[idx] !== timestamp) {\n      this.times[idx] = timestamp;\n      this.hits[idx] = 1;\n    } else {\n      this.hits[idx]++;\n    }\n  }\n  \n  getHits(timestamp) {\n    let totalHits = 0;\n    for (let i = 0; i < 300; i++) {\n      if (timestamp - this.times[i] < 300) {\n        totalHits += this.hits[i];\n      }\n    }\n    return totalHits;\n  }\n}",
      "complexity": {
        "time": "O(1)",
        "space": "O(1)",
        "explanation_time": "`hit` is O(1). `getHits` requires iterating over a fixed-size array of 300 elements, which is a constant amount of work, making it O(1) as well.",
        "explanation_space": "We use two arrays of a fixed size (300), which is constant space, O(1)."
      },
      "annotations": [
        {
          "lines": [
            2,
            3
          ],
          "text": "Circular Buffer: Fixed size 300 for 5-minute window. Saves space vs storing all timestamps."
        },
        {
          "lines": [
            7,
            8
          ],
          "text": "Index Mapping: Modulo 300 maps indefinite time to fixed buckets."
        },
        {
          "lines": [
            9
          ],
          "text": "Reset: If bucket's timestamp is old, it belongs to a previous cycle. Overwrite it."
        },
        {
          "lines": [
            18
          ],
          "text": "Sum: Aggregate hits only from buckets falling within the last 300s window."
        }
      ],
      "quizzes": [
        {
          "question": "What is the optimal data structure for this?",
          "options": [
            "LinkedList",
            "Circular Buffer (Buckets)",
            "Large Array",
            "BST"
          ],
          "correct": 1
        },
        {
          "question": "Why not store every timestamp?",
          "options": [
            "Too much memory/slow query",
            "Not precise",
            "Hard to implementing",
            "Integer overflow"
          ],
          "correct": 0
        },
        {
          "question": "What is the size of the array needed?",
          "options": [
            "Unlimited",
            "60",
            "300",
            "86400"
          ],
          "correct": 2
        },
        {
          "question": "How do we handle concurrent hits?",
          "options": [
            "Locks/Atomic increment",
            "Ignore race conditions",
            "Use separate arrays",
            "Delay writes"
          ],
          "correct": 0
        },
        {
          "question": "How do we handle stale buckets?",
          "options": [
            "Delete them",
            "Reset value when index is reused",
            "Shift entire array",
            "Use garbage collection"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "design-5",
      "title": "Design Add and Search Words Data Structure",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/design-add-and-search-words-data-structure/",
      "follow_up": {
        "scenario": "User searches for '.......' (all dots). This triggers a DFS of the entire Trie.",
        "trade_off": "Strict regex support vs Denial of Service risk.",
        "strategy": "Limit the number of dots allowed. Optimize DFS by pruning branches that cannot match the remaining length. Store string lengths in nodes.",
        "answering_guide": "Frame this as a Security/Reliability issue (DoS). Suggest <strong>'Input Validation'</strong> (limiting dots) and <strong>'Pruning'</strong> as the technical mitigation."
      },
      "related": [
        {
          "id": "design-3",
          "title": "Design Search Autocomplete System",
          "category": "design"
        },
        {
          "id": "strings-1",
          "title": "KMP Pattern Matching",
          "category": "strings"
        }
      ],
      "tags": [
        "design",
        "trie",
        "dfs",
        "wildcard"
      ],
      "problem_statement": "Design a data structure that supports adding new words and finding if a string matches any previously added string.\n\nImplement the `WordDictionary` class:\n*   `WordDictionary()` Initializes the object.\n*   `void addWord(word)` Adds `word` to the data structure, it can be matched later.\n*   `bool search(word)` Returns `true` if there is any string in the data structure that matches `word` or `false` otherwise. `word` may contain dots `.` where `.` can be matched with any letter.\n\n**Example 1:**\n```\nInput\n[\"WordDictionary\",\"addWord\",\"addWord\",\"addWord\",\"search\",\"search\",\"search\",\"search\"]\n[[],[\"bad\"],[\"dad\"],[\"mad\"],[\"pad\"],[\"bad\"],[\".ad\"],[\"b..\"]]\nOutput\n[null,null,null,null,false,true,true,true]\n```",
      "explanation": {
        "understanding_the_problem": "We need to store a dictionary of words and perform searches that can include wildcards. This is another problem that is perfectly suited for a **Trie (Prefix Tree)**.",
        "brute_force": "Storing words in a simple list or hash set would make `addWord` easy. However, searching for a word with a wildcard like 'b.d' would require iterating through every word in the dictionary and checking it against the pattern, which would be very slow.",
        "bottleneck": "The search operation is the bottleneck for simple data structures when wildcards are involved.",
        "optimized_approach": "A **Trie** allows us to search prefix-by-prefix. When we encounter a normal character, we traverse to the corresponding child node. When we encounter a `.` wildcard, it means we can match any character. Therefore, we must explore *all* children of the current node and see if any of them lead to a valid match.",
        "algorithm_steps": "1.  **Trie Node Structure:** Each node has `children` (a map or array) and a boolean `isEndOfWord`.\n\n2.  **`addWord(word)`:** This is a standard Trie insertion. Traverse the Trie character by character, creating nodes as needed. At the final node, set `isEndOfWord = true`.\n\n3.  **`search(word)`:** This is a recursive DFS-style search.\n    a.  Define `dfs(index, node)` which searches for `word` starting from `index` in the subtree rooted at `node`.\n    b.  **Base Case:** If `index` reaches the end of the word, a match is found if `node.isEndOfWord` is true.\n    c.  Get the current character `c = word[index]`.\n    d.  **If `c` is a `.`:** Iterate through all children of the current `node`. For each `child`, if a recursive call `dfs(index + 1, child)` returns true, then we have found a match, so return true.\n    e.  **If `c` is a normal character:** Check if `node.children` has `c`. If not, no match. If yes, recurse on that child: `dfs(index + 1, node.children[c])`."
      },
      "code": "class WordDictionary {\n  constructor() {\n    this.root = {};\n  }\n  \n  addWord(word) {\n    let node = this.root;\n    for (const char of word) {\n      if (!node[char]) node[char] = {};\n      node = node[char];\n    }\n    node.isEnd = true;\n  }\n  \n  search(word) {\n    return this.dfs(word, 0, this.root);\n  }\n  \n  dfs(word, index, node) {\n    if (index === word.length) {\n      return !!node.isEnd;\n    }\n    \n    const char = word[index];\n    if (char === '.') {\n      for (const key in node) {\n        if (key !== 'isEnd' && this.dfs(word, index + 1, node[key])) {\n          return true;\n        }\n      }\n      return false;\n    } else {\n      if (!node[char]) return false;\n      return this.dfs(word, index + 1, node[char]);\n    }\n  }\n}",
      "complexity": {
        "time": "O(M) for add, O(N*26^M) worst-case for search",
        "space": "O(Total Chars)",
        "explanation_time": "`addWord` takes O(M) where M is the word length. `search` also takes O(M) for words without wildcards. With wildcards, the search can branch. In the worst case (a search for '....'), we might have to explore a large portion of the Trie.",
        "explanation_space": "The space is proportional to the total number of characters in all words stored in the Trie."
      },
      "annotations": [
        {
          "lines": [
            7
          ],
          "text": "Add: Standard Trie insertion. Create path if missing."
        },
        {
          "lines": [
            22
          ],
          "text": "Wildcard: ' . ' matches any char. Must try ALL children of current node recursively."
        },
        {
          "lines": [
            29
          ],
          "text": "Standard Match: Proceed down specific child path."
        }
      ],
      "quizzes": [
        {
          "question": "Which data structure is best for prefix-based storage?",
          "options": [
            "Hash Set",
            "Trie",
            "Array",
            "Linked List"
          ],
          "correct": 1
        },
        {
          "question": "How do we handle the '.' wildcard?",
          "options": [
            "Ignore it",
            "Check all children of current node",
            "Guess the character",
            "Jump to leaf"
          ],
          "correct": 1
        },
        {
          "question": "What is the worst-case search complexity with wildcards?",
          "options": [
            "O(M)",
            "O(26^M)",
            "O(1)",
            "O(log N)"
          ],
          "correct": 1
        },
        {
          "question": "What flag is crucial in each Trie node?",
          "options": [
            "isVisited",
            "isEndOfWord",
            "charValue",
            "hasChildren"
          ],
          "correct": 1
        },
        {
          "question": "Does 'addWord' handle wildcards?",
          "options": [
            "Yes",
            "No, only search does",
            "Ideally yes",
            "Depends on implementation"
          ],
          "correct": 1
        }
      ]
    }
  ],
  "optimization": [
    {
      "id": "optimization-1",
      "title": "Sliding Window Maximum",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/sliding-window-maximum/",
      "follow_up": {
        "scenario": "The input stream is infinite and high-speed. K is 1 Million.",
        "trade_off": "Storing 1M indices in a standard List causes GC pressure and cache misses.",
        "strategy": "Use a fixed-size circular buffer for the Deque. Optimize memory layout (Int32Array) to prevent pointer chasing.",
        "answering_guide": "Move beyond the algorithm to <strong>'Memory Locality'</strong> and <strong>'GC Pressure'</strong>. Mentioning <strong>'Typed Arrays'</strong> or <strong>'Ring Buffers'</strong> demonstrates systems knowledge."
      },
      "related": [
        {
          "id": "optimization-2",
          "title": "Largest Rectangle in Histogram",
          "category": "optimization"
        },
        {
          "id": "design-4",
          "title": "Design Hit Counter",
          "category": "design"
        }
      ],
      "tags": [
        "sliding-window",
        "deque",
        "monotonic"
      ],
      "problem_statement": "You are given an array of integers `nums`, there is a sliding window of size `k` which is moving from the very left of the array to the very right. You can only see the `k` numbers in the window. Each time the sliding window moves right by one position.\n\nReturn *the max sliding window*.\n\n**Example 1:**\n```\nInput: nums = [1,3,-1,-3,5,3,6,7], k = 3\nOutput: [3,3,5,5,6,7]\nExplanation: \nWindow position                Max\n---------------               -----\n[1  3  -1] -3  5  3  6  7       3\n 1 [3  -1  -3] 5  3  6  7       3\n 1  3 [-1  -3  5] 3  6  7       5\n 1  3  -1 [-3  5  3] 6  7       5\n 1  3  -1  -3 [5  3  6] 7       6\n 1  3  -1  -3  5 [3  6  7]      7\n```",
      "explanation": {
        "understanding_the_problem": "We need to find the maximum element within a fixed-size window that slides across an array. A new maximum is produced for each position the window slides to.",
        "brute_force": "For each of the `N-k+1` possible windows, we could iterate through the `k` elements within it to find the maximum. This would have a time complexity of O(N*k), which is too slow if N and k are large.",
        "bottleneck": "Re-scanning the `k` elements for every single window is highly redundant. The maximum of the current window is very likely to be the maximum of the next window, but we aren't reusing that information.",
        "optimized_approach": "An optimal O(N) solution uses a **monotonic deque** (a double-ended queue). This deque will store *indices* of elements from the array, and it will be kept in strictly decreasing order based on the values at those indices. \n\nThe front of the deque will always hold the index of the maximum element in the current window. As we slide the window, we add new elements to the back of the deque, but before adding, we remove any elements from the back that are smaller than the new element, thus maintaining the monotonic property.",
        "algorithm_steps": "1.  Initialize an empty deque and a results array.\n2.  Iterate through `nums` from left to right with index `i`:\n    a.  **Clean Front:** Remove any indices from the *front* of the deque that are no longer in the current window (i.e., index `<= i - k`).\n    b.  **Clean Back:** While the deque is not empty and the value at the *back* of the deque is less than `nums[i]`, pop from the back. This is the key step to maintain the decreasing monotonic property.\n    c.  **Add Index:** Push the current index `i` to the back.\n    d.  **Store Result:** If our window is full (`i >= k - 1`), the maximum for this window is the value at the index at the *front* of the deque. Add this to the results."
      },
      "code": "function maxSlidingWindow(nums, k) {\n  const deque = [];\n  const result = [];\n  \n  for (let i = 0; i < nums.length; i++) {\n    while (deque.length && deque[0] <= i - k) {\n      deque.shift();\n    }\n    \n    while (deque.length && nums[deque[deque.length - 1]] <= nums[i]) {\n      deque.pop();\n    }\n    \n    deque.push(i);\n    \n    if (i >= k - 1) {\n      result.push(nums[deque[0]]);\n    }\n  }\n  \n  return result;\n}",
      "complexity": {
        "time": "O(N)",
        "space": "O(k)",
        "explanation_time": "Each index from the input array is pushed onto and popped from the deque at most once. This gives an amortized O(1) cost for each of the N elements, leading to a total linear time complexity.",
        "explanation_space": "The deque will store at most `k` indices, so the space complexity is O(k)."
      },
      "annotations": [
        {
          "lines": [
            2
          ],
          "text": "Deque stores indices, not values. It maintains a decreasing order of values."
        },
        {
          "lines": [
            6
          ],
          "text": "Clean Front: Remove indices that have fallen out of the current window (i - k)."
        },
        {
          "lines": [
            10
          ],
          "text": "Clean Back: Remove indices whose values are smaller than current num. They can't be max anymore."
        },
        {
          "lines": [
            16
          ],
          "text": "Result: The front of the deque always holds the index of the max value for the current window."
        }
      ],
      "quizzes": [
        {
          "question": "What property does the deque maintain?",
          "options": [
            "Increasing values",
            "Decreasing values",
            "Sorted indices",
            "Random order"
          ],
          "correct": 1
        },
        {
          "question": "Why do we store indices instead of values?",
          "options": [
            "To save memory",
            "To check window bounds (expiry)",
            "Faster comparison",
            "Required by array"
          ],
          "correct": 1
        },
        {
          "question": "When do we pop from the back?",
          "options": [
            "When window slides",
            "When new element is larger than back",
            "When deque is full",
            "Never"
          ],
          "correct": 1
        },
        {
          "question": "When do we pop from the front?",
          "options": [
            "When element is too small",
            "When element is out of window range",
            "When finding max",
            "Randomly"
          ],
          "correct": 1
        },
        {
          "question": "What is the time complexity?",
          "options": [
            "O(N*k)",
            "O(N log k)",
            "O(N)",
            "O(N²)",
            "O(N)"
          ],
          "correct": 0
        }
      ]
    },
    {
      "id": "optimization-2",
      "title": "Largest Rectangle in Histogram",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/largest-rectangle-in-histogram/",
      "follow_up": {
        "scenario": "The histogram is distributed across 1000 machines (MapReduce).",
        "trade_off": "Cannot just sum the local results. The largest rectangle might span across machine boundaries.",
        "strategy": "Each machine computes local max area AND a 'skyline' profile (left/right increasing subsequences). The reducer merges these 1000 compressed profiles.",
        "answering_guide": "This is a classic <strong>'Divide and Conquer'</strong> at scale. Explain that the result is NOT just the max of local results; <strong>boundary merging</strong> is the key."
      },
      "related": [
        {
          "id": "optimization-3",
          "title": "Maximal Rectangle",
          "category": "optimization"
        },
        {
          "id": "optimization-5",
          "title": "Trapping Rain Water",
          "category": "optimization"
        }
      ],
      "tags": [
        "stack",
        "monotonic",
        "optimization"
      ],
      "problem_statement": "Given an array of integers `heights` representing the histogram's bar height where the width of each bar is `1`, return *the area of the largest rectangle in the histogram*.\n\n**Example 1:**\n```\nInput: heights = [2,1,5,6,2,3]\nOutput: 10\nExplanation: The largest rectangle has an area = 10 units.\n```",
      "explanation": {
        "understanding_the_problem": "We need to find the largest rectangular area that can be inscribed within the given histogram bars.",
        "brute_force": "For each bar `i`, we can try to form the largest rectangle that has this bar as its height. To do this, we would expand left and right from `i` to find the boundaries (the first bars shorter than `heights[i]`). This would take O(N) for each bar, leading to an overall O(N²) time complexity.",
        "bottleneck": "The O(N²) approach is slow because for each bar, we are re-scanning its left and right sides to find the boundaries.",
        "optimized_approach": "An O(N) solution uses a **monotonic stack**. The stack will store indices of bars in increasing order of their height. For any bar, its contribution to the max area is `height * width`. The width is determined by the first shorter bar to its left and the first shorter bar to its right. The monotonic stack helps us find these boundaries efficiently for all bars in a single pass.\n\nWhen we encounter a bar that is shorter than the one at the top of the stack, we know we've found the 'right boundary' for the bar on the stack. Its 'left boundary' is the next element in the stack.",
        "algorithm_steps": "1.  Initialize an empty stack (to store indices) and `max_area = 0`.\n2.  Iterate through the `heights` array (it's helpful to append a 0 to the end to ensure all bars are processed).\n3.  For each bar `i`, while the stack is not empty and `heights[i]` is less than the height of the bar at the stack's top:\n    a.  Pop the index `h_idx` from the stack. This is the bar whose max area we are calculating.\n    b.  The `height` is `heights[h_idx]`.\n    c.  The right boundary is `i`. The left boundary is the next element in the stack.\n    d.  Calculate `width = i - (stack.length > 0 ? stack[stack.length-1] : -1) - 1`.\n    e.  Update `max_area = max(max_area, height * width)`.\n4.  Push the current index `i` onto the stack."
      },
      "code": "function largestRectangleArea(heights) {\n  const stack = [];\n  let maxArea = 0;\n  heights.push(0);\n  \n  for (let i = 0; i < heights.length; i++) {\n    while (stack.length && heights[i] < heights[stack[stack.length - 1]]) {\n      const h = heights[stack.pop()];\n      const w = stack.length ? i - stack[stack.length - 1] - 1 : i;\n      maxArea = Math.max(maxArea, h * w);\n    }\n    stack.push(i);\n  }\n  \n  return maxArea;\n}",
      "complexity": {
        "time": "O(N)",
        "space": "O(N)",
        "explanation_time": "Each bar's index is pushed onto and popped from the stack exactly once, leading to a linear time complexity.",
        "explanation_space": "In the worst case (a histogram with increasing heights), the stack could hold all N indices."
      },
      "annotations": [
        {
          "lines": [
            4
          ],
          "text": "Sentinel: Add 0 height at the end to force processing remaining bars in stack."
        },
        {
          "lines": [
            7
          ],
          "text": "Condition: Current bar is shorter than stack top? Stack top is the 'height' of the rect."
        },
        {
          "lines": [
            9
          ],
          "text": "Width Calc: i is right boundary (exclusive), new stack top is left boundary (exclusive)."
        },
        {
          "lines": [
            10
          ],
          "text": "Area: height * width. Update maxArea if this rectangle is bigger."
        }
      ],
      "quizzes": [
        {
          "question": "What does the stack store?",
          "options": [
            "Heights",
            "Indices",
            "Areas",
            "Reversed Heights"
          ],
          "correct": 1
        },
        {
          "question": "What property of heights does the stack maintain?",
          "options": [
            "Decreasing",
            "Increasing",
            "Random",
            "Constant"
          ],
          "correct": 1
        },
        {
          "question": "What triggers a calculation?",
          "options": [
            "Pushing a taller bar",
            "Pushing a shorter bar",
            "Stack empty",
            "Every step"
          ],
          "correct": 1
        },
        {
          "question": "How is width calculated?",
          "options": [
            "right - left",
            "right - left - 1",
            "right + left",
            "constant"
          ],
          "correct": 1
        },
        {
          "question": "Why append 0 at the end?",
          "options": [
            "To avoid overflow",
            "To flush the stack",
            "To start index at 1",
            "Padding"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "optimization-3",
      "title": "Maximal Rectangle",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/maximal-rectangle/",
      "follow_up": {
        "scenario": "The matrix is extremely sparse (99.9% zeros).",
        "trade_off": "The O(N*M) solution processes mainly empty space, wasting cycles.",
        "strategy": "Use Sparse Matrix representation (e.g., List of coordinates for 1s). Iterate only through 1s and look for connected components or use 2D Segment Trees.",
        "answering_guide": "Question the input format. If it's sparse, 'Dense Matrix' algorithms are wrong. pivot to <strong>'Sparse Representation'</strong> (Coordinate List)."
      },
      "related": [
        {
          "id": "optimization-2",
          "title": "Largest Rectangle in Histogram",
          "category": "optimization"
        },
        {
          "id": "dp-5",
          "title": "Maximal Square",
          "category": "dp"
        }
      ],
      "tags": [
        "dp",
        "stack",
        "histogram"
      ],
      "problem_statement": "Given a `rows x cols` binary `matrix` filled with `0`'s and `1`'s, find the largest rectangle containing only `1`'s and return *its area*.\n\n**Example 1:**\n```\nInput: matrix = [[\"1\",\"0\",\"1\",\"0\",\"0\"],[\"1\",\"0\",\"1\",\"1\",\"1\"],[\"1\",\"1\",\"1\",\"1\",\"1\"],[\"1\",\"0\",\"0\",\"1\",\"0\"]]\nOutput: 6\nExplanation: The largest rectangle is shown in the bolded 1s.\n```",
      "explanation": {
        "understanding_the_problem": "We need to find the largest all-1s rectangle within a grid.",
        "brute_force": "We could check every possible rectangle. A rectangle is defined by its top-left and bottom-right corners. This would be O(M² * N²) rectangles, and for each, we'd have to check if it's all 1s, leading to a very slow solution.",
        "bottleneck": "Checking every possible rectangle is too slow. We need a more structured approach.",
        "optimized_approach": "This problem can be cleverly reduced to the **'Largest Rectangle in Histogram'** problem.\n\nWe can process the matrix row by row. For each row, we can imagine it as the base of a histogram. The height of each bar at column `j` is the number of consecutive 1s directly above it in that column, including the cell in the current row.\n\nBy doing this, for each row, we generate a new histogram. We can then run the O(N) 'Largest Rectangle in Histogram' algorithm on this generated histogram. The overall maximum area found across all rows will be the answer.",
        "algorithm_steps": "1.  Initialize `maxArea = 0` and a `heights` array of size N (number of columns), filled with zeros.\n2.  Iterate through each `row` of the matrix from top to bottom:\n    a.  Update the `heights` array for the current row: For each column `j`, if `matrix[row][j] == '1'`, increment `heights[j]`. If it's '0', reset `heights[j] = 0`.\n    b.  After updating the `heights` array, it represents a new histogram.\n    c.  Run the 'Largest Rectangle in Histogram' algorithm on this `heights` array.\n    d.  Update `maxArea` with the result.\n3.  Return `maxArea`."
      },
      "code": "function maximalRectangle(matrix) {\n  if (!matrix.length) return 0;\n  const m = matrix.length, n = matrix[0].length;\n  const heights = new Array(n).fill(0);\n  let maxArea = 0;\n  \n  for (let i = 0; i < m; i++) {\n    for (let j = 0; j < n; j++) {\n      heights[j] = matrix[i][j] === '1' ? heights[j] + 1 : 0;\n    }\n    maxArea = Math.max(maxArea, largestRectangleArea([...heights]));\n  }\n  \n  return maxArea;\n}\n\nfunction largestRectangleArea(heights) {\n  const stack = [];\n  let maxArea = 0;\n  heights.push(0);\n  \n  for (let i = 0; i < heights.length; i++) {\n    while (stack.length && heights[i] < heights[stack[stack.length - 1]]) {\n      const h = heights[stack.pop()];\n      const w = stack.length ? i - stack[stack.length - 1] - 1 : i;\n      maxArea = Math.max(maxArea, h * w);\n    }\n    stack.push(i);\n  }\n  \n  return maxArea;\n}",
      "complexity": {
        "time": "O(M * N)",
        "space": "O(N)",
        "explanation_time": "We iterate through M rows, and for each row, we perform an O(N) operation (updating the heights array and running the histogram algorithm). This gives a total time complexity of O(M * N).",
        "explanation_space": "We need O(N) space to store the `heights` array for each row. The stack used in the histogram algorithm also takes O(N) space."
      },
      "annotations": [
        {
          "lines": [
            7
          ],
          "text": "Row Scan: Treat each row as the base of a histogram."
        },
        {
          "lines": [
            9
          ],
          "text": "Build Histogram: If '1', add to height. If '0', reset height to 0."
        },
        {
          "lines": [
            11
          ],
          "text": "Solve Subproblem: Run 'Largest Rectangle in Histogram' on current row's heights."
        }
      ],
      "quizzes": [
        {
          "question": "What easier problem does this reduce to?",
          "options": [
            "Maximum Subarray Sum",
            "Largest Rectangle in Histogram",
            "Longest Common Subsequence",
            "Knapsack Problem"
          ],
          "correct": 1
        },
        {
          "question": "How do we build the histogram for each row?",
          "options": [
            "Count 1s in row",
            "Accumulate 1s from top (reset on 0)",
            "Count 0s",
            "Randomly"
          ],
          "correct": 1
        },
        {
          "question": "What happens if matrix[i][j] is '0'?",
          "options": [
            "Height becomes 0",
            "Height stays same",
            "Height decreases by 1",
            "Height increases"
          ],
          "correct": 0
        },
        {
          "question": "What is the overall time complexity?",
          "options": [
            "O(M*N)",
            "O(M²*N)",
            "O(M*N²)",
            "O(M+N)"
          ],
          "correct": 0
        },
        {
          "question": "What is the space complexity?",
          "options": [
            "O(1)",
            "O(N) (width of matrix)",
            "O(M*N)",
            "O(M)"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "optimization-4",
      "title": "Minimum Window Substring",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/minimum-window-substring/",
      "follow_up": {
        "scenario": "Searching for DNA patterns (A, C, G, T) in a 100GB genome file.",
        "trade_off": "Hash Map overhead is too high for just 4 characters.",
        "strategy": "Use Integer arrays (size 128) instead of Maps. Even better: encode ACGT as 2 bits. Fits 32 chars in a single 64-bit integer, allowing bitwise sliding window ops.",
        "answering_guide": "Show off <strong>'Data Packing'</strong> skills. 2-bit encoding allows <strong>'Bitwise Parallelism'</strong>. This is a very senior optimization for string problems."
      },
      "related": [
        {
          "id": "optimization-1",
          "title": "Sliding Window Maximum",
          "category": "optimization"
        },
        {
          "id": "strings-2",
          "title": "Longest Substring Without Repeating Characters",
          "category": "strings"
        }
      ],
      "tags": [
        "sliding-window",
        "two-pointers",
        "hash-map"
      ],
      "problem_statement": "Given two strings `s` and `t` of lengths `m` and `n` respectively, return *the **minimum window substring** of `s` such that every character in `t` (including duplicates) is included in the window. If there is no such substring, return the empty string `\"\"`*.\n\nThe testcases will be generated such that the answer is **unique**.\n\n**Example 1:**\n```\nInput: s = \"ADOBECODEBANC\", t = \"ABC\"\nOutput: \"BANC\"\nExplanation: The minimum window substring \"BANC\" includes 'A', 'B', and 'C' from string t.\n```",
      "explanation": {
        "understanding_the_problem": "We need to find the shortest contiguous block in string `s` that covers all characters required by string `t`.",
        "brute_force": "We could generate every possible substring of `s`, and for each one, check if it contains all the characters of `t`. This would be very inefficient, around O(N³).",
        "bottleneck": "Checking every substring is the bottleneck. We need a way to efficiently check substrings without starting from scratch each time.",
        "optimized_approach": "This is a classic **Sliding Window** problem that can be solved with two pointers (`left` and `right`). We expand the window by moving `right` and shrink it by moving `left`.\n\nWe use two hash maps: one to store the frequency of characters needed by `t`, and another to store the character frequencies in our current window. We also use a counter to track how many of the required character counts we have fully satisfied.",
        "algorithm_steps": "1.  Create a `need` map to store the character frequencies of `t`.\n2.  Initialize `left = 0`, `right = 0`, and a `valid` counter to 0.\n3.  **Expand Window:** Loop `right` from 0 to the end of `s`.\n    a.  Add the character `s[right]` to our `window` map.\n    b.  If this character is in `need` and its count in `window` now matches its count in `need`, increment `valid`.\n4.  **Shrink Window:** Once `valid` equals the number of unique characters in `t`, our window is valid. Now we try to shrink it:\n    a.  While the window is valid, update our minimum length found so far.\n    b.  Move `left` to the right, removing `s[left]` from the window.\n    c.  If `s[left]` was a needed character and its count in `window` now falls below what's needed, decrement `valid`.\n5.  The inner shrinking loop stops when the window is no longer valid, and the outer loop continues expanding."
      },
      "code": "function minWindow(s, t) {\n  const need = new Map();\n  for (const char of t) {\n    need.set(char, (need.get(char) || 0) + 1);\n  }\n  \n  let left = 0, right = 0;\n  let valid = 0;\n  let start = 0, len = Infinity;\n  const window = new Map();\n  \n  while (right < s.length) {\n    const c = s[right];\n    right++;\n    \n    if (need.has(c)) {\n      window.set(c, (window.get(c) || 0) + 1);\n      if (window.get(c) === need.get(c)) {\n        valid++;\n      }\n    }\n    \n    while (valid === need.size) {\n      if (right - left < len) {\n        start = left;\n        len = right - left;\n      }\n      \n      const d = s[left];\n      left++;\n      \n      if (need.has(d)) {\n        if (window.get(d) === need.get(d)) {\n          valid--;\n        }\n        window.set(d, window.get(d) - 1);\n      }\n    }\n  }\n  \n  return len === Infinity ? '' : s.substr(start, len);\n}",
      "complexity": {
        "time": "O(|s| + |t|)",
        "space": "O(|s| + |t|)",
        "explanation_time": "The `left` and `right` pointers each traverse the string `s` only once. The initial scan of `t` takes O(|t|) time. This gives a total linear time complexity.",
        "explanation_space": "In the worst case, the `need` and `window` hash maps could store all unique characters from both strings."
      },
      "annotations": [
        {
          "lines": [
            2
          ],
          "text": "Map 'need': Count frequency of each char in target string t."
        },
        {
          "lines": [
            12
          ],
          "text": "Expand: Add char at 'right' to current window counts."
        },
        {
          "lines": [
            18
          ],
          "text": "Valid Check: If we have enough valid characters, try to shrink."
        },
        {
          "lines": [
            25
          ],
          "text": "Shrink: Remove char at 'left' to find smaller valid window."
        }
      ],
      "quizzes": [
        {
          "question": "What techniques are combined here?",
          "options": [
            "Sliding Window + Hash Map",
            "Dynamic Programming + Stack",
            "Greedy + Heap",
            "DFS + Backtracking"
          ],
          "correct": 0
        },
        {
          "question": "When do we try to shrink the window (move left)?",
          "options": [
            "When the window is valid (contains all chars)",
            "When we reach end of string",
            "At every step",
            "When window is empty"
          ],
          "correct": 0
        },
        {
          "question": "What does the 'need' map store?",
          "options": [
            "Indices of chars",
            "Frequency of chars in target T",
            "Visited chars",
            "Result substring"
          ],
          "correct": 1
        },
        {
          "question": "What is the time complexity?",
          "options": [
            "O(N^2)",
            "O(N log N)",
            "O(|S| + |T|)",
            "O(N^3)"
          ],
          "correct": 2
        },
        {
          "question": "What if the target T has duplicates (e.g., 'AA')?",
          "options": [
            "Ignore duplicates",
            "The window must also contain duplicates",
            "Duplicates are impossible",
            "Use a Set instead of Map"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "optimization-5",
      "title": "Trapping Rain Water",
      "difficulty": "Hard",
      "leetcode_url": "https://leetcode.com/problems/trapping-rain-water/",
      "follow_up": {
        "scenario": "Optimizing water calculation for a dynamic terrain (e.g., Minecraft, where blocks are added/removed).",
        "trade_off": "Recomputing O(N) for every block change is too slow (O(K*N)).",
        "strategy": "Use a Segment Tree. Each node stores max_height and water_volume. Updates take O(log N). Sub-problems merge by calculating water trapped between left_child_max and right_child_max.",
        "answering_guide": "Recognize that 'Dynamic Updates' = <strong>'Segment Tree'</strong> (or Fenwick). Explain that pre-computation allows O(log N) updates vs O(N) rebuilds."
      },
      "related": [
        {
          "id": "optimization-2",
          "title": "Largest Rectangle in Histogram",
          "category": "optimization"
        },
        {
          "id": "two-pointers-1",
          "title": "Container With Most Water",
          "category": "two-pointers"
        }
      ],
      "tags": [
        "two-pointers",
        "optimization",
        "greedy"
      ],
      "problem_statement": "Given `n` non-negative integers representing an elevation map where the width of each bar is `1`, compute how much water it can trap after raining.\n\n**Example 1:**\n```\nInput: height = [0,1,0,2,1,0,1,3,2,1,2,1]\nOutput: 6\nExplanation: The above elevation map (black section) is represented by array [0,1,0,2,1,0,1,3,2,1,2,1]. In this case, 6 units of rain water (blue section) are being trapped.\n```",
      "explanation": {
        "understanding_the_problem": "Imagine the given array as a 2D profile. After it rains, water will be trapped between taller bars. We need to calculate the total volume of this trapped water.",
        "brute_force": "For any given bar `i`, the amount of water it can hold above it is determined by the height of the tallest bar to its left (`max_left`) and the tallest bar to its right (`max_right`). The water level above bar `i` will be `min(max_left, max_right)`. The trapped water is `min(max_left, max_right) - height[i]`. We could iterate through each bar, and for each one, scan its left and right sides to find the max heights. This would be O(N²).",
        "bottleneck": "The O(N²) approach is slow because we repeatedly scan the left and right sides for every bar.",
        "optimized_approach": "**1. DP Approach (O(N) time, O(N) space):**\nWe can precompute the `max_left` and `max_right` for every position. First, iterate from left to right to create a `max_lefts` array. Then, iterate from right to left for a `max_rights` array. Finally, in a third pass, calculate the trapped water at each bar with the formula.\n\n**2. Two Pointers (O(N) time, O(1) space):**\nThis is the most optimal solution. We maintain two pointers, `left` and `right`, at the ends of the array, and two variables, `left_max` and `right_max`.\nThe key insight is that the amount of water trapped at any point is limited by the shorter of the two max walls (`left_max` and `right_max`). If `height[left]` is less than `height[right]`, we know the `left_max` is the limiting factor for the `left` pointer. We can safely calculate the trapped water at `left` because we know `right_max` is at least as high as `height[right]`, which is higher than `height[left]`.",
        "algorithm_steps": "1.  Initialize `left = 0`, `right = N-1`, `left_max = 0`, `right_max = 0`, and `water = 0`.\n2.  Loop while `left < right`:\n    a.  If `height[left] < height[right]`:\n        i.  If `height[left] >= left_max`, update `left_max` because this new bar is taller.\n        ii. Else, water is trapped. Add `left_max - height[left]` to `water`.\n        iii. Move `left++`.\n    b.  Else (if `height[right] <= height[left]`):\n        i.  Do the symmetric operation for the right side. Update `right_max` or add `right_max - height[right]` to `water`.\n        ii. Move `right--`."
      },
      "code": "function trap(height) {\n  if (height.length < 3) return 0;\n  \n  let left = 0, right = height.length - 1;\n  let leftMax = 0, rightMax = 0;\n  let water = 0;\n  \n  while (left < right) {\n    if (height[left] < height[right]) {\n      if (height[left] >= leftMax) {\n        leftMax = height[left];\n      } else {\n        water += leftMax - height[left];\n      }\n      left++;\n    } else {\n      if (height[right] >= rightMax) {\n        rightMax = height[right];\n      } else {\n        water += rightMax - height[right];\n      }\n      right--;\n    }\n  }\n  \n  return water;\n}",
      "complexity": {
        "time": "O(N)",
        "space": "O(1)",
        "explanation_time": "The two pointers, `left` and `right`, traverse the array once, meeting in the middle. This results in a single pass and linear time complexity.",
        "explanation_space": "We only use a few variables to keep track of pointers and maximums, giving us constant space complexity."
      },
      "annotations": [
        {
          "lines": [
            8
          ],
          "text": "Key Insight: The shorter wall determines water level. If left < right, we trust leftMax."
        },
        {
          "lines": [
            9
          ],
          "text": "Update Left Max: We found a taller wall, so no water trapped here."
        },
        {
          "lines": [
            11
          ],
          "text": "Trap Water: Current bar is shorter than leftMax, so it holds water."
        },
        {
          "lines": [
            15
          ],
          "text": "Symmetric: Same logic for the right side if right wall is shorter."
        }
      ],
      "quizzes": [
        {
          "question": "What determines height of water at index i?",
          "options": [
            "min(max_left, max_right) - height[i]",
            "max(max_left, max_right)",
            "Average height",
            "max_left + max_right"
          ],
          "correct": 0
        },
        {
          "question": "What is the space complexity of the Two-Pointer approach?",
          "options": [
            "O(N)",
            "O(1)",
            "O(log N)",
            "O(N^2)"
          ],
          "correct": 1
        },
        {
          "question": "Why can we trust 'leftMax' when height[left] < height[right]?",
          "options": [
            "We can't",
            "Because rightMax is guaranteed to be >= height[right] > height[left]",
            "It's a heuristic",
            "Magic"
          ],
          "correct": 1
        },
        {
          "question": "What happens if height[left] >= leftMax?",
          "options": [
            "Update leftMax",
            "Trap water",
            "Move right pointer",
            "Reset"
          ],
          "correct": 0
        },
        {
          "question": "How does this differ from 'Container With Most Water'?",
          "options": [
            "It doesn't",
            "Container cares about width * min_height, this sums volumes",
            "This is 3D",
            "Different inputs"
          ],
          "correct": 1
        }
      ]
    }
  ],
  "physics": [
    {
      "id": "physics-1",
      "title": "Relativistic Collision (Four-Momentum)",
      "difficulty": "Medium",
      "related": [
        {
          "id": "math-1",
          "title": "Modular Exponentiation",
          "category": "math"
        }
      ],
      "tags": [
        "relativity",
        "four-momentum",
        "invariant"
      ],
      "problem_statement": "A particle of rest mass $m_0$ with kinetic energy $T$ strikes a **stationary particle** of the same rest mass. Find the **rest mass** $M$ and the **velocity** $V$ of the compound particle formed as a result of the collision.\n\n**Given:**\n- Rest mass of each particle: $m_0$\n- Kinetic energy of striking particle: $T$\n- Stationary target particle\n\n**Find:**\n- Rest mass $M$ of compound particle\n- Velocity $V$ of compound particle",
      "explanation": {
        "understanding_the_problem": "This is a classic relativistic collision problem. Unlike Newtonian mechanics where kinetic energy can be converted to other forms, in relativity we must use the **four-momentum invariant** $s = E^2/c^2 - p^2$.\n\nThe key insight is that this invariant quantity is the same in all reference frames. We can calculate it in the lab frame (where one particle is stationary) and set it equal to the rest frame of the compound particle.",
        "brute_force": "The four-momentum for a single particle is $(E/c, \\vec{p})$. Its magnitude squared is:\n\n$$s = \\frac{E^2}{c^2} - p^2 = m_0^2 c^2$$\n\nThis is the **invariant mass** relation: $E^2 = (pc)^2 + (m_0 c^2)^2$\n\nFor a system of particles, the total four-momentum is conserved, and its invariant is related to the total rest mass $M$ of the system.",
        "bottleneck": "The challenge is correctly handling the relativistic relations. We cannot simply add kinetic energies or momenta like in classical mechanics.",
        "optimized_approach": "**Step 1: Initial State**\n\nStriking particle:\n- Total energy: $E_1 = T + m_0 c^2$\n- Momentum: From $E_1^2 = (p_1 c)^2 + (m_0 c^2)^2$, we get $p_1 = \\frac{1}{c}\\sqrt{E_1^2 - m_0^2 c^4}$\n\nStationary particle:\n- Total energy: $E_2 = m_0 c^2$\n- Momentum: $p_2 = 0$\n\n**Step 2: Total Four-Momentum Invariant**\n\n$$s = \\frac{(E_1 + E_2)^2}{c^2} - (p_1 + p_2)^2 = M^2 c^2$$\n\nSubstituting:\n$$M^2 c^4 = (T + 2m_0 c^2)^2 - (E_1^2 - m_0^2 c^4)$$\n\nSimplifying:\n$$M = 2m_0\\sqrt{1 + \\frac{T}{2m_0 c^2}}$$\n\n**Step 3: Velocity from Momentum Conservation**\n\nUsing $p_{total} = p_1 = \\gamma M V$:\n$$V = \\frac{p_1 c^2}{E_1 + E_2}$$",
        "algorithm_steps": "1. Calculate total energy of striking particle: $E_1 = T + m_0 c^2$\n2. Calculate momentum: $p_1 = \\frac{1}{c}\\sqrt{T(T + 2m_0 c^2)}$\n3. Apply invariant: $M^2 c^4 = (E_1 + m_0 c^2)^2 - (p_1 c)^2$\n4. Solve for $M = 2m_0\\sqrt{1 + \\frac{T}{2m_0 c^2}}$\n5. Find velocity: $V = \\frac{p_1 c^2}{E_1 + m_0 c^2}$",
        "result_analysis": "**Final Results:**\n\n$$M = 2m_0\\sqrt{1 + \\frac{T}{2m_0 c^2}}$$\n\n$$V = \\frac{c\\sqrt{T(T + 2m_0 c^2)}}{T + 2m_0 c^2}$$\n\n**Limiting Cases:**\n\n1. **Non-relativistic limit** ($T \\ll m_0 c^2$): $M \\approx 2m_0$ and $V \\approx \\sqrt{T/(2m_0)}$ — recovers classical result\n\n2. **Ultra-relativistic limit** ($T \\gg m_0 c^2$): $M \\approx \\sqrt{2m_0 T}/c$ and $V \\to c$\n\n**Physical Interpretation:**\n\nThe compound particle has **more rest mass** than the sum of the original rest masses ($M > 2m_0$ when $T > 0$). This 'extra' mass comes from the kinetic energy of the collision, which is converted to rest mass energy via $E = mc^2$.\n\n**Applications:**\n\n- **Particle Accelerators**: This is how new heavy particles are created at CERN. High-energy protons collide, and some kinetic energy converts to the mass of new particles (Higgs boson, etc.)\n- **Threshold Energy**: To create a particle of mass $M_{new}$, we need minimum kinetic energy $T_{min}$ — this formula helps calculate it\n- **Cosmic Ray Physics**: Understanding how high-energy cosmic rays create particle showers in the atmosphere"
      },
      "quizzes": [
        {
          "question": "What quantity is invariant in all reference frames?",
          "options": [
            "Kinetic energy",
            "Momentum magnitude",
            "E² - (pc)²",
            "Total energy"
          ],
          "correct": 2
        },
        {
          "question": "For a stationary particle, what is its total relativistic energy?",
          "options": [
            "0",
            "m₀c²",
            "½m₀v²",
            "pc"
          ],
          "correct": 1
        },
        {
          "question": "Why can't we just add kinetic energies in relativity?",
          "options": [
            "Energy isn't conserved",
            "Rest mass energy must be included",
            "Momentum is more important",
            "Time dilation prevents it"
          ],
          "correct": 1
        },
        {
          "question": "What is the four-momentum of a particle?",
          "options": [
            "(m, v)",
            "(E/c, p)",
            "(t, x)",
            "(γ, β)"
          ],
          "correct": 1
        },
        {
          "question": "If T = 0 (no kinetic energy), what is M?",
          "options": [
            "m₀",
            "2m₀",
            "√2 m₀",
            "0"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "physics-2",
      "title": "Expanding Particle Cloud (Critical Density Universe)",
      "difficulty": "Medium",
      "related": [
        {
          "id": "physics-1",
          "title": "Relativistic Collision",
          "category": "physics"
        }
      ],
      "tags": [
        "mechanics",
        "gravity",
        "cosmology",
        "energy-conservation"
      ],
      "problem_statement": "A spherical cloud of particles is uniformly distributed. Each particle has a **radial velocity** directed outward from the center with magnitude $v = Hr$, where $r$ is the distance from the center and $H$ is a constant.\n\nThe cloud has the **minimum density** that allows it to expand forever (not collapse back due to gravity).\n\n**Find:**\nHow does the speed $v$ of a particular particle depend on its distance $R$ from the center at a future time?\n\n**Given:**\n- Initial distance of particle: $r_0$\n- Initial velocity: $v_0 = Hr_0$\n- Velocity law: $v = Hr$ (Hubble-like expansion)\n- Critical density condition (minimum to escape)",
      "explanation": {
        "understanding_the_problem": "This is a classic problem in classical mechanics that parallels the dynamics of the **expansion of the Universe** — specifically the Newtonian derivation of the Friedmann equations for a 'flat' universe.\n\nThe condition 'minimum density to expand forever' means the cloud is at the boundary between expanding forever and collapsing back. This is the **critical density** condition where **Total Energy = 0**.\n\n**Key Insight (Shell Theorem):** A particle at distance $R$ is only affected by the mass $M$ enclosed within radius $R$. Since the expansion is radial and $v \\propto r$, layers don't cross, so $M$ remains constant for our particle.",
        "brute_force": "Since Total Energy = 0 (critical density condition):\n\n$$E_{total} = K + U = 0$$\n$$\\frac{1}{2}mv^2 - \\frac{GMm}{R} = 0$$\n\nwhere:\n- $v$ is the speed at distance $R$\n- $G$ is the gravitational constant\n- $M$ is the enclosed mass (constant)\n- $R$ is the instantaneous distance from center",
        "bottleneck": "We have two unknowns: $v$ and $M$. We need to eliminate $M$ using the initial conditions.",
        "optimized_approach": "**Step 1: Solve for velocity at any distance R**\n\nFrom the energy equation:\n$$\\frac{1}{2}mv^2 = \\frac{GMm}{R}$$\n$$v^2 = \\frac{2GM}{R}$$\n$$v(R) = \\sqrt{\\frac{2GM}{R}}$$\n\n**Step 2: Apply initial conditions to find 2GM**\n\nAt $t = 0$: $R = r_0$ and $v = Hr_0$\n\nSubstitute:\n$$(Hr_0) = \\sqrt{\\frac{2GM}{r_0}}$$\n\nSquare both sides:\n$$H^2 r_0^2 = \\frac{2GM}{r_0}$$\n$$2GM = H^2 r_0^3$$\n\n**Step 3: Substitute back**\n\n$$v(R) = \\sqrt{\\frac{H^2 r_0^3}{R}} = Hr_0\\sqrt{\\frac{r_0}{R}}$$",
        "algorithm_steps": "1. Write energy conservation: $\\frac{1}{2}mv^2 - \\frac{GMm}{R} = 0$\n2. Solve for $v$: $v = \\sqrt{2GM/R}$\n3. Apply initial condition: At $R = r_0$, $v = Hr_0$\n4. Find constant: $2GM = H^2 r_0^3$\n5. Substitute: $v(R) = Hr_0\\sqrt{r_0/R} = v_0\\sqrt{r_0/R}$",
        "result_analysis": "**Final Result:**\n\n$$v(R) = v_0\\sqrt{\\frac{r_0}{R}} \\propto \\frac{1}{\\sqrt{R}}$$\n\n**The Critical Density:**\n\nThe 'minimum density' condition means Total Energy = 0. Using $v = Hr$ and $M = \\frac{4}{3}\\pi r^3 \\rho$:\n\n$$\\frac{1}{2}(Hr)^2 = \\frac{GM}{r} = \\frac{4\\pi G \\rho r^2}{3}$$\n\nSolving for the critical density:\n$$\\rho_{crit} = \\frac{3H^2}{8\\pi G}$$\n\n**How Density Evolves:**\n\nAs the cloud expands, the total mass $M$ stays constant, but the volume grows. Since radius scales as $R \\propto t^{2/3}$ (for critical density), the density drops as:\n\n$$\\rho(t) \\propto \\frac{1}{R^3} \\propto \\frac{1}{t^2}$$\n\n**Physical Interpretation:**\n\nAs the cloud expands ($R$ increases), the particle **slows down** — converting kinetic energy to gravitational potential energy. Because the cloud is at critical density, the particle's speed approaches (but never quite reaches) zero as $R \\to \\infty$. It's on an exact **parabolic escape trajectory**.\n\n**Connection to Cosmology:**\n\nThis is a **Newtonian model of our Universe**! The velocity law $v = Hr$ is exactly the **Hubble Law**, and the critical density corresponds to a **flat universe** ($\\Omega = 1$). With $H \\approx 70$ km/s/Mpc, we get $\\rho_{crit} \\approx 10^{-26}$ kg/m³ — about 6 hydrogen atoms per cubic meter!\n\n**What About Fluctuations?**\n\nIf some particles have slightly different initial velocities:\n- **Slower particles** → gravitationally attract neighbors → form **overdense clumps** (galaxies, clusters)\n- **Faster particles** → escape from neighbors → create **voids**\n\nThis is the origin of **cosmic structure formation** (the Zel'dovich Approximation). Random fluctuations in the early universe created the cosmic web of galaxies, filaments, and voids we observe today!\n\n**The Pancake Effect:**\n\nCollapse happens anisotropically — faster along one axis first, creating:\n1. **Sheets** ('pancakes') — collapse in 1D\n2. **Filaments** — sheets collapse in 2D\n3. **Clusters** — filaments collapse to nodes\n\nThis web-like structure matches observations of the large-scale universe."
      },
      "quizzes": [
        {
          "question": "What physical condition defines 'minimum density to dissolve'?",
          "options": [
            "Kinetic Energy = Potential Energy",
            "Total Energy = 0 (critical density)",
            "Momentum is conserved",
            "Temperature is constant"
          ],
          "correct": 1
        },
        {
          "question": "Why does the enclosed mass M remain constant for our particle?",
          "options": [
            "Mass is created during expansion",
            "Layers don't cross because v ∝ r",
            "Gravity doesn't act on moving particles",
            "The cloud is infinitely large"
          ],
          "correct": 1
        },
        {
          "question": "How does v depend on R?",
          "options": [
            "v ∝ R",
            "v ∝ 1/R",
            "v ∝ 1/√R",
            "v is constant"
          ],
          "correct": 2
        },
        {
          "question": "What happens as R → ∞?",
          "options": [
            "v → ∞",
            "v → c (speed of light)",
            "v → 0",
            "v oscillates"
          ],
          "correct": 2
        },
        {
          "question": "What cosmological structure does this model explain?",
          "options": [
            "Black holes only",
            "The cosmic web (galaxies, filaments, voids)",
            "Planetary orbits",
            "Atomic structure"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "physics-3",
      "title": "Random Walk to Diffusion Equation (Einstein 1905)",
      "difficulty": "Hard",
      "related": [
        {
          "id": "physics-2",
          "title": "Expanding Particle Cloud",
          "category": "physics"
        }
      ],
      "tags": [
        "probability",
        "taylor-series",
        "diffusion",
        "brownian-motion"
      ],
      "problem_statement": "A particle performs a **random walk** on a 1D line. At each time step $\\Delta t$, it moves:\n- Right by $+\\Delta x$ with probability $\\frac{1}{2}$\n- Left by $-\\Delta x$ with probability $\\frac{1}{2}$\n\n**Derive** the equation governing the probability density $P(x,t)$ in the continuous limit ($\\Delta x, \\Delta t \\to 0$).\n\n**This is the Diffusion Equation** — the foundation of Brownian motion, stock price models, and AI image generation!",
      "explanation": {
        "understanding_the_problem": "This derivation (Einstein, 1905) connects **discrete probability** (coin flips) to **continuous differential equations** (the heat equation).\n\nThe key insight is that Taylor expansion reveals the hidden calculus structure inside a simple random walk.\n\n**Why This Matters:**\n- **Physics**: Explains Brownian motion (pollen jiggling in water) — proved atoms exist!\n- **Finance**: Replace 'position' with 'stock price' → Black-Scholes equation\n- **AI**: This is the math behind **Diffusion Models** (Stable Diffusion, Midjourney)",
        "brute_force": "**Step 1: The Master Equation**\n\nIf a particle is at position $x$ at time $t + \\Delta t$, it must have come from either $(x - \\Delta x)$ or $(x + \\Delta x)$ at time $t$:\n\n$$P(x, t + \\Delta t) = \\frac{1}{2}P(x - \\Delta x, t) + \\frac{1}{2}P(x + \\Delta x, t)$$\n\nThis is the discrete 'Master Equation' for the random walk.",
        "bottleneck": "The equation is discrete (finite steps). We need to extract the continuous differential equation hiding inside.",
        "optimized_approach": "**Step 2: Taylor Expand the Left Side (Time)**\n\nExpand $P(x, t + \\Delta t)$ to first order in time:\n$$P(x, t + \\Delta t) \\approx P(x,t) + \\frac{\\partial P}{\\partial t}\\Delta t$$\n\n**Step 3: Taylor Expand the Right Side (Space)**\n\nExpand each spatial term to second order:\n$$P(x \\pm \\Delta x, t) \\approx P(x,t) \\pm \\frac{\\partial P}{\\partial x}\\Delta x + \\frac{1}{2}\\frac{\\partial^2 P}{\\partial x^2}(\\Delta x)^2$$\n\n**Step 4: Substitute into Master Equation**\n\nLeft side:\n$$P + \\frac{\\partial P}{\\partial t}\\Delta t$$\n\nRight side (add the two terms):\n$$\\frac{1}{2}\\left[P - \\frac{\\partial P}{\\partial x}\\Delta x + \\frac{1}{2}\\frac{\\partial^2 P}{\\partial x^2}(\\Delta x)^2\\right] + \\frac{1}{2}\\left[P + \\frac{\\partial P}{\\partial x}\\Delta x + \\frac{1}{2}\\frac{\\partial^2 P}{\\partial x^2}(\\Delta x)^2\\right]$$\n\n**Step 5: The Magic Cancellation**\n\nThe first-order spatial terms **cancel** (because the walk is symmetric):\n$$\\pm\\frac{\\partial P}{\\partial x}\\Delta x \\to 0$$\n\nWhat survives:\n$$P + \\frac{1}{2}\\frac{\\partial^2 P}{\\partial x^2}(\\Delta x)^2$$\n\n**Step 6: Combine and Simplify**\n\n$$P + \\frac{\\partial P}{\\partial t}\\Delta t = P + \\frac{1}{2}\\frac{\\partial^2 P}{\\partial x^2}(\\Delta x)^2$$\n\nCancel $P$ and divide by $\\Delta t$:\n$$\\frac{\\partial P}{\\partial t} = \\frac{(\\Delta x)^2}{2\\Delta t}\\frac{\\partial^2 P}{\\partial x^2}$$",
        "algorithm_steps": "1. Write Master Equation: $P(x,t+\\Delta t) = \\frac{1}{2}P(x-\\Delta x,t) + \\frac{1}{2}P(x+\\Delta x,t)$\n2. Taylor expand LHS in time (1st order)\n3. Taylor expand RHS in space (2nd order)\n4. Substitute and observe: first-order $\\partial P/\\partial x$ terms cancel!\n5. Simplify: $\\frac{\\partial P}{\\partial t} = D\\frac{\\partial^2 P}{\\partial x^2}$\n6. Define diffusion coefficient: $D = \\frac{(\\Delta x)^2}{2\\Delta t}$",
        "result_analysis": "**Final Result — The Diffusion Equation:**\n\n$$\\frac{\\partial P}{\\partial t} = D\\frac{\\partial^2 P}{\\partial x^2}$$\n\nwhere the **Diffusion Coefficient** is:\n$$D = \\frac{(\\Delta x)^2}{2\\Delta t}$$\n\n**Why First-Order Canceled:**\n\nThe $\\partial P/\\partial x$ terms canceled because the walk is **symmetric** (equal probability left and right). If we had biased probabilities, we'd get an additional **drift term** $v\\frac{\\partial P}{\\partial x}$ — this is the Fokker-Planck equation!\n\n**Physical Interpretation:**\n\n- The equation says probability **spreads out** over time (second derivative drives diffusion)\n- Peaked distributions flatten; edges blur\n- This is why ink drops spread in water, heat conducts through metal, and perfume fills a room\n\n**Einstein's 1905 Result:**\n\nEinstein showed that for a particle of radius $r$ in fluid of viscosity $\\eta$ at temperature $T$:\n$$D = \\frac{k_B T}{6\\pi\\eta r}$$\n\nBy measuring the spread of Brownian particles, Perrin verified this and calculated Avogadro's number — proving atoms exist!\n\n**Applications:**\n\n- **Brownian Motion**: Pollen grains jiggling in water\n- **Black-Scholes**: Replace $x$ with stock price → foundation of options pricing\n- **AI Diffusion Models**: Stable Diffusion, Midjourney, DALL-E 3 all use this equation! They add noise (forward diffusion) then learn to reverse it (denoising)\n- **Heat Conduction**: Same equation governs temperature flow"
      },
      "quizzes": [
        {
          "question": "Why does the first-order spatial derivative cancel?",
          "options": [
            "Taylor series doesn't have first-order terms",
            "The walk is symmetric (equal probability left/right)",
            "Space is continuous",
            "Time is discrete"
          ],
          "correct": 1
        },
        {
          "question": "What is the Diffusion Coefficient D in terms of step size and time?",
          "options": [
            "D = Δx/Δt",
            "D = (Δx)²/(2Δt)",
            "D = Δt/(Δx)²",
            "D = 2ΔxΔt"
          ],
          "correct": 1
        },
        {
          "question": "What physical phenomenon did this derivation explain?",
          "options": [
            "Planetary orbits",
            "Brownian motion (pollen in water)",
            "Electromagnetic waves",
            "Nuclear decay"
          ],
          "correct": 1
        },
        {
          "question": "What modern AI technology uses this equation?",
          "options": [
            "ChatGPT (transformers)",
            "Stable Diffusion / Midjourney (diffusion models)",
            "AlphaGo (reinforcement learning)",
            "BERT (language models)"
          ],
          "correct": 1
        },
        {
          "question": "What would happen if probabilities were biased (not 50/50)?",
          "options": [
            "Equation unchanged",
            "Add a drift term (Fokker-Planck equation)",
            "No diffusion occurs",
            "Probability becomes negative"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "physics-4",
      "title": "The Brachistochrone Problem (1696)",
      "difficulty": "Medium",
      "related": [
        {
          "id": "physics-2",
          "title": "Expanding Particle Cloud",
          "category": "physics"
        }
      ],
      "tags": [
        "calculus-of-variations",
        "optimization",
        "cycloid",
        "lagrangian-mechanics"
      ],
      "problem_statement": "A bead slides frictionlessly down a wire from point $A$ to point $B$ (which is lower but not directly below $A$). Gravity pulls the bead downward.\n\n**What shape of wire** minimizes the travel time from $A$ to $B$?\n\n**Historical Context:** Johann Bernoulli posed this challenge in 1696 to the greatest mathematicians of Europe. Newton, Leibniz, and others all solved it — launching the field of **Calculus of Variations**.",
      "explanation": {
        "understanding_the_problem": "This is the founding problem of **Calculus of Variations** — the math of optimizing not just numbers, but entire *functions* (curves).\n\nIntuition might suggest:\n- **Straight line?** Shortest distance, but the bead starts slowly (low speed at top).\n- **Steep drop first?** Gains speed early, but path becomes longer.\n\nThe optimal curve balances these tradeoffs. The answer is neither a line nor a parabola — it's a **Cycloid**!",
        "brute_force": "**Setting Up the Time Integral**\n\nUsing energy conservation, the speed at height $y$ below the start is:\n$$v = \\sqrt{2gy}$$\n\nThe time to traverse a small arc length $ds$ is:\n$$dt = \\frac{ds}{v} = \\frac{\\sqrt{1 + (dy/dx)^2}}{\\sqrt{2gy}}dx$$\n\nTotal time:\n$$T = \\int_0^{x_B} \\frac{\\sqrt{1 + y'^2}}{\\sqrt{2gy}} dx$$\n\nWe need to find the function $y(x)$ that minimizes this integral.",
        "bottleneck": "This is not ordinary calculus — we're minimizing over all possible *curves*, not just finding the minimum of a function of one variable.",
        "optimized_approach": "**The Euler-Lagrange Equation**\n\nFor functionals of the form $\\int F(y, y', x) dx$, the minimizing curve satisfies:\n$$\\frac{\\partial F}{\\partial y} - \\frac{d}{dx}\\frac{\\partial F}{\\partial y'} = 0$$\n\nSince our $F$ doesn't depend on $x$ explicitly, there's a shortcut (Beltrami identity):\n$$F - y'\\frac{\\partial F}{\\partial y'} = C$$\n\n**Solving the ODE**\n\nAfter substitution and simplification, this leads to:\n$$y(1 + y'^2) = 2R$$\n\nfor some constant $R$. This is the differential equation of a **Cycloid**!\n\n**Parametric Solution:**\n$$x = R(\\theta - \\sin\\theta), \\quad y = R(1 - \\cos\\theta)$$",
        "algorithm_steps": "1. Write time as functional: $T[y] = \\int \\frac{\\sqrt{1+y'^2}}{\\sqrt{2gy}} dx$\n2. Identify the Lagrangian $F(y, y')$\n3. Apply Beltrami identity (since $F$ is independent of $x$)\n4. Solve the resulting ODE: $y(1 + y'^2) = const$\n5. Recognize solution as parametric cycloid equations",
        "result_analysis": "**Final Answer — The Cycloid:**\n\n$$x(\\theta) = R(\\theta - \\sin\\theta)$$\n$$y(\\theta) = R(1 - \\cos\\theta)$$\n\nwhere $R$ is chosen so the curve passes through point $B$.\n\n**What is a Cycloid?**\n\nIt's the curve traced by a point on the rim of a rolling wheel! Imagine a circle rolling along a ceiling — the path of a point on its edge is exactly the brachistochrone.\n\n**Why is it Faster Than a Straight Line?**\n\nThe cycloid **drops steeply at first**, gaining speed quickly while the path is still short. The 'extra' distance is compensated by higher velocity.\n\n**The Tautochrone Property:**\n\nRemarkably, the cycloid is also **isochronous** — a ball released from *any* point on a cycloid takes the **same time** to reach the bottom! This was used in Huygens' pendulum clocks.\n\n**Applications:**\n\n- **Lagrangian Mechanics**: This problem birthed the principle of 'Least Action'\n- **Roller Coaster Design**: Optimizing thrill vs. safety\n- **Optics**: Fermat's principle (light takes the fastest path) uses the same math\n- **Machine Learning**: Gradient descent is a discrete version of this optimization"
      },
      "quizzes": [
        {
          "question": "What is the shape of the curve that minimizes travel time?",
          "options": [
            "Straight line",
            "Parabola",
            "Cycloid",
            "Catenary"
          ],
          "correct": 2
        },
        {
          "question": "What is a cycloid?",
          "options": [
            "The graph of y = 1/x",
            "The path traced by a point on a rolling wheel",
            "A spiral curve",
            "An ellipse"
          ],
          "correct": 1
        },
        {
          "question": "Why isn't a straight line the fastest path?",
          "options": [
            "Gravity doesn't work on straight lines",
            "The bead moves slowly at the top where speed is low",
            "Friction is higher on straight lines",
            "The mass of the bead changes"
          ],
          "correct": 1
        },
        {
          "question": "What branch of mathematics was founded by this problem?",
          "options": [
            "Number Theory",
            "Topology",
            "Calculus of Variations",
            "Abstract Algebra"
          ],
          "correct": 2
        },
        {
          "question": "What other remarkable property does the cycloid have?",
          "options": [
            "Infinite length",
            "Tautochrone (same descent time from any point)",
            "Zero curvature",
            "Constant speed descent"
          ],
          "correct": 1
        }
      ]
    },
    {
      "id": "physics-5",
      "title": "Shannon's Channel Capacity (1948)",
      "difficulty": "Medium",
      "related": [
        {
          "id": "physics-3",
          "title": "Random Walk to Diffusion",
          "category": "physics"
        }
      ],
      "tags": [
        "information-theory",
        "communication",
        "logarithms",
        "signal-processing"
      ],
      "problem_statement": "A communication channel has **bandwidth** $B$ (in Hz) and a **signal-to-noise ratio** $S/N$.\n\n**What is the maximum rate** (in bits per second) at which data can be transmitted with arbitrarily low error probability?\n\n**Given:**\n- Bandwidth: $B$ Hz\n- Signal power: $S$\n- Noise power: $N$\n\n**Historical Context:** Claude Shannon proved this result in 1948, founding the field of **Information Theory** and enabling all modern digital communication.",
      "explanation": {
        "understanding_the_problem": "This is the **fundamental law of digital communication**. Shannon proved that every noisy channel has a maximum data rate $C$ (the 'Channel Capacity') such that:\n\n- **Below $C$**: Error-free communication is possible (with clever coding)\n- **Above $C$**: Errors are unavoidable, no matter how smart the encoding\n\nThis is like a 'speed limit' set by physics itself.",
        "brute_force": "**Intuition Building:**\n\n1. **More Bandwidth** → More 'lanes' to send data → Higher capacity\n2. **Higher Signal Power** → Easier to distinguish signal from noise → Higher capacity\n3. **More Noise** → Harder to distinguish signal → Lower capacity\n\nThe question is: how do these combine mathematically?",
        "bottleneck": "The surprising part is the **logarithmic** relationship with $S/N$. Doubling signal power doesn't double capacity!",
        "optimized_approach": "**Shannon's Derivation (Simplified)**\n\n1. In bandwidth $B$, you can send $2B$ independent samples per second (Nyquist).\n\n2. Each sample can encode information based on how many 'levels' you can distinguish above the noise.\n\n3. With signal power $S$ and noise power $N$, you can reliably distinguish about $\\sqrt{1 + S/N}$ levels.\n\n4. Information per sample = $\\log_2(\\text{levels}) = \\frac{1}{2}\\log_2(1 + S/N)$\n\n5. Total capacity:\n$$C = 2B \\times \\frac{1}{2}\\log_2(1 + S/N) = B\\log_2(1 + S/N)$$",
        "algorithm_steps": "1. Identify bandwidth $B$ (Hz) — determines samples per second\n2. Identify signal-to-noise ratio $S/N$\n3. Calculate distinguishable levels: $\\approx\\sqrt{1 + S/N}$\n4. Information per sample: $\\frac{1}{2}\\log_2(1 + S/N)$ bits\n5. Multiply by $2B$ samples/sec: $C = B\\log_2(1 + S/N)$ bits/sec",
        "result_analysis": "**Shannon-Hartley Theorem:**\n\n$$C = B \\log_2\\left(1 + \\frac{S}{N}\\right)$$\n\n**Units:** bits per second (bps)\n\n**Key Insights:**\n\n1. **Bandwidth matters linearly**: Double the bandwidth = double the capacity.\n2. **SNR matters logarithmically**: Doubling signal power adds only ~1 bit/sample.\n3. **Tradeoff**: You can trade bandwidth for power: narrow bandwidth + high power ≈ wide bandwidth + low power.\n\n**Why Logarithmic?**\n\nTo send $n$ bits, you need $2^n$ distinguishable levels. But noise 'blurs' levels together. The number of distinguishable levels scales as $\\sqrt{S/N}$, not $S/N$.\n\n**Real-World Examples:**\n\n- **WiFi 6** (one stream): 160 MHz bandwidth, ~30 dB SNR → **~1.6 Gbps**\n- **5G mmWave**: 400 MHz bandwidth, ~20 dB SNR → **~2.7 Gbps**\n- **Fiber Optic**: 4 THz bandwidth, ~25 dB SNR → **~33 Tbps**\n\n**Applications:**\n\n- **WiFi/5G**: Shannon's limit is why your WiFi has a speed limit!\n- **Fiber Optics**: Pushing channels to theoretical limits\n- **Space Communication**: NASA calculates optimal power for deep space probes\n- **Compression**: MP3/JPEG approach Shannon's source coding theorem"
      },
      "quizzes": [
        {
          "question": "What happens if you try to transmit faster than the channel capacity?",
          "options": [
            "The signal gets louder",
            "Errors become unavoidable",
            "The bandwidth increases",
            "Nothing, it just works"
          ],
          "correct": 1
        },
        {
          "question": "If you double the bandwidth, what happens to capacity?",
          "options": [
            "Doubles",
            "Quadruples",
            "Stays the same",
            "Halves"
          ],
          "correct": 0
        },
        {
          "question": "If you double the signal power (with same noise), capacity increases by approximately:",
          "options": [
            "2x",
            "1 bit per sample",
            "No change",
            "4x"
          ],
          "correct": 1
        },
        {
          "question": "What field did Shannon found with this theorem?",
          "options": [
            "Quantum Mechanics",
            "Information Theory",
            "Thermodynamics",
            "Number Theory"
          ],
          "correct": 1
        },
        {
          "question": "Which technology uses Shannon's theorem to calculate maximum data rates?",
          "options": [
            "Only radio",
            "Only fiber optics",
            "All digital communication (WiFi, 5G, fiber, etc.)",
            "Only satellite"
          ],
          "correct": 2
        }
      ]
    }
  ]
}